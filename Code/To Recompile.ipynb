{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset \n",
    "X_dev=pd.read_csv(\"X_dev.csv\")\n",
    "y_dev=pd.read_csv(\"y_dev.csv\")\n",
    "\n",
    "X_dev.drop(['Unnamed: 0'], axis=1, inplace =True)\n",
    "y_dev.drop(['Unnamed: 0'], axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEE(y_real, y_pred, **kwarg):\n",
    "    sum_t = 0\n",
    "    for i in range(len(y_real)):\n",
    "        sum_t += np.sqrt(np.power((y_real[i][0]-y_pred[i][0]), 2)+np.power((y_real[i][1]-y_pred[i][1]), 2))\n",
    "    return sum_t / len(y_real)\n",
    "\n",
    "MEE=make_scorer(MEE, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEE_k(y_real, y_pred):\n",
    "     return K.mean(K.sqrt(K.sum(K.square(y_pred - y_real), axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=5, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=0.1, mom=0.1, alpha=0.01, unit1=10, unit2=10, act='sigmoid'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(unit1, input_dim=10, activation=act, \n",
    "                    kernel_regularizer=l2(alpha), \n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(unit2, activation=act, \n",
    "                    kernel_regularizer=l2(alpha),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=lr, momentum=mom))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Epoch 1/600\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 57.0625\n",
      "Epoch 2/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 52.6237\n",
      "Epoch 3/600\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 45.0907\n",
      "Epoch 4/600\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 34.6479\n",
      "Epoch 5/600\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 24.0360\n",
      "Epoch 6/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.8218\n",
      "Epoch 7/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.9086\n",
      "Epoch 8/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.1172\n",
      "Epoch 9/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.1929\n",
      "Epoch 10/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 13.8942\n",
      "Epoch 11/600\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 11.7342\n",
      "Epoch 12/600\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 9.7995\n",
      "Epoch 13/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.7027\n",
      "Epoch 14/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4869\n",
      "Epoch 15/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.3083\n",
      "Epoch 16/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1445\n",
      "Epoch 17/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 7.9852\n",
      "Epoch 18/600\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 7.8449\n",
      "Epoch 19/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.6783\n",
      "Epoch 20/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 7.5538\n",
      "Epoch 21/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 7.3193\n",
      "Epoch 22/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.1066\n",
      "Epoch 23/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 6.9078\n",
      "Epoch 24/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.6848\n",
      "Epoch 25/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.4739\n",
      "Epoch 26/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.2532\n",
      "Epoch 27/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.0037\n",
      "Epoch 28/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.7790\n",
      "Epoch 29/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5336\n",
      "Epoch 30/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.3218\n",
      "Epoch 31/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 5.148 - 0s 4ms/step - loss: 5.1366\n",
      "Epoch 32/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8710\n",
      "Epoch 33/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7318\n",
      "Epoch 34/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4997\n",
      "Epoch 35/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4124\n",
      "Epoch 36/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.2253\n",
      "Epoch 37/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1019\n",
      "Epoch 38/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0155\n",
      "Epoch 39/600\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.9098\n",
      "Epoch 40/600\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 3.8190\n",
      "Epoch 41/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.7720\n",
      "Epoch 42/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.7272\n",
      "Epoch 43/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.6954\n",
      "Epoch 44/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.6563\n",
      "Epoch 45/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.5764\n",
      "Epoch 46/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.6524\n",
      "Epoch 47/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.5652\n",
      "Epoch 48/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5144\n",
      "Epoch 49/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.5305\n",
      "Epoch 50/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.4470\n",
      "Epoch 51/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3962\n",
      "Epoch 52/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3693\n",
      "Epoch 53/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4572\n",
      "Epoch 54/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3047\n",
      "Epoch 55/600\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.2819\n",
      "Epoch 56/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3378\n",
      "Epoch 57/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.223 - 0s 3ms/step - loss: 3.2497\n",
      "Epoch 58/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3646\n",
      "Epoch 59/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2405\n",
      "Epoch 60/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1896\n",
      "Epoch 61/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2577\n",
      "Epoch 62/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1679\n",
      "Epoch 63/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1587\n",
      "Epoch 64/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1328\n",
      "Epoch 65/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1604\n",
      "Epoch 66/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.061 - 0s 3ms/step - loss: 3.1037\n",
      "Epoch 67/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0831\n",
      "Epoch 68/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0680\n",
      "Epoch 69/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1153\n",
      "Epoch 70/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0750\n",
      "Epoch 71/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0306\n",
      "Epoch 72/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0350\n",
      "Epoch 73/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0231\n",
      "Epoch 74/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0188\n",
      "Epoch 75/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0303\n",
      "Epoch 76/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0745\n",
      "Epoch 77/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9745\n",
      "Epoch 78/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0812\n",
      "Epoch 79/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9640\n",
      "Epoch 80/600\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.9668\n",
      "Epoch 81/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9581\n",
      "Epoch 82/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9414\n",
      "Epoch 83/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9496\n",
      "Epoch 84/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9489\n",
      "Epoch 85/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9245\n",
      "Epoch 86/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9517\n",
      "Epoch 87/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9119\n",
      "Epoch 88/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8951\n",
      "Epoch 89/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9074\n",
      "Epoch 90/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8857\n",
      "Epoch 91/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9553\n",
      "Epoch 92/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9295\n",
      "Epoch 93/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8603\n",
      "Epoch 94/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9191\n",
      "Epoch 95/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8950\n",
      "Epoch 96/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8744\n",
      "Epoch 97/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8451\n",
      "Epoch 98/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8932\n",
      "Epoch 99/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8288\n",
      "Epoch 100/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8347\n",
      "Epoch 101/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8567\n",
      "Epoch 102/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8262\n",
      "Epoch 103/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8267\n",
      "Epoch 104/600\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.8069\n",
      "Epoch 105/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8249\n",
      "Epoch 106/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8094\n",
      "Epoch 107/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8833\n",
      "Epoch 108/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7998\n",
      "Epoch 109/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8414\n",
      "Epoch 110/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8481\n",
      "Epoch 111/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8162\n",
      "Epoch 112/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7825\n",
      "Epoch 113/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8428\n",
      "Epoch 114/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8111\n",
      "Epoch 115/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7895\n",
      "Epoch 116/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7816\n",
      "Epoch 117/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7878\n",
      "Epoch 118/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8012\n",
      "Epoch 119/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7986\n",
      "Epoch 120/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8976\n",
      "Epoch 121/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8098\n",
      "Epoch 122/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8052\n",
      "Epoch 123/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7938\n",
      "Epoch 124/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7543\n",
      "Epoch 125/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7875\n",
      "Epoch 126/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7490\n",
      "Epoch 127/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7546\n",
      "Epoch 128/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8546\n",
      "Epoch 129/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7520\n",
      "Epoch 130/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7360\n",
      "Epoch 131/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7460\n",
      "Epoch 132/600\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.7390\n",
      "Epoch 133/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7310\n",
      "Epoch 134/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7259\n",
      "Epoch 135/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7149\n",
      "Epoch 136/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7515\n",
      "Epoch 137/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7912\n",
      "Epoch 138/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7203\n",
      "Epoch 139/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7019\n",
      "Epoch 140/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7362\n",
      "Epoch 141/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7168\n",
      "Epoch 142/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7062\n",
      "Epoch 143/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.866 - 0s 3ms/step - loss: 2.8490\n",
      "Epoch 144/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7202\n",
      "Epoch 145/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7187\n",
      "Epoch 146/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7769\n",
      "Epoch 147/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7671\n",
      "Epoch 148/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7070\n",
      "Epoch 149/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6981\n",
      "Epoch 150/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7173\n",
      "Epoch 151/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7439\n",
      "Epoch 152/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6926\n",
      "Epoch 153/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6904\n",
      "Epoch 154/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6728\n",
      "Epoch 155/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.704 - 0s 3ms/step - loss: 2.7015\n",
      "Epoch 156/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7503\n",
      "Epoch 157/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6891\n",
      "Epoch 158/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6817\n",
      "Epoch 159/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7515\n",
      "Epoch 160/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6731\n",
      "Epoch 161/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7411\n",
      "Epoch 162/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7435\n",
      "Epoch 163/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6736\n",
      "Epoch 164/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6635\n",
      "Epoch 165/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7845\n",
      "Epoch 166/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6922\n",
      "Epoch 167/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6737\n",
      "Epoch 168/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7732\n",
      "Epoch 169/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7512\n",
      "Epoch 170/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6805\n",
      "Epoch 171/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7135\n",
      "Epoch 172/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6934\n",
      "Epoch 173/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6768\n",
      "Epoch 174/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6729\n",
      "Epoch 175/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7264\n",
      "Epoch 176/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6576\n",
      "Epoch 177/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6637\n",
      "Epoch 178/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6500\n",
      "Epoch 179/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6758\n",
      "Epoch 180/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6412\n",
      "Epoch 181/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6701\n",
      "Epoch 182/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6288\n",
      "Epoch 183/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6554\n",
      "Epoch 184/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6888\n",
      "Epoch 185/600\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.6613\n",
      "Epoch 186/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6652\n",
      "Epoch 187/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6470\n",
      "Epoch 188/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6469\n",
      "Epoch 189/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6368\n",
      "Epoch 190/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6339\n",
      "Epoch 191/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6423\n",
      "Epoch 192/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6185\n",
      "Epoch 193/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6424\n",
      "Epoch 194/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7498\n",
      "Epoch 195/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6442\n",
      "Epoch 196/600\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2.6607\n",
      "Epoch 197/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6298\n",
      "Epoch 198/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6234\n",
      "Epoch 199/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8276\n",
      "Epoch 200/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6213\n",
      "Epoch 201/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6345\n",
      "Epoch 202/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6641\n",
      "Epoch 203/600\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.6881\n",
      "Epoch 204/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6093\n",
      "Epoch 205/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6016\n",
      "Epoch 206/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6130\n",
      "Epoch 207/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6208\n",
      "Epoch 208/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6418\n",
      "Epoch 209/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6207\n",
      "Epoch 210/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6283\n",
      "Epoch 211/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6290\n",
      "Epoch 212/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6013\n",
      "Epoch 213/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6225\n",
      "Epoch 214/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6269\n",
      "Epoch 215/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6041\n",
      "Epoch 216/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6052\n",
      "Epoch 217/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6122\n",
      "Epoch 218/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5919\n",
      "Epoch 219/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5972\n",
      "Epoch 220/600\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 2.6680\n",
      "Epoch 221/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5834\n",
      "Epoch 222/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6207\n",
      "Epoch 223/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5991\n",
      "Epoch 224/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5867\n",
      "Epoch 225/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5977\n",
      "Epoch 226/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6201\n",
      "Epoch 227/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5863\n",
      "Epoch 228/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6414\n",
      "Epoch 229/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.6137\n",
      "Epoch 230/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5742\n",
      "Epoch 231/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5812\n",
      "Epoch 232/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5823\n",
      "Epoch 233/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6751\n",
      "Epoch 234/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5860\n",
      "Epoch 235/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6392\n",
      "Epoch 236/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.568 - 0s 17ms/step - loss: 2.5875\n",
      "Epoch 237/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6093\n",
      "Epoch 238/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6474\n",
      "Epoch 239/600\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.6123\n",
      "Epoch 240/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6070\n",
      "Epoch 241/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.6232\n",
      "Epoch 242/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5886\n",
      "Epoch 243/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6165\n",
      "Epoch 244/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5628\n",
      "Epoch 245/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6171\n",
      "Epoch 246/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5898\n",
      "Epoch 247/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.5814\n",
      "Epoch 248/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5992\n",
      "Epoch 249/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5747\n",
      "Epoch 250/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5871\n",
      "Epoch 251/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5595\n",
      "Epoch 252/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6087\n",
      "Epoch 253/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6005\n",
      "Epoch 254/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5628\n",
      "Epoch 255/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6073\n",
      "Epoch 256/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5514\n",
      "Epoch 257/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5655\n",
      "Epoch 258/600\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.5549\n",
      "Epoch 259/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5745\n",
      "Epoch 260/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6346\n",
      "Epoch 261/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5600\n",
      "Epoch 262/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6253\n",
      "Epoch 263/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5589\n",
      "Epoch 264/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5998\n",
      "Epoch 265/600\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 2.5858\n",
      "Epoch 266/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5518\n",
      "Epoch 267/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5976\n",
      "Epoch 268/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5525\n",
      "Epoch 269/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5485\n",
      "Epoch 270/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7095\n",
      "Epoch 271/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5761\n",
      "Epoch 272/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5579\n",
      "Epoch 273/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5550\n",
      "Epoch 274/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5405\n",
      "Epoch 275/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5554\n",
      "Epoch 276/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5679\n",
      "Epoch 277/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6481\n",
      "Epoch 278/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5450\n",
      "Epoch 279/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5414\n",
      "Epoch 280/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5442\n",
      "Epoch 281/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5704\n",
      "Epoch 282/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5535\n",
      "Epoch 283/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5396\n",
      "Epoch 284/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5859\n",
      "Epoch 285/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5427\n",
      "Epoch 286/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5968\n",
      "Epoch 287/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5281\n",
      "Epoch 288/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5290\n",
      "Epoch 289/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5311\n",
      "Epoch 290/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5371\n",
      "Epoch 291/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6057\n",
      "Epoch 292/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5251\n",
      "Epoch 293/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5576\n",
      "Epoch 294/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5465\n",
      "Epoch 295/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5806\n",
      "Epoch 296/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5266\n",
      "Epoch 297/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5301\n",
      "Epoch 298/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5688\n",
      "Epoch 299/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5545\n",
      "Epoch 300/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5448\n",
      "Epoch 301/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5579\n",
      "Epoch 302/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5322\n",
      "Epoch 303/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5277\n",
      "Epoch 304/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5969\n",
      "Epoch 305/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5184\n",
      "Epoch 306/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5300\n",
      "Epoch 307/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5279\n",
      "Epoch 308/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6069\n",
      "Epoch 309/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5307\n",
      "Epoch 310/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5836\n",
      "Epoch 311/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5738\n",
      "Epoch 312/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5175\n",
      "Epoch 313/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5794\n",
      "Epoch 314/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5144\n",
      "Epoch 315/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5144\n",
      "Epoch 316/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5186\n",
      "Epoch 317/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5018\n",
      "Epoch 318/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5458\n",
      "Epoch 319/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5614\n",
      "Epoch 320/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7543\n",
      "Epoch 321/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5158\n",
      "Epoch 322/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5397\n",
      "Epoch 323/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5189\n",
      "Epoch 324/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5259\n",
      "Epoch 325/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5014\n",
      "Epoch 326/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5137\n",
      "Epoch 327/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4973\n",
      "Epoch 328/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4958\n",
      "Epoch 329/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5177\n",
      "Epoch 330/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5354\n",
      "Epoch 331/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4947\n",
      "Epoch 332/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4972\n",
      "Epoch 333/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4895\n",
      "Epoch 334/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5017\n",
      "Epoch 335/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5146\n",
      "Epoch 336/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5066\n",
      "Epoch 337/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4941\n",
      "Epoch 338/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5750\n",
      "Epoch 339/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5564\n",
      "Epoch 340/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4880\n",
      "Epoch 341/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4836\n",
      "Epoch 342/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5026\n",
      "Epoch 343/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5984\n",
      "Epoch 344/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5149\n",
      "Epoch 345/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4915\n",
      "Epoch 346/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4944\n",
      "Epoch 347/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4822\n",
      "Epoch 348/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4835\n",
      "Epoch 349/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5194\n",
      "Epoch 350/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4886\n",
      "Epoch 351/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4823\n",
      "Epoch 352/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4804\n",
      "Epoch 353/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5469\n",
      "Epoch 354/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4835\n",
      "Epoch 355/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5175\n",
      "Epoch 356/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5336\n",
      "Epoch 357/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4763\n",
      "Epoch 358/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5303\n",
      "Epoch 359/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4765\n",
      "Epoch 360/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4820\n",
      "Epoch 361/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4916\n",
      "Epoch 362/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5157\n",
      "Epoch 363/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.471 - 0s 3ms/step - loss: 2.5077\n",
      "Epoch 364/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5110\n",
      "Epoch 365/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4771\n",
      "Epoch 366/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4898\n",
      "Epoch 367/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4682\n",
      "Epoch 368/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4705\n",
      "Epoch 369/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4736\n",
      "Epoch 370/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4865\n",
      "Epoch 371/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5562\n",
      "Epoch 372/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5034\n",
      "Epoch 373/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4700\n",
      "Epoch 374/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5002\n",
      "Epoch 375/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4647\n",
      "Epoch 376/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4548\n",
      "Epoch 377/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4687\n",
      "Epoch 378/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4721\n",
      "Epoch 379/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4704\n",
      "Epoch 380/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4959\n",
      "Epoch 381/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4580\n",
      "Epoch 382/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4574\n",
      "Epoch 383/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5473\n",
      "Epoch 384/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5124\n",
      "Epoch 385/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5044\n",
      "Epoch 386/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4982\n",
      "Epoch 387/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5029\n",
      "Epoch 388/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4576\n",
      "Epoch 389/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4688\n",
      "Epoch 390/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4694\n",
      "Epoch 391/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4595\n",
      "Epoch 392/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4837\n",
      "Epoch 393/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4607\n",
      "Epoch 394/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4789\n",
      "Epoch 395/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5473\n",
      "Epoch 396/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5093\n",
      "Epoch 397/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4794\n",
      "Epoch 398/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4454\n",
      "Epoch 399/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4649\n",
      "Epoch 400/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4517\n",
      "Epoch 401/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4874\n",
      "Epoch 402/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5042\n",
      "Epoch 403/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4569\n",
      "Epoch 404/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4941\n",
      "Epoch 405/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4912\n",
      "Epoch 406/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4521\n",
      "Epoch 407/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4765\n",
      "Epoch 408/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4760\n",
      "Epoch 409/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4374\n",
      "Epoch 410/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4427\n",
      "Epoch 411/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4490\n",
      "Epoch 412/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5237\n",
      "Epoch 413/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4382\n",
      "Epoch 414/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4474\n",
      "Epoch 415/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5518\n",
      "Epoch 416/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4745\n",
      "Epoch 417/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4457\n",
      "Epoch 418/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4794\n",
      "Epoch 419/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4306\n",
      "Epoch 420/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4938\n",
      "Epoch 421/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4352\n",
      "Epoch 422/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4567\n",
      "Epoch 423/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5312\n",
      "Epoch 424/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4643\n",
      "Epoch 425/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4603\n",
      "Epoch 426/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5026\n",
      "Epoch 427/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4378\n",
      "Epoch 428/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4481\n",
      "Epoch 429/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4629\n",
      "Epoch 430/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4451\n",
      "Epoch 431/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4576\n",
      "Epoch 432/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4433\n",
      "Epoch 433/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5023\n",
      "Epoch 434/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4636\n",
      "Epoch 435/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4383\n",
      "Epoch 436/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4587\n",
      "Epoch 437/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4345\n",
      "Epoch 438/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4505\n",
      "Epoch 439/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4426\n",
      "Epoch 440/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5126\n",
      "Epoch 441/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4276\n",
      "Epoch 442/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4451\n",
      "Epoch 443/600\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4226\n",
      "Epoch 444/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4223\n",
      "Epoch 445/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4181\n",
      "Epoch 446/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4328\n",
      "Epoch 447/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4211\n",
      "Epoch 448/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4326\n",
      "Epoch 449/600\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.4104\n",
      "Epoch 450/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4141\n",
      "Epoch 451/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4884\n",
      "Epoch 452/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4697\n",
      "Epoch 453/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4319\n",
      "Epoch 454/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4047\n",
      "Epoch 455/600\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.4174\n",
      "Epoch 456/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4152\n",
      "Epoch 457/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4290\n",
      "Epoch 458/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4254\n",
      "Epoch 459/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5054\n",
      "Epoch 460/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4321\n",
      "Epoch 461/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5039\n",
      "Epoch 462/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4353\n",
      "Epoch 463/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4200\n",
      "Epoch 464/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5365\n",
      "Epoch 465/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4401\n",
      "Epoch 466/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4652\n",
      "Epoch 467/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4700\n",
      "Epoch 468/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4130\n",
      "Epoch 469/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4269\n",
      "Epoch 470/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4076\n",
      "Epoch 471/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4402\n",
      "Epoch 472/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4133\n",
      "Epoch 473/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4360\n",
      "Epoch 474/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4828\n",
      "Epoch 475/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4395\n",
      "Epoch 476/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4004\n",
      "Epoch 477/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4178\n",
      "Epoch 478/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4141\n",
      "Epoch 479/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4001\n",
      "Epoch 480/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4419\n",
      "Epoch 481/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4015\n",
      "Epoch 482/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5483\n",
      "Epoch 483/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4575\n",
      "Epoch 484/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4532\n",
      "Epoch 485/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3913\n",
      "Epoch 486/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4079\n",
      "Epoch 487/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.386 - 0s 3ms/step - loss: 2.3923\n",
      "Epoch 488/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4170\n",
      "Epoch 489/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3842\n",
      "Epoch 490/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3889\n",
      "Epoch 491/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4073\n",
      "Epoch 492/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4414\n",
      "Epoch 493/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4441\n",
      "Epoch 494/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4256\n",
      "Epoch 495/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4718\n",
      "Epoch 496/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3949\n",
      "Epoch 497/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4856\n",
      "Epoch 498/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3854\n",
      "Epoch 499/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4242\n",
      "Epoch 500/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4246\n",
      "Epoch 501/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3913\n",
      "Epoch 502/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4592\n",
      "Epoch 503/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4065\n",
      "Epoch 504/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4149\n",
      "Epoch 505/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4078\n",
      "Epoch 506/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3770\n",
      "Epoch 507/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3805\n",
      "Epoch 508/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4590\n",
      "Epoch 509/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3783\n",
      "Epoch 510/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4666\n",
      "Epoch 511/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4074\n",
      "Epoch 512/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4742\n",
      "Epoch 513/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3819\n",
      "Epoch 514/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3883\n",
      "Epoch 515/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3751\n",
      "Epoch 516/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3878\n",
      "Epoch 517/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3982\n",
      "Epoch 518/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4292\n",
      "Epoch 519/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3723\n",
      "Epoch 520/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3766\n",
      "Epoch 521/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4687\n",
      "Epoch 522/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3960\n",
      "Epoch 523/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4462\n",
      "Epoch 524/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3804\n",
      "Epoch 525/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4382\n",
      "Epoch 526/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3871\n",
      "Epoch 527/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3683\n",
      "Epoch 528/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3800\n",
      "Epoch 529/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3728\n",
      "Epoch 530/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4165\n",
      "Epoch 531/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3881\n",
      "Epoch 532/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4248\n",
      "Epoch 533/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4000\n",
      "Epoch 534/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3727\n",
      "Epoch 535/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3929\n",
      "Epoch 536/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3921\n",
      "Epoch 537/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3835\n",
      "Epoch 538/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3597\n",
      "Epoch 539/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3645\n",
      "Epoch 540/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3618\n",
      "Epoch 541/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3652\n",
      "Epoch 542/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3783\n",
      "Epoch 543/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3680\n",
      "Epoch 544/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4205\n",
      "Epoch 545/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3937\n",
      "Epoch 546/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4147\n",
      "Epoch 547/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4517\n",
      "Epoch 548/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3802\n",
      "Epoch 549/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3719\n",
      "Epoch 550/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3931\n",
      "Epoch 551/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3810\n",
      "Epoch 552/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3658\n",
      "Epoch 553/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3954\n",
      "Epoch 554/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3641\n",
      "Epoch 555/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3931\n",
      "Epoch 556/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4062\n",
      "Epoch 557/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3997\n",
      "Epoch 558/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3513\n",
      "Epoch 559/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4090\n",
      "Epoch 560/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3866\n",
      "Epoch 561/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3919\n",
      "Epoch 562/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3562\n",
      "Epoch 563/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3691\n",
      "Epoch 564/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3877\n",
      "Epoch 565/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3568\n",
      "Epoch 566/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4224\n",
      "Epoch 567/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4136\n",
      "Epoch 568/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3661\n",
      "Epoch 569/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3666\n",
      "Epoch 570/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3738\n",
      "Epoch 571/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3742\n",
      "Epoch 572/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3885\n",
      "Epoch 573/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3641\n",
      "Epoch 574/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3404\n",
      "Epoch 575/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3565\n",
      "Epoch 576/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4199\n",
      "Epoch 577/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3399\n",
      "Epoch 578/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3473\n",
      "Epoch 579/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3803\n",
      "Epoch 580/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3659\n",
      "Epoch 581/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3529\n",
      "Epoch 582/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3531\n",
      "Epoch 583/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3478\n",
      "Epoch 584/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3779\n",
      "Epoch 585/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3932\n",
      "Epoch 586/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3579\n",
      "Epoch 587/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3355\n",
      "Epoch 588/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3365\n",
      "Epoch 589/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3505\n",
      "Epoch 590/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3364\n",
      "Epoch 591/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3440\n",
      "Epoch 592/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3779\n",
      "Epoch 593/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3818\n",
      "Epoch 594/600\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.4339\n",
      "Epoch 595/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3755\n",
      "Epoch 596/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4607\n",
      "Epoch 597/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3380\n",
      "Epoch 598/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4130\n",
      "Epoch 599/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3844\n",
      "Epoch 600/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3302\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "hyper_params_space = {\n",
    "        'unit1' : [25, 30, 40, 50, 80],\n",
    "        'lr' : [0.001, 0.01, 0.1, 0.2, 0.3, 0.5],\n",
    "        'unit2': [5, 15, 25, 35],\n",
    "        'mom' : [0.01, 0.1, 0.5, 0.7, 0.9],\n",
    "        'alpha' : [0.1, 1e-5, 1e-8, 1e-10, 1e-20],\n",
    "        'act' : ['sigmoid', 'softmax']\n",
    "    },\n",
    "\n",
    "\n",
    "print('===================================')\n",
    "model = KerasRegressor(build_fn=create_model, batch_size=1036, epochs=600)\n",
    "mlpr = GridSearchCV( model\n",
    "                    , hyper_params_space, scoring='r2', cv=kf, \n",
    "                    refit='r2', \n",
    "                    n_jobs=2)\n",
    "mlpr.fit(X_dev, y_dev)\n",
    "print(\"DONE\")\n",
    "mlpr.best_estimator_.model.save(\"test_model2HL.h5\")\n",
    "resultGSCV=pd.DataFrame(mlpr.cv_results_)\n",
    "#print(pd.DataFrame(mlpr.cv_results_))\n",
    "resultGSCV.to_csv(r'try2HL.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act': 'sigmoid',\n",
       " 'alpha': 1e-08,\n",
       " 'lr': 0.2,\n",
       " 'mom': 0.5,\n",
       " 'unit1': 80,\n",
       " 'unit2': 25}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=X_dev.to_numpy()\n",
    "y_dev=y_dev.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modelLC():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(25, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 57.6771 - val_loss: 56.2214\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 56.2941 - val_loss: 54.1453\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 54.2270 - val_loss: 51.5861\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.6781 - val_loss: 48.4797\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 48.5812 - val_loss: 44.7068\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 44.8175 - val_loss: 40.2267\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 40.3509 - val_loss: 35.1406\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 35.2926 - val_loss: 29.7562\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 29.9579 - val_loss: 24.7929\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 24.9970 - val_loss: 21.5089\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 21.4211 - val_loss: 19.5207\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 19.1970 - val_loss: 18.1172\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 17.6546 - val_loss: 17.1167\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 16.5597 - val_loss: 16.3909\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 15.7683 - val_loss: 15.7690\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 15.1129 - val_loss: 15.0710\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 14.4149 - val_loss: 14.2544\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 13.6226 - val_loss: 13.4864\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 12.8927 - val_loss: 12.6081\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 12.0684 - val_loss: 11.5482\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 11.0687 - val_loss: 10.6935\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.2955 - val_loss: 9.9964\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 9.6756 - val_loss: 9.4106\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.1889 - val_loss: 8.9615\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.8717 - val_loss: 8.6392\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 8.6675 - val_loss: 8.4116\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 8.5172 - val_loss: 8.2518\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.3931 - val_loss: 8.1314\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2912 - val_loss: 8.0274\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.1961 - val_loss: 7.9379\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.1066 - val_loss: 7.8587\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0222 - val_loss: 7.7820\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.9411 - val_loss: 7.7031\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.8609 - val_loss: 7.6196\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.7791 - val_loss: 7.5317\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6945 - val_loss: 7.4401\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.6065 - val_loss: 7.3451\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.5152 - val_loss: 7.2473\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.4209 - val_loss: 7.1477\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.3238 - val_loss: 7.0463\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.2241 - val_loss: 6.9428\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.1222 - val_loss: 6.8369\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.0180 - val_loss: 6.7279\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.9119 - val_loss: 6.6161\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.8039 - val_loss: 6.5021\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.6944 - val_loss: 6.3866\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.5840 - val_loss: 6.2707\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.4731 - val_loss: 6.1544\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.3620 - val_loss: 6.0379\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.2512 - val_loss: 5.9218\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.1412 - val_loss: 5.8059\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.0319 - val_loss: 5.6910\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.9233 - val_loss: 5.5778\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.8155 - val_loss: 5.4663\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 5.7088 - val_loss: 5.3558\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 5.6037 - val_loss: 5.2466\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 5.5004 - val_loss: 5.1382\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 5.3992 - val_loss: 5.0303\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 5.3004 - val_loss: 4.9251\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.2040 - val_loss: 4.8238\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 5.1102 - val_loss: 4.7266\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.0189 - val_loss: 4.6343\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.9301 - val_loss: 4.5471\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.8443 - val_loss: 4.4650\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 4.7622 - val_loss: 4.3883\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 4.6844 - val_loss: 4.3172\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 4.6109 - val_loss: 4.2503\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.5410 - val_loss: 4.1867\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.4750 - val_loss: 4.1276\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.4125 - val_loss: 4.0713\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 4.3529 - val_loss: 4.0169\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 4.2964 - val_loss: 3.9656\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.2431 - val_loss: 3.9179\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.1927 - val_loss: 3.8740\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.1452 - val_loss: 3.8328\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.0999 - val_loss: 3.7936\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.0566 - val_loss: 3.7570\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4.0153 - val_loss: 3.7223\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 3.9756 - val_loss: 3.6904\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.9380 - val_loss: 3.6608\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.9024 - val_loss: 3.6323\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.8681 - val_loss: 3.6049\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.8350 - val_loss: 3.5792\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 3.8034 - val_loss: 3.5545\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.7731 - val_loss: 3.5303\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.7439 - val_loss: 3.5067\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.7158 - val_loss: 3.4838\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 3.6886 - val_loss: 3.4615\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 3.6624 - val_loss: 3.4402\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.6374 - val_loss: 3.4201\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.6135 - val_loss: 3.4010\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5905 - val_loss: 3.3826\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5684 - val_loss: 3.3645\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.5471 - val_loss: 3.3473\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.5264 - val_loss: 3.3308\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.5062 - val_loss: 3.3148\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.4867 - val_loss: 3.2992\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.4680 - val_loss: 3.2840\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 3.4497 - val_loss: 3.2697\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.4318 - val_loss: 3.2558\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.4145 - val_loss: 3.2422\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.3977 - val_loss: 3.2290\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.3815 - val_loss: 3.2157\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3657 - val_loss: 3.2028\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.3504 - val_loss: 3.1901\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 3.3358 - val_loss: 3.1778\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.3216 - val_loss: 3.1657\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.3078 - val_loss: 3.1540\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2944 - val_loss: 3.1426\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2815 - val_loss: 3.1316\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.2690 - val_loss: 3.1210\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.2569 - val_loss: 3.1109\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2452 - val_loss: 3.1014\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.2339 - val_loss: 3.0925\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.2230 - val_loss: 3.0838\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 3.2124 - val_loss: 3.0754\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 3.2023 - val_loss: 3.0672\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1924 - val_loss: 3.0592\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1829 - val_loss: 3.0515\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1738 - val_loss: 3.0441\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 3.1650 - val_loss: 3.0369\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1564 - val_loss: 3.0300\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.1482 - val_loss: 3.0235\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.1402 - val_loss: 3.0174\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1325 - val_loss: 3.0118\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1250 - val_loss: 3.0067\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1178 - val_loss: 3.0024\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 3.1108 - val_loss: 2.9986\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1041 - val_loss: 2.9951\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0976 - val_loss: 2.9921\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0913 - val_loss: 2.9890\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0852 - val_loss: 2.9867\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0793 - val_loss: 2.9838\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 3.0735 - val_loss: 2.9819\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0679 - val_loss: 2.9790\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0626 - val_loss: 2.9776\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.0573 - val_loss: 2.9745\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0524 - val_loss: 2.9737\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0475 - val_loss: 2.9702\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0432 - val_loss: 2.9704\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0389 - val_loss: 2.9666\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0360 - val_loss: 2.9684\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0322 - val_loss: 2.9645\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0314 - val_loss: 2.9683\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0286 - val_loss: 2.9631\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0278 - val_loss: 2.9664\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0236 - val_loss: 2.9596\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0207 - val_loss: 2.9614\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.0152 - val_loss: 2.9550\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0115 - val_loss: 2.9559\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0058 - val_loss: 2.9505\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0023 - val_loss: 2.9516\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9977 - val_loss: 2.9472\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.9946 - val_loss: 2.9482\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9904 - val_loss: 2.9443\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9876 - val_loss: 2.9447\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9835 - val_loss: 2.9419\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9813 - val_loss: 2.9450\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9797 - val_loss: 2.9407\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9771 - val_loss: 2.9424\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9749 - val_loss: 2.9402\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9738 - val_loss: 2.9405\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9700 - val_loss: 2.9356\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.9661 - val_loss: 2.9364\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9629 - val_loss: 2.9339\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9605 - val_loss: 2.9335\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.9574 - val_loss: 2.9313\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9554 - val_loss: 2.9313\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9527 - val_loss: 2.9291\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9503 - val_loss: 2.9281\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9472 - val_loss: 2.9265\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.9450 - val_loss: 2.9252\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9420 - val_loss: 2.9237\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9395 - val_loss: 2.9222\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9367 - val_loss: 2.9211\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9343 - val_loss: 2.9194\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9317 - val_loss: 2.9186\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9293 - val_loss: 2.9167\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.9270 - val_loss: 2.9163\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9245 - val_loss: 2.9142\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9224 - val_loss: 2.9139\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9200 - val_loss: 2.9113\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9177 - val_loss: 2.9111\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9151 - val_loss: 2.9082\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9123 - val_loss: 2.9076\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9095 - val_loss: 2.9038\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9056 - val_loss: 2.9034\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9030 - val_loss: 2.8995\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8992 - val_loss: 2.8993\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8971 - val_loss: 2.8964\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8945 - val_loss: 2.8970\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8931 - val_loss: 2.8940\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8909 - val_loss: 2.8949\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8897 - val_loss: 2.8917\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8875 - val_loss: 2.8920\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8856 - val_loss: 2.8887\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8834 - val_loss: 2.8893\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8817 - val_loss: 2.8857\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8795 - val_loss: 2.8869\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8779 - val_loss: 2.8832\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8760 - val_loss: 2.8846\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8742 - val_loss: 2.8808\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8725 - val_loss: 2.8824\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8706 - val_loss: 2.8784\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8691 - val_loss: 2.8802\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8670 - val_loss: 2.8764\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8661 - val_loss: 2.8783\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8636 - val_loss: 2.8747\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8634 - val_loss: 2.8767\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8605 - val_loss: 2.8730\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8608 - val_loss: 2.8750\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8573 - val_loss: 2.8709\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8578 - val_loss: 2.8730\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8538 - val_loss: 2.8684\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8543 - val_loss: 2.8710\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8502 - val_loss: 2.8659\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8507 - val_loss: 2.8691\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8466 - val_loss: 2.8635\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8471 - val_loss: 2.8672\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8431 - val_loss: 2.8611\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8436 - val_loss: 2.8654\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8395 - val_loss: 2.8586\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8399 - val_loss: 2.8634\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8358 - val_loss: 2.8560\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8360 - val_loss: 2.8612\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8319 - val_loss: 2.8534\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8318 - val_loss: 2.8587\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8277 - val_loss: 2.8508\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8276 - val_loss: 2.8559\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8233 - val_loss: 2.8484\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8234 - val_loss: 2.8528\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8186 - val_loss: 2.8461\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8189 - val_loss: 2.8495\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8138 - val_loss: 2.8439\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8142 - val_loss: 2.8463\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8092 - val_loss: 2.8418\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8096 - val_loss: 2.8435\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8050 - val_loss: 2.8400\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8057 - val_loss: 2.8415\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8016 - val_loss: 2.8388\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8028 - val_loss: 2.8402\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7990 - val_loss: 2.8364\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7994 - val_loss: 2.8381\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7958 - val_loss: 2.8345\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7961 - val_loss: 2.8358\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7926 - val_loss: 2.8327\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7929 - val_loss: 2.8338\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7896 - val_loss: 2.8310\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7900 - val_loss: 2.8322\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7869 - val_loss: 2.8298\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7876 - val_loss: 2.8307\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7845 - val_loss: 2.8285\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7853 - val_loss: 2.8292\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7821 - val_loss: 2.8271\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7829 - val_loss: 2.8275\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7796 - val_loss: 2.8255\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7803 - val_loss: 2.8257\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7769 - val_loss: 2.8237\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7774 - val_loss: 2.8236\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7739 - val_loss: 2.8216\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7741 - val_loss: 2.8212\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7706 - val_loss: 2.8192\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7706 - val_loss: 2.8187\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7673 - val_loss: 2.8168\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7671 - val_loss: 2.8162\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7640 - val_loss: 2.8146\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7639 - val_loss: 2.8141\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7611 - val_loss: 2.8127\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7612 - val_loss: 2.8122\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7585 - val_loss: 2.8111\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7588 - val_loss: 2.8106\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7562 - val_loss: 2.8096\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.7567 - val_loss: 2.8091\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7540 - val_loss: 2.8082\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7547 - val_loss: 2.8076\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7519 - val_loss: 2.8068\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7527 - val_loss: 2.8061\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7498 - val_loss: 2.8054\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7507 - val_loss: 2.8046\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7477 - val_loss: 2.8040\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7486 - val_loss: 2.8031\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.745 - 0s 93ms/step - loss: 2.7455 - val_loss: 2.8024\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7463 - val_loss: 2.8014\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7431 - val_loss: 2.8006\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7435 - val_loss: 2.7994\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7403 - val_loss: 2.7984\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.7399 - val_loss: 2.7968\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7370 - val_loss: 2.7961\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7358 - val_loss: 2.7935\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7332 - val_loss: 2.7941\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7326 - val_loss: 2.7921\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7307 - val_loss: 2.7929\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7305 - val_loss: 2.7913\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7288 - val_loss: 2.7923\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7287 - val_loss: 2.7903\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7270 - val_loss: 2.7914\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7268 - val_loss: 2.7892\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.7251 - val_loss: 2.7904\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7249 - val_loss: 2.7881\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7231 - val_loss: 2.7893\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7229 - val_loss: 2.7869\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7211 - val_loss: 2.7883\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7208 - val_loss: 2.7858\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 2.7192 - val_loss: 2.7873\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 2.7189 - val_loss: 2.7848\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7173 - val_loss: 2.7864\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7170 - val_loss: 2.7839\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.7154 - val_loss: 2.7856\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.7152 - val_loss: 2.7831\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7137 - val_loss: 2.7849\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7136 - val_loss: 2.7824\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7122 - val_loss: 2.7847\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.7124 - val_loss: 2.7822\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 2.7111 - val_loss: 2.7862\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7130 - val_loss: 2.7835\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.7119 - val_loss: 2.7884\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.7151 - val_loss: 2.7863\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7139 - val_loss: 2.7899\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7173 - val_loss: 2.7884\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7151 - val_loss: 2.7898\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7178 - val_loss: 2.7890\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.7148 - val_loss: 2.7873\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7153 - val_loss: 2.7870\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.7116 - val_loss: 2.7848\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2.7108 - val_loss: 2.7839\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7074 - val_loss: 2.7831\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 2.7069 - val_loss: 2.7821\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7042 - val_loss: 2.7818\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7038 - val_loss: 2.7806\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 2.7014 - val_loss: 2.7806\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.7010 - val_loss: 2.7793\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6990 - val_loss: 2.7796\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6984 - val_loss: 2.7781\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6967 - val_loss: 2.7786\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6962 - val_loss: 2.7774\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6949 - val_loss: 2.7779\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6946 - val_loss: 2.7778\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6939 - val_loss: 2.7780\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6943 - val_loss: 2.7812\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6956 - val_loss: 2.7801\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6972 - val_loss: 2.7869\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7001 - val_loss: 2.7862\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7059 - val_loss: 2.7988\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7123 - val_loss: 2.7980\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7229 - val_loss: 2.8097\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7262 - val_loss: 2.7995\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7291 - val_loss: 2.8073\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.7256 - val_loss: 2.7944\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 2.7227 - val_loss: 2.7976\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7149 - val_loss: 2.7867\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.7134 - val_loss: 2.7935\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7098 - val_loss: 2.7839\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7073 - val_loss: 2.7876\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7033 - val_loss: 2.7796\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 2.6979 - val_loss: 2.7804\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.6942 - val_loss: 2.7712\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6880 - val_loss: 2.7762\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6873 - val_loss: 2.7696\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6844 - val_loss: 2.7752\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6849 - val_loss: 2.7706\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6833 - val_loss: 2.7775\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6859 - val_loss: 2.7714\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6843 - val_loss: 2.7793\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6873 - val_loss: 2.7709\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6855 - val_loss: 2.7803\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6887 - val_loss: 2.7706\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6875 - val_loss: 2.7804\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6900 - val_loss: 2.7704\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6885 - val_loss: 2.7795\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6901 - val_loss: 2.7691\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6876 - val_loss: 2.7773\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6883 - val_loss: 2.7666\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6846 - val_loss: 2.7737\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6848 - val_loss: 2.7635\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6801 - val_loss: 2.7695\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6804 - val_loss: 2.7605\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6748 - val_loss: 2.7648\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6747 - val_loss: 2.7586\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6698 - val_loss: 2.7627\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6707 - val_loss: 2.7582\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6678 - val_loss: 2.7644\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6706 - val_loss: 2.7590\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6694 - val_loss: 2.7682\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6744 - val_loss: 2.7605\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6733 - val_loss: 2.7691\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6771 - val_loss: 2.7609\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6750 - val_loss: 2.7680\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6770 - val_loss: 2.7592\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6738 - val_loss: 2.7656\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6749 - val_loss: 2.7568\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6709 - val_loss: 2.7626\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6718 - val_loss: 2.7545\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6675 - val_loss: 2.7596\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6687 - val_loss: 2.7524\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6645 - val_loss: 2.7574\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6660 - val_loss: 2.7511\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6622 - val_loss: 2.7563\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6642 - val_loss: 2.7505\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6610 - val_loss: 2.7561\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6638 - val_loss: 2.7507\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6612 - val_loss: 2.7568\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6647 - val_loss: 2.7512\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6624 - val_loss: 2.7573\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6657 - val_loss: 2.7512\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6628 - val_loss: 2.7566\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6653 - val_loss: 2.7501\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6616 - val_loss: 2.7547\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6635 - val_loss: 2.7483\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6593 - val_loss: 2.7526\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6612 - val_loss: 2.7465\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6569 - val_loss: 2.7506\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6590 - val_loss: 2.7449\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6548 - val_loss: 2.7491\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6572 - val_loss: 2.7437\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6531 - val_loss: 2.7478\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6557 - val_loss: 2.7427\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.6517 - val_loss: 2.7466\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6543 - val_loss: 2.7418\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6503 - val_loss: 2.7455\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6530 - val_loss: 2.7409\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6489 - val_loss: 2.7443\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6516 - val_loss: 2.7399\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6475 - val_loss: 2.7431\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6502 - val_loss: 2.7389\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6460 - val_loss: 2.7419\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6488 - val_loss: 2.7380\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6446 - val_loss: 2.7407\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6473 - val_loss: 2.7370\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6431 - val_loss: 2.7394\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6459 - val_loss: 2.7360\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6416 - val_loss: 2.7382\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6444 - val_loss: 2.7351\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6401 - val_loss: 2.7369\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6430 - val_loss: 2.7341\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6386 - val_loss: 2.7357\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6415 - val_loss: 2.7332\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6371 - val_loss: 2.7344\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6400 - val_loss: 2.7322\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6356 - val_loss: 2.7332\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6384 - val_loss: 2.7312\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6339 - val_loss: 2.7318\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.6368 - val_loss: 2.7303\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6322 - val_loss: 2.7305\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6350 - val_loss: 2.7293\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.6303 - val_loss: 2.7290\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.6331 - val_loss: 2.7283\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6283 - val_loss: 2.7275\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6311 - val_loss: 2.7273\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6264 - val_loss: 2.7262\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6291 - val_loss: 2.7264\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6247 - val_loss: 2.7252\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6274 - val_loss: 2.7255\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6233 - val_loss: 2.7244\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6262 - val_loss: 2.7247\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6223 - val_loss: 2.7240\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6256 - val_loss: 2.7242\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6218 - val_loss: 2.7239\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6255 - val_loss: 2.7238\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6217 - val_loss: 2.7238\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6256 - val_loss: 2.7235\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6217 - val_loss: 2.7234\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6255 - val_loss: 2.7230\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.6211 - val_loss: 2.7228\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 2.6248 - val_loss: 2.7223\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6200 - val_loss: 2.7219\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6236 - val_loss: 2.7213\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6185 - val_loss: 2.7206\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6218 - val_loss: 2.7200\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6164 - val_loss: 2.7166\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6173 - val_loss: 2.7177\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6114 - val_loss: 2.7113\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6104 - val_loss: 2.7144\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6058 - val_loss: 2.7079\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6039 - val_loss: 2.7088\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5998 - val_loss: 2.7057\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5972 - val_loss: 2.7060\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5959 - val_loss: 2.7052\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5946 - val_loss: 2.7068\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5952 - val_loss: 2.7072\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5956 - val_loss: 2.7085\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5964 - val_loss: 2.7090\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5986 - val_loss: 2.7124\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6002 - val_loss: 2.7121\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6050 - val_loss: 2.7159\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6057 - val_loss: 2.7160\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6116 - val_loss: 2.7193\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 2.6114 - val_loss: 2.7258\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.6227 - val_loss: 2.7258\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.6206 - val_loss: 2.7300\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6287 - val_loss: 2.7272\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6231 - val_loss: 2.7275\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 2.6275 - val_loss: 2.7229\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6162 - val_loss: 2.7197\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6195 - val_loss: 2.7174\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.6084 - val_loss: 2.7140\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6113 - val_loss: 2.7134\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6031 - val_loss: 2.7119\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6069 - val_loss: 2.7105\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 2.5995 - val_loss: 2.7074\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.6012 - val_loss: 2.7066\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.5943 - val_loss: 2.6997\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.5926 - val_loss: 2.7004\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.5865 - val_loss: 2.6936\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.5840 - val_loss: 2.6942\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5803 - val_loss: 2.6902\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.5780 - val_loss: 2.6927\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5771 - val_loss: 2.6892\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5762 - val_loss: 2.6923\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5762 - val_loss: 2.6893\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.5761 - val_loss: 2.6936\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.5769 - val_loss: 2.6914\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5780 - val_loss: 2.6974\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5798 - val_loss: 2.6960\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5835 - val_loss: 2.7008\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5839 - val_loss: 2.7036\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.5917 - val_loss: 2.7076\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5927 - val_loss: 2.7139\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6048 - val_loss: 2.7161\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6026 - val_loss: 2.7240\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6173 - val_loss: 2.7269\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6132 - val_loss: 2.7275\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6218 - val_loss: 2.7244\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6099 - val_loss: 2.7209\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6148 - val_loss: 2.7163\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6006 - val_loss: 2.7126\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6062 - val_loss: 2.7109\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5927 - val_loss: 2.7022\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5945 - val_loss: 2.7042\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5852 - val_loss: 2.6974\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5864 - val_loss: 2.6958\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5776 - val_loss: 2.6912\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5772 - val_loss: 2.6904\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5704 - val_loss: 2.6842\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5685 - val_loss: 2.6853\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5642 - val_loss: 2.6786\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5623 - val_loss: 2.6815\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5604 - val_loss: 2.6757\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5594 - val_loss: 2.6795\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5584 - val_loss: 2.6750\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.5582 - val_loss: 2.6791\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5579 - val_loss: 2.6754\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5583 - val_loss: 2.6823\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5597 - val_loss: 2.6804\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5634 - val_loss: 2.6880\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5652 - val_loss: 2.6907\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5730 - val_loss: 2.6943\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5722 - val_loss: 2.6983\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5815 - val_loss: 2.7007\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5772 - val_loss: 2.7038\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5888 - val_loss: 2.7116\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5855 - val_loss: 2.7128\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5985 - val_loss: 2.7179\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5930 - val_loss: 2.7170\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6030 - val_loss: 2.7152\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5916 - val_loss: 2.7113\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5983 - val_loss: 2.7084\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.5843 - val_loss: 2.7039\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.5909 - val_loss: 2.7024\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.5764 - val_loss: 2.6948\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.5801 - val_loss: 2.6972\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5692 - val_loss: 2.6897\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5717 - val_loss: 2.6878\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.5613 - val_loss: 2.6811\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5603 - val_loss: 2.6812\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5531 - val_loss: 2.6732\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5510 - val_loss: 2.6754\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5462 - val_loss: 2.6658\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5443 - val_loss: 2.6702\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5419 - val_loss: 2.6623\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5408 - val_loss: 2.6672\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5401 - val_loss: 2.6628\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5408 - val_loss: 2.6680\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5414 - val_loss: 2.6682\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5449 - val_loss: 2.6761\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5464 - val_loss: 2.6766\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5540 - val_loss: 2.6847\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5541 - val_loss: 2.6877\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5636 - val_loss: 2.6911\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5594 - val_loss: 2.6942\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5701 - val_loss: 2.6975\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5654 - val_loss: 2.7040\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5814 - val_loss: 2.7136\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5790 - val_loss: 2.7118\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5908 - val_loss: 2.7139\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5794 - val_loss: 2.7051\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5847 - val_loss: 2.7061\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5718 - val_loss: 2.6982\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5778 - val_loss: 2.6965\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5627 - val_loss: 2.6867\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5665 - val_loss: 2.6895\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5529 - val_loss: 2.6756\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.5521 - val_loss: 2.6788\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5431 - val_loss: 2.6693\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5417 - val_loss: 2.6710\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5354 - val_loss: 2.6631\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5341 - val_loss: 2.6672\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5309 - val_loss: 2.6619\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5319 - val_loss: 2.6667\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5304 - val_loss: 2.6624\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5327 - val_loss: 2.6699\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 56.5981 - val_loss: 53.9958\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 55.1701 - val_loss: 51.7741\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 52.9485 - val_loss: 48.8860\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 50.0572 - val_loss: 45.2179\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 46.3810 - val_loss: 40.6832\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 41.8287 - val_loss: 35.3967\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 36.4973 - val_loss: 29.7419\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 30.7319 - val_loss: 24.4209\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 25.2286 - val_loss: 20.6643\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 21.1878 - val_loss: 18.7186\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 18.9409 - val_loss: 17.5588\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.5204 - val_loss: 16.7841\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 16.5556 - val_loss: 16.1855\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 15.8240 - val_loss: 15.6041\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 15.1591 - val_loss: 14.9218\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 14.4364 - val_loss: 14.1075\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 13.6090 - val_loss: 13.1832\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.6871 - val_loss: 12.0423\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.5806 - val_loss: 10.8797\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 10.4711 - val_loss: 10.1166\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.6935 - val_loss: 9.5851\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.1574 - val_loss: 9.2407\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.8665 - val_loss: 9.0214\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.7137 - val_loss: 8.8696\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.5957 - val_loss: 8.7584\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.4962 - val_loss: 8.6710\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.4117 - val_loss: 8.5940\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.3337 - val_loss: 8.5231\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2584 - val_loss: 8.4592\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.1887 - val_loss: 8.4012\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.1260 - val_loss: 8.3462\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.0679 - val_loss: 8.2909\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0113 - val_loss: 8.2338\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.9544 - val_loss: 8.1751\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.8967 - val_loss: 8.1154\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.8384 - val_loss: 8.0550\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.7796 - val_loss: 7.9942\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.7203 - val_loss: 7.9329\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.6604 - val_loss: 7.8710\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.5998 - val_loss: 7.8082\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.5385 - val_loss: 7.7443\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.4762 - val_loss: 7.6792\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.4128 - val_loss: 7.6127\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.3482 - val_loss: 7.5448\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.2822 - val_loss: 7.4754\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.2148 - val_loss: 7.4047\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.1459 - val_loss: 7.3326\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.0754 - val_loss: 7.2589\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.0033 - val_loss: 7.1833\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.9295 - val_loss: 7.1062\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.8540 - val_loss: 7.0275\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.7766 - val_loss: 6.9468\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.6973 - val_loss: 6.8643\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6.6163 - val_loss: 6.7799\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.5335 - val_loss: 6.6937\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.4493 - val_loss: 6.6063\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.3638 - val_loss: 6.5156\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2769 - val_loss: 6.4243\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 6.1890 - val_loss: 6.3322\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.1001 - val_loss: 6.2387\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.0104 - val_loss: 6.1448\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.9202 - val_loss: 6.0506\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.8294 - val_loss: 5.9554\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 5.7381 - val_loss: 5.8588\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 5.6465 - val_loss: 5.7619\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 5.5543 - val_loss: 5.6648\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.4618 - val_loss: 5.5672\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 5.3689 - val_loss: 5.4693\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.2755 - val_loss: 5.3713\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.1817 - val_loss: 5.2729\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 5.0873 - val_loss: 5.1739\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.9925 - val_loss: 5.0743\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.8973 - val_loss: 4.9745\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.8016 - val_loss: 4.8750\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.7066 - val_loss: 4.7778\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.6142 - val_loss: 4.6867\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.5264 - val_loss: 4.6011\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.4432 - val_loss: 4.5184\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.3641 - val_loss: 4.4419\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.2885 - val_loss: 4.3681\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.2170 - val_loss: 4.2985\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.1503 - val_loss: 4.2341\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.0890 - val_loss: 4.1715\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.0335 - val_loss: 4.1144\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.9840 - val_loss: 4.0582\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.9392 - val_loss: 4.0082\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.8982 - val_loss: 3.9591\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.8602 - val_loss: 3.9176\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.8252 - val_loss: 3.8736\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.7921 - val_loss: 3.8376\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.7613 - val_loss: 3.8010\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.7326 - val_loss: 3.7682\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.7055 - val_loss: 3.7373\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.6797 - val_loss: 3.7077\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.6553 - val_loss: 3.6810\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.6320 - val_loss: 3.6567\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6100 - val_loss: 3.6329\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.5890 - val_loss: 3.6118\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.5690 - val_loss: 3.5915\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5499 - val_loss: 3.5726\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.5315 - val_loss: 3.5505\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.5138 - val_loss: 3.5368\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4967 - val_loss: 3.5143\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.4800 - val_loss: 3.5032\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.4639 - val_loss: 3.4818\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.4481 - val_loss: 3.4682\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.4330 - val_loss: 3.4531\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.4183 - val_loss: 3.4368\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4042 - val_loss: 3.4233\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.3904 - val_loss: 3.4082\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3772 - val_loss: 3.3956\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.3644 - val_loss: 3.3825\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.3522 - val_loss: 3.3708\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3405 - val_loss: 3.3576\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3292 - val_loss: 3.3456\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3182 - val_loss: 3.3337\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.3075 - val_loss: 3.3229\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.2972 - val_loss: 3.3117\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2873 - val_loss: 3.3008\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.2776 - val_loss: 3.2900\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2682 - val_loss: 3.2794\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2590 - val_loss: 3.2691\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.2501 - val_loss: 3.2588\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2413 - val_loss: 3.2491\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.2328 - val_loss: 3.2397\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2245 - val_loss: 3.2309\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2163 - val_loss: 3.2219\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2084 - val_loss: 3.2138\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2008 - val_loss: 3.2032\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1933 - val_loss: 3.1971\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1861 - val_loss: 3.1860\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1790 - val_loss: 3.1811\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1720 - val_loss: 3.1695\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1651 - val_loss: 3.1643\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1582 - val_loss: 3.1533\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.1515 - val_loss: 3.1480\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1448 - val_loss: 3.1383\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1383 - val_loss: 3.1329\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1318 - val_loss: 3.1241\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.1254 - val_loss: 3.1188\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1190 - val_loss: 3.1106\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1128 - val_loss: 3.1056\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1066 - val_loss: 3.0974\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1006 - val_loss: 3.0938\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0948 - val_loss: 3.0845\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0893 - val_loss: 3.0872\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0846 - val_loss: 3.0689\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0806 - val_loss: 3.0881\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0781 - val_loss: 3.0558\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0781 - val_loss: 3.0980\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0785 - val_loss: 3.0504\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0796 - val_loss: 3.1078\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0818 - val_loss: 3.0467\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0842 - val_loss: 3.1142\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0844 - val_loss: 3.0430\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0832 - val_loss: 3.1136\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0821 - val_loss: 3.0357\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0777 - val_loss: 3.1062\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0750 - val_loss: 3.0280\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0696 - val_loss: 3.0978\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0669 - val_loss: 3.0203\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0616 - val_loss: 3.0922\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0609 - val_loss: 3.0149\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.0567 - val_loss: 3.0889\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0566 - val_loss: 3.0100\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0524 - val_loss: 3.0843\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0514 - val_loss: 3.0043\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0468 - val_loss: 3.0791\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.0459 - val_loss: 2.9991\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0425 - val_loss: 3.0765\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0429 - val_loss: 2.9970\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0422 - val_loss: 3.0754\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0409 - val_loss: 2.9931\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0382 - val_loss: 3.0690\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0347 - val_loss: 2.9868\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0305 - val_loss: 3.0602\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0265 - val_loss: 2.9802\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0225 - val_loss: 3.0523\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0191 - val_loss: 2.9742\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0154 - val_loss: 3.0452\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0125 - val_loss: 2.9686\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0087 - val_loss: 3.0377\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0057 - val_loss: 2.9629\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0017 - val_loss: 3.0291\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9977 - val_loss: 2.9566\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9939 - val_loss: 3.0211\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9903 - val_loss: 2.9513\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9875 - val_loss: 3.0151\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9845 - val_loss: 2.9469\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9821 - val_loss: 3.0101\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9796 - val_loss: 2.9428\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9770 - val_loss: 3.0052\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9746 - val_loss: 2.9387\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9717 - val_loss: 2.9998\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9692 - val_loss: 2.9344\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9661 - val_loss: 2.9941\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9635 - val_loss: 2.9299\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9603 - val_loss: 2.9883\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9577 - val_loss: 2.9256\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9546 - val_loss: 2.9828\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9522 - val_loss: 2.9214\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9493 - val_loss: 2.9779\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9473 - val_loss: 2.9177\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9445 - val_loss: 2.9744\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9435 - val_loss: 2.9145\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9408 - val_loss: 2.9723\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9408 - val_loss: 2.9116\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9375 - val_loss: 2.9684\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9369 - val_loss: 2.9074\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9323 - val_loss: 2.9620\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9306 - val_loss: 2.9014\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9249 - val_loss: 2.9518\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9217 - val_loss: 2.8942\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9154 - val_loss: 2.9413\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9123 - val_loss: 2.8859\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9057 - val_loss: 2.9313\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9031 - val_loss: 2.8804\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8983 - val_loss: 2.9263\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8980 - val_loss: 2.8774\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8950 - val_loss: 2.9274\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8976 - val_loss: 2.8774\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8953 - val_loss: 2.9311\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9001 - val_loss: 2.8793\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8980 - val_loss: 2.9365\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9042 - val_loss: 2.8798\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8988 - val_loss: 2.9339\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9013 - val_loss: 2.8759\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8949 - val_loss: 2.9278\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8952 - val_loss: 2.8711\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8880 - val_loss: 2.9183\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8867 - val_loss: 2.8644\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8786 - val_loss: 2.9073\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8770 - val_loss: 2.8544\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8678 - val_loss: 2.8986\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8684 - val_loss: 2.8510\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8630 - val_loss: 2.8965\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8659 - val_loss: 2.8491\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8607 - val_loss: 2.8951\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8641 - val_loss: 2.8504\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8639 - val_loss: 2.9019\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8695 - val_loss: 2.8505\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8642 - val_loss: 2.8985\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8659 - val_loss: 2.8460\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8580 - val_loss: 2.8919\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8597 - val_loss: 2.8416\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8529 - val_loss: 2.8847\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8530 - val_loss: 2.8340\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8441 - val_loss: 2.8757\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8441 - val_loss: 2.8288\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8371 - val_loss: 2.8709\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8394 - val_loss: 2.8260\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8342 - val_loss: 2.8712\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8391 - val_loss: 2.8283\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8372 - val_loss: 2.8763\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8440 - val_loss: 2.8327\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8431 - val_loss: 2.8799\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8477 - val_loss: 2.8309\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8406 - val_loss: 2.8723\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8401 - val_loss: 2.8227\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8298 - val_loss: 2.8592\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8278 - val_loss: 2.8108\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8147 - val_loss: 2.8426\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8124 - val_loss: 2.8042\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8050 - val_loss: 2.8387\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8085 - val_loss: 2.8029\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 2.8044 - val_loss: 2.8408\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 2.8097 - val_loss: 2.8031\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.8049 - val_loss: 2.8403\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8087 - val_loss: 2.8025\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 2.8037 - val_loss: 2.8407\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 2.8091 - val_loss: 2.8055\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.8073 - val_loss: 2.8460\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8141 - val_loss: 2.8088\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.8116 - val_loss: 2.8457\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8136 - val_loss: 2.8032\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8041 - val_loss: 2.8354\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8039 - val_loss: 2.7955\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7935 - val_loss: 2.8235\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7925 - val_loss: 2.7856\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7814 - val_loss: 2.8147\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7833 - val_loss: 2.7825\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7769 - val_loss: 2.8158\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7839 - val_loss: 2.7858\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7813 - val_loss: 2.8215\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7893 - val_loss: 2.7900\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7870 - val_loss: 2.8253\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7928 - val_loss: 2.7898\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7864 - val_loss: 2.8214\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7886 - val_loss: 2.7853\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7799 - val_loss: 2.8146\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7816 - val_loss: 2.7807\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7736 - val_loss: 2.8092\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7761 - val_loss: 2.7773\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7692 - val_loss: 2.8062\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7730 - val_loss: 2.7756\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7672 - val_loss: 2.8047\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7713 - val_loss: 2.7746\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7658 - val_loss: 2.8032\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7697 - val_loss: 2.7732\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7639 - val_loss: 2.8011\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7672 - val_loss: 2.7713\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7612 - val_loss: 2.7982\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7641 - val_loss: 2.7686\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7576 - val_loss: 2.7945\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7601 - val_loss: 2.7636\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7514 - val_loss: 2.7888\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7537 - val_loss: 2.7596\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.7456 - val_loss: 2.7845\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7492 - val_loss: 2.7576\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.7426 - val_loss: 2.7826\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7470 - val_loss: 2.7567\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7411 - val_loss: 2.7815\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7456 - val_loss: 2.7559\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7397 - val_loss: 2.7796\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7436 - val_loss: 2.7548\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7374 - val_loss: 2.7766\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7404 - val_loss: 2.7528\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7346 - val_loss: 2.7737\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7372 - val_loss: 2.7510\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7321 - val_loss: 2.7710\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7344 - val_loss: 2.7497\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7301 - val_loss: 2.7691\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7322 - val_loss: 2.7489\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7286 - val_loss: 2.7676\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7304 - val_loss: 2.7484\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7273 - val_loss: 2.7665\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7290 - val_loss: 2.7480\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7259 - val_loss: 2.7657\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7277 - val_loss: 2.7475\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7249 - val_loss: 2.7654\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7270 - val_loss: 2.7475\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7247 - val_loss: 2.7661\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.7274 - val_loss: 2.7488\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7259 - val_loss: 2.7676\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.7289 - val_loss: 2.7501\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7271 - val_loss: 2.7671\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7283 - val_loss: 2.7482\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7245 - val_loss: 2.7635\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7242 - val_loss: 2.7443\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7194 - val_loss: 2.7591\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7190 - val_loss: 2.7415\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7155 - val_loss: 2.7566\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7159 - val_loss: 2.7401\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.7135 - val_loss: 2.7556\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7145 - val_loss: 2.7399\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7128 - val_loss: 2.7552\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7139 - val_loss: 2.7398\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7122 - val_loss: 2.7544\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7128 - val_loss: 2.7391\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7108 - val_loss: 2.7529\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7109 - val_loss: 2.7379\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7089 - val_loss: 2.7513\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7088 - val_loss: 2.7368\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7071 - val_loss: 2.7499\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7070 - val_loss: 2.7359\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7055 - val_loss: 2.7486\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7053 - val_loss: 2.7350\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7040 - val_loss: 2.7473\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7035 - val_loss: 2.7340\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7023 - val_loss: 2.7460\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7017 - val_loss: 2.7331\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7007 - val_loss: 2.7447\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6999 - val_loss: 2.7322\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6992 - val_loss: 2.7434\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6982 - val_loss: 2.7313\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6976 - val_loss: 2.7422\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6965 - val_loss: 2.7304\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6961 - val_loss: 2.7410\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.6949 - val_loss: 2.7296\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6946 - val_loss: 2.7399\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6934 - val_loss: 2.7288\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6932 - val_loss: 2.7388\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6919 - val_loss: 2.7280\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6918 - val_loss: 2.7378\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6905 - val_loss: 2.7272\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6904 - val_loss: 2.7367\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6890 - val_loss: 2.7264\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6890 - val_loss: 2.7356\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6875 - val_loss: 2.7255\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6876 - val_loss: 2.7344\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6858 - val_loss: 2.7245\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6860 - val_loss: 2.7327\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 2.6837 - val_loss: 2.7232\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6840 - val_loss: 2.7300\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6808 - val_loss: 2.7217\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6814 - val_loss: 2.7283\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6785 - val_loss: 2.7205\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6797 - val_loss: 2.7291\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6783 - val_loss: 2.7206\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6801 - val_loss: 2.7294\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6787 - val_loss: 2.7208\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6799 - val_loss: 2.7276\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6773 - val_loss: 2.7196\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6777 - val_loss: 2.7247\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6736 - val_loss: 2.7170\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6743 - val_loss: 2.7229\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6705 - val_loss: 2.7155\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6723 - val_loss: 2.7218\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6690 - val_loss: 2.7152\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 2.6715 - val_loss: 2.7209\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6681 - val_loss: 2.7147\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6705 - val_loss: 2.7221\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6688 - val_loss: 2.7153\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6718 - val_loss: 2.7226\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6693 - val_loss: 2.7146\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6710 - val_loss: 2.7211\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6674 - val_loss: 2.7134\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6688 - val_loss: 2.7171\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6633 - val_loss: 2.7111\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6646 - val_loss: 2.7146\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6595 - val_loss: 2.7094\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6620 - val_loss: 2.7135\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6575 - val_loss: 2.7088\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6609 - val_loss: 2.7131\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6568 - val_loss: 2.7086\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6606 - val_loss: 2.7128\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6564 - val_loss: 2.7082\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6601 - val_loss: 2.7122\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6556 - val_loss: 2.7075\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6590 - val_loss: 2.7112\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6541 - val_loss: 2.7064\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6574 - val_loss: 2.7098\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6522 - val_loss: 2.7051\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6555 - val_loss: 2.7085\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6502 - val_loss: 2.7039\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6538 - val_loss: 2.7074\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6486 - val_loss: 2.7030\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6525 - val_loss: 2.7065\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6472 - val_loss: 2.7018\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6511 - val_loss: 2.7059\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6459 - val_loss: 2.7003\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6492 - val_loss: 2.7041\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6436 - val_loss: 2.6987\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6457 - val_loss: 2.7012\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6399 - val_loss: 2.6963\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6422 - val_loss: 2.6993\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6369 - val_loss: 2.6939\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6396 - val_loss: 2.6982\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6350 - val_loss: 2.6930\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6384 - val_loss: 2.6978\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6342 - val_loss: 2.6939\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6390 - val_loss: 2.6984\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6349 - val_loss: 2.6957\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6411 - val_loss: 2.6997\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6367 - val_loss: 2.6965\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6422 - val_loss: 2.6995\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6365 - val_loss: 2.6955\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6407 - val_loss: 2.6979\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6343 - val_loss: 2.6932\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6377 - val_loss: 2.6958\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6312 - val_loss: 2.6900\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6335 - val_loss: 2.6932\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6274 - val_loss: 2.6871\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6294 - val_loss: 2.6911\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6243 - val_loss: 2.6857\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6271 - val_loss: 2.6904\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6229 - val_loss: 2.6855\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6266 - val_loss: 2.6905\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6226 - val_loss: 2.6861\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6272 - val_loss: 2.6909\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6230 - val_loss: 2.6897\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6304 - val_loss: 2.6930\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6260 - val_loss: 2.6908\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6324 - val_loss: 2.6928\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6259 - val_loss: 2.6894\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6303 - val_loss: 2.6908\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6230 - val_loss: 2.6846\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6249 - val_loss: 2.6874\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6178 - val_loss: 2.6813\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6197 - val_loss: 2.6853\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.6145 - val_loss: 2.6802\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6177 - val_loss: 2.6850\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6136 - val_loss: 2.6806\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6178 - val_loss: 2.6852\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6137 - val_loss: 2.6811\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6182 - val_loss: 2.6852\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6135 - val_loss: 2.6814\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6181 - val_loss: 2.6848\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6129 - val_loss: 2.6805\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6170 - val_loss: 2.6838\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6114 - val_loss: 2.6793\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6151 - val_loss: 2.6827\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6096 - val_loss: 2.6785\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6136 - val_loss: 2.6819\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6082 - val_loss: 2.6779\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6124 - val_loss: 2.6813\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6071 - val_loss: 2.6774\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6114 - val_loss: 2.6808\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6061 - val_loss: 2.6769\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6105 - val_loss: 2.6803\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6051 - val_loss: 2.6764\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6095 - val_loss: 2.6797\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6040 - val_loss: 2.6759\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6084 - val_loss: 2.6791\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6029 - val_loss: 2.6753\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6074 - val_loss: 2.6786\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6018 - val_loss: 2.6748\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6063 - val_loss: 2.6780\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6007 - val_loss: 2.6743\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6052 - val_loss: 2.6775\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5996 - val_loss: 2.6738\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6042 - val_loss: 2.6770\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5986 - val_loss: 2.6733\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6032 - val_loss: 2.6765\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5975 - val_loss: 2.6728\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6022 - val_loss: 2.6761\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5965 - val_loss: 2.6723\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6012 - val_loss: 2.6756\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5955 - val_loss: 2.6718\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6002 - val_loss: 2.6751\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5944 - val_loss: 2.6713\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5992 - val_loss: 2.6747\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5934 - val_loss: 2.6702\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5975 - val_loss: 2.6738\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5917 - val_loss: 2.6694\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5959 - val_loss: 2.6731\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5904 - val_loss: 2.6690\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5941 - val_loss: 2.6720\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5887 - val_loss: 2.6685\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5929 - val_loss: 2.6719\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5879 - val_loss: 2.6673\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5912 - val_loss: 2.6711\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5861 - val_loss: 2.6669\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5904 - val_loss: 2.6710\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5857 - val_loss: 2.6678\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5905 - val_loss: 2.6713\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5857 - val_loss: 2.6677\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5902 - val_loss: 2.6711\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5849 - val_loss: 2.6661\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5879 - val_loss: 2.6696\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5824 - val_loss: 2.6646\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5849 - val_loss: 2.6680\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5799 - val_loss: 2.6638\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5826 - val_loss: 2.6670\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5782 - val_loss: 2.6634\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5815 - val_loss: 2.6670\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5776 - val_loss: 2.6633\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5815 - val_loss: 2.6677\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5779 - val_loss: 2.6644\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5828 - val_loss: 2.6685\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5788 - val_loss: 2.6654\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5835 - val_loss: 2.6686\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5787 - val_loss: 2.6649\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5827 - val_loss: 2.6680\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5775 - val_loss: 2.6635\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5805 - val_loss: 2.6668\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5753 - val_loss: 2.6619\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.5776 - val_loss: 2.6653\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5729 - val_loss: 2.6608\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5753 - val_loss: 2.6645\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5712 - val_loss: 2.6604\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5742 - val_loss: 2.6645\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5706 - val_loss: 2.6610\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5751 - val_loss: 2.6657\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5718 - val_loss: 2.6623\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5764 - val_loss: 2.6663\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5724 - val_loss: 2.6626\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5766 - val_loss: 2.6660\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5718 - val_loss: 2.6620\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5754 - val_loss: 2.6652\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5703 - val_loss: 2.6603\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5726 - val_loss: 2.6635\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5673 - val_loss: 2.6586\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5692 - val_loss: 2.6622\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5648 - val_loss: 2.6578\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5661 - val_loss: 2.6609\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5623 - val_loss: 2.6581\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5646 - val_loss: 2.6612\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5615 - val_loss: 2.6582\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5659 - val_loss: 2.6634\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5640 - val_loss: 2.6596\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5694 - val_loss: 2.6647\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5665 - val_loss: 2.6614\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5711 - val_loss: 2.6641\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5658 - val_loss: 2.6614\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5682 - val_loss: 2.6630\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5626 - val_loss: 2.6586\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5638 - val_loss: 2.6611\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5590 - val_loss: 2.6566\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5608 - val_loss: 2.6602\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5573 - val_loss: 2.6573\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5596 - val_loss: 2.6601\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5564 - val_loss: 2.6573\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5591 - val_loss: 2.6606\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5561 - val_loss: 2.6571\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5593 - val_loss: 2.6611\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5565 - val_loss: 2.6577\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5603 - val_loss: 2.6618\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5570 - val_loss: 2.6586\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5607 - val_loss: 2.6621\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5570 - val_loss: 2.6590\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5602 - val_loss: 2.6621\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5562 - val_loss: 2.6577\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5579 - val_loss: 2.6606\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5535 - val_loss: 2.6561\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5549 - val_loss: 2.6594\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5513 - val_loss: 2.6558\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5533 - val_loss: 2.6592\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5503 - val_loss: 2.6558\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5527 - val_loss: 2.6596\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 56.2695 - val_loss: 55.1217\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 54.8092 - val_loss: 52.7837\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 52.4771 - val_loss: 49.6934\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 49.3956 - val_loss: 45.7958\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 45.5087 - val_loss: 41.0570\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 40.7825 - val_loss: 35.5355\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 35.2801 - val_loss: 29.5951\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 29.3829 - val_loss: 24.2111\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 24.0762 - val_loss: 20.6342\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 20.6665 - val_loss: 18.4327\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 18.5224 - val_loss: 17.0123\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.1133 - val_loss: 16.0769\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 16.1798 - val_loss: 15.4044\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 15.5087 - val_loss: 14.8505\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 14.9569 - val_loss: 14.2944\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 14.4009 - val_loss: 13.5987\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.7041 - val_loss: 12.6482\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 12.7607 - val_loss: 11.5333\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 11.6729 - val_loss: 10.6730\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 10.8299 - val_loss: 10.0112\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 10.1827 - val_loss: 9.4989\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.6601 - val_loss: 9.1552\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.2771 - val_loss: 8.9000\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.9937 - val_loss: 8.7061\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.7932 - val_loss: 8.5889\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.6762 - val_loss: 8.4889\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.5758 - val_loss: 8.4029\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.4898 - val_loss: 8.3278\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.4106 - val_loss: 8.2556\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3294 - val_loss: 8.1844\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.2498 - val_loss: 8.1110\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.1704 - val_loss: 8.0369\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0917 - val_loss: 7.9625\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.0128 - val_loss: 7.8869\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.9328 - val_loss: 7.8084\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.8509 - val_loss: 7.7260\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.7662 - val_loss: 7.6400\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.6787 - val_loss: 7.5510\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.5886 - val_loss: 7.4597\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.4964 - val_loss: 7.3669\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.4027 - val_loss: 7.2732\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.3080 - val_loss: 7.1788\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.2123 - val_loss: 7.0836\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.1160 - val_loss: 6.9878\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.0190 - val_loss: 6.8913\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.9214 - val_loss: 6.7939\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.8232 - val_loss: 6.6958\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.7243 - val_loss: 6.5972\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.6248 - val_loss: 6.4980\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.5246 - val_loss: 6.3980\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 6.4237 - val_loss: 6.2974\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 6.3222 - val_loss: 6.1962\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 6.2198 - val_loss: 6.0943\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.1165 - val_loss: 5.9915\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 6.0121 - val_loss: 5.8882\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.9069 - val_loss: 5.7846\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.8012 - val_loss: 5.6811\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.6956 - val_loss: 5.5767\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.5907 - val_loss: 5.4733\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.4867 - val_loss: 5.3713\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.3838 - val_loss: 5.2706\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.2820 - val_loss: 5.1704\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.1814 - val_loss: 5.0711\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.0817 - val_loss: 4.9731\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.9826 - val_loss: 4.8757\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.8840 - val_loss: 4.7778\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.7853 - val_loss: 4.6793\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.6868 - val_loss: 4.5818\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4.5906 - val_loss: 4.4883\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.4995 - val_loss: 4.4011\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.4145 - val_loss: 4.3199\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.3360 - val_loss: 4.2472\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.2635 - val_loss: 4.1834\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.1964 - val_loss: 4.1255\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 4.1349 - val_loss: 4.0724\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.0787 - val_loss: 4.0236\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.0274 - val_loss: 3.9777\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.9798 - val_loss: 3.9355\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.9354 - val_loss: 3.8948\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.8934 - val_loss: 3.8578\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.8539 - val_loss: 3.8218\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.8166 - val_loss: 3.7865\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7816 - val_loss: 3.7530\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.7495 - val_loss: 3.7207\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.7190 - val_loss: 3.6920\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6900 - val_loss: 3.6649\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.6624 - val_loss: 3.6377\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.6364 - val_loss: 3.6110\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.6116 - val_loss: 3.5901\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.5883 - val_loss: 3.5655\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5665 - val_loss: 3.5452\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5459 - val_loss: 3.5258\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.5264 - val_loss: 3.5057\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.5075 - val_loss: 3.4881\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.4893 - val_loss: 3.4707\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.4717 - val_loss: 3.4528\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.4548 - val_loss: 3.4355\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.4385 - val_loss: 3.4203\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.4229 - val_loss: 3.4053\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.4077 - val_loss: 3.3915\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3930 - val_loss: 3.3783\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.3787 - val_loss: 3.3645\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.3648 - val_loss: 3.3513\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.3512 - val_loss: 3.3391\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.3379 - val_loss: 3.3269\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.3250 - val_loss: 3.3147\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.3123 - val_loss: 3.3030\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.3000 - val_loss: 3.2919\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.2879 - val_loss: 3.2814\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 3.2760 - val_loss: 3.2714\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2645 - val_loss: 3.2621\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 3.2533 - val_loss: 3.2526\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 3.2425 - val_loss: 3.2437\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.2319 - val_loss: 3.2351\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.2217 - val_loss: 3.2270\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.2116 - val_loss: 3.2192\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.2018 - val_loss: 3.2115\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1922 - val_loss: 3.2038\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1828 - val_loss: 3.1963\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1735 - val_loss: 3.1890\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.1643 - val_loss: 3.1816\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1554 - val_loss: 3.1744\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1466 - val_loss: 3.1673\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1380 - val_loss: 3.1603\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1295 - val_loss: 3.1537\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1213 - val_loss: 3.1477\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1133 - val_loss: 3.1416\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1054 - val_loss: 3.1356\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0978 - val_loss: 3.1296\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0904 - val_loss: 3.1237\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0832 - val_loss: 3.1181\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0762 - val_loss: 3.1129\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0695 - val_loss: 3.1079\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0630 - val_loss: 3.1030\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0566 - val_loss: 3.0984\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0505 - val_loss: 3.0939\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0446 - val_loss: 3.0895\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0388 - val_loss: 3.0852\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0331 - val_loss: 3.0810\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0276 - val_loss: 3.0769\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0222 - val_loss: 3.0725\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0169 - val_loss: 3.0683\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0118 - val_loss: 3.0644\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0068 - val_loss: 3.0608\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.0019 - val_loss: 3.0572\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.9971 - val_loss: 3.0538\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9923 - val_loss: 3.0505\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9877 - val_loss: 3.0473\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9831 - val_loss: 3.0442\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.9787 - val_loss: 3.0411\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9743 - val_loss: 3.0381\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9701 - val_loss: 3.0352\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9660 - val_loss: 3.0325\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9619 - val_loss: 3.0300\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9580 - val_loss: 3.0277\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.9541 - val_loss: 3.0254\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9503 - val_loss: 3.0226\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.9467 - val_loss: 3.0207\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9431 - val_loss: 3.0185\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9396 - val_loss: 3.0161\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.9362 - val_loss: 3.0139\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.9329 - val_loss: 3.0117\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9296 - val_loss: 3.0095\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 2.9263 - val_loss: 3.0073\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9231 - val_loss: 3.0052\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9200 - val_loss: 3.0031\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9169 - val_loss: 3.0009\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9138 - val_loss: 2.9990\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9108 - val_loss: 2.9971\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9079 - val_loss: 2.9952\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9049 - val_loss: 2.9933\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9021 - val_loss: 2.9915\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8992 - val_loss: 2.9897\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8965 - val_loss: 2.9879\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8937 - val_loss: 2.9862\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8910 - val_loss: 2.9846\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8883 - val_loss: 2.9829\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.8856 - val_loss: 2.9812\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8830 - val_loss: 2.9796\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8805 - val_loss: 2.9778\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.8779 - val_loss: 2.9768\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 2.8754 - val_loss: 2.9737\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8730 - val_loss: 2.9742\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8706 - val_loss: 2.9711\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.8682 - val_loss: 2.9713\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8658 - val_loss: 2.9684\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8633 - val_loss: 2.9687\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.8610 - val_loss: 2.9659\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.8586 - val_loss: 2.9658\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8562 - val_loss: 2.9636\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8539 - val_loss: 2.9631\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.8517 - val_loss: 2.9613\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8494 - val_loss: 2.9607\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8472 - val_loss: 2.9592\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8449 - val_loss: 2.9583\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8427 - val_loss: 2.9572\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8406 - val_loss: 2.9562\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8384 - val_loss: 2.9551\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8363 - val_loss: 2.9540\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8342 - val_loss: 2.9530\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8321 - val_loss: 2.9519\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8300 - val_loss: 2.9509\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8280 - val_loss: 2.9498\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8259 - val_loss: 2.9488\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8239 - val_loss: 2.9477\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 2.8219 - val_loss: 2.9467\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8200 - val_loss: 2.9457\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8180 - val_loss: 2.9447\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8161 - val_loss: 2.9437\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8142 - val_loss: 2.9427\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8123 - val_loss: 2.9417\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8104 - val_loss: 2.9408\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8085 - val_loss: 2.9399\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8066 - val_loss: 2.9390\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8048 - val_loss: 2.9382\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.8029 - val_loss: 2.9374\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8011 - val_loss: 2.9366\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7993 - val_loss: 2.9358\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7975 - val_loss: 2.9349\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.7957 - val_loss: 2.9342\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7939 - val_loss: 2.9333\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.7922 - val_loss: 2.9327\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7904 - val_loss: 2.9316\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 2.7887 - val_loss: 2.9312\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.7869 - val_loss: 2.9297\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7852 - val_loss: 2.9295\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.7836 - val_loss: 2.9286\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7821 - val_loss: 2.9275\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7808 - val_loss: 2.9285\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7799 - val_loss: 2.9268\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7783 - val_loss: 2.9251\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7772 - val_loss: 2.9276\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7755 - val_loss: 2.9219\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.7743 - val_loss: 2.9271\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7726 - val_loss: 2.9207\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.7712 - val_loss: 2.9249\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7695 - val_loss: 2.9196\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7678 - val_loss: 2.9232\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7662 - val_loss: 2.9182\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 2.7644 - val_loss: 2.9206\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7626 - val_loss: 2.9177\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7610 - val_loss: 2.9187\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7594 - val_loss: 2.9173\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7578 - val_loss: 2.9157\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7564 - val_loss: 2.9160\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7549 - val_loss: 2.9148\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7535 - val_loss: 2.9141\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7520 - val_loss: 2.9152\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7505 - val_loss: 2.9122\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7492 - val_loss: 2.9145\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.7478 - val_loss: 2.9113\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.7465 - val_loss: 2.9127\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7451 - val_loss: 2.9104\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7440 - val_loss: 2.9115\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7429 - val_loss: 2.9082\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.7424 - val_loss: 2.9132\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.7418 - val_loss: 2.9066\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2.7409 - val_loss: 2.9121\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7396 - val_loss: 2.9052\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7380 - val_loss: 2.9095\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7364 - val_loss: 2.9042\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7350 - val_loss: 2.9095\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7334 - val_loss: 2.9022\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7323 - val_loss: 2.9101\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7308 - val_loss: 2.8997\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7295 - val_loss: 2.9080\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7280 - val_loss: 2.8987\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7269 - val_loss: 2.9066\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7257 - val_loss: 2.8975\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7249 - val_loss: 2.9058\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7239 - val_loss: 2.8971\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7229 - val_loss: 2.9053\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7218 - val_loss: 2.8958\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7206 - val_loss: 2.9047\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7193 - val_loss: 2.8939\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7182 - val_loss: 2.9041\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7169 - val_loss: 2.8924\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7159 - val_loss: 2.9029\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7148 - val_loss: 2.8912\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7139 - val_loss: 2.9024\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7129 - val_loss: 2.8900\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7120 - val_loss: 2.9022\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7110 - val_loss: 2.8887\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7101 - val_loss: 2.9018\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.7091 - val_loss: 2.8872\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7080 - val_loss: 2.9011\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7070 - val_loss: 2.8859\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 2.7059 - val_loss: 2.9004\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7050 - val_loss: 2.8845\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7039 - val_loss: 2.8999\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7031 - val_loss: 2.8833\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7020 - val_loss: 2.9033\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.7024 - val_loss: 2.8819\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7013 - val_loss: 2.9046\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7016 - val_loss: 2.8806\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.6999 - val_loss: 2.9037\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6998 - val_loss: 2.8793\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6978 - val_loss: 2.9027\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6975 - val_loss: 2.8782\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6955 - val_loss: 2.9021\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6955 - val_loss: 2.8772\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6936 - val_loss: 2.9021\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6939 - val_loss: 2.8761\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6920 - val_loss: 2.9031\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6927 - val_loss: 2.8751\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6907 - val_loss: 2.9064\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6925 - val_loss: 2.8743\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.6899 - val_loss: 2.9072\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.6914 - val_loss: 2.8736\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6881 - val_loss: 2.9058\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.6892 - val_loss: 2.8725\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 2.6860 - val_loss: 2.9051\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.6871 - val_loss: 2.8715\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.6840 - val_loss: 2.9026\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6843 - val_loss: 2.8704\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.6815 - val_loss: 2.9001\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6813 - val_loss: 2.8692\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.6789 - val_loss: 2.8990\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.6789 - val_loss: 2.8683\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6770 - val_loss: 2.8992\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6773 - val_loss: 2.8677\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6756 - val_loss: 2.9010\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6767 - val_loss: 2.8673\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6749 - val_loss: 2.9031\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.6762 - val_loss: 2.8669\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6739 - val_loss: 2.9029\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6747 - val_loss: 2.8661\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6721 - val_loss: 2.9018\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6725 - val_loss: 2.8650\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6701 - val_loss: 2.9014\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6706 - val_loss: 2.8641\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6683 - val_loss: 2.9011\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6689 - val_loss: 2.8633\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6667 - val_loss: 2.9009\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.6673 - val_loss: 2.8626\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6651 - val_loss: 2.9006\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6657 - val_loss: 2.8618\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6635 - val_loss: 2.9003\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6640 - val_loss: 2.8609\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6619 - val_loss: 2.8999\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6624 - val_loss: 2.8601\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6604 - val_loss: 2.8996\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6607 - val_loss: 2.8593\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6588 - val_loss: 2.8992\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6591 - val_loss: 2.8585\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6573 - val_loss: 2.8989\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6576 - val_loss: 2.8576\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6558 - val_loss: 2.8986\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6560 - val_loss: 2.8568\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6544 - val_loss: 2.8983\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6545 - val_loss: 2.8560\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6529 - val_loss: 2.8980\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6529 - val_loss: 2.8551\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6515 - val_loss: 2.8978\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6513 - val_loss: 2.8542\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6500 - val_loss: 2.8975\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6498 - val_loss: 2.8533\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6487 - val_loss: 2.8971\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6483 - val_loss: 2.8525\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6474 - val_loss: 2.8965\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6473 - val_loss: 2.8523\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.6469 - val_loss: 2.8974\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6471 - val_loss: 2.8518\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6470 - val_loss: 2.8978\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6464 - val_loss: 2.8509\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6460 - val_loss: 2.8968\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.6447 - val_loss: 2.8499\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6443 - val_loss: 2.8961\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6420 - val_loss: 2.8478\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.6413 - val_loss: 2.8938\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6389 - val_loss: 2.8470\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.6388 - val_loss: 2.8928\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6371 - val_loss: 2.8464\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6378 - val_loss: 2.8933\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6362 - val_loss: 2.8457\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6374 - val_loss: 2.8933\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6353 - val_loss: 2.8450\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6365 - val_loss: 2.8925\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6339 - val_loss: 2.8442\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6351 - val_loss: 2.8917\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6323 - val_loss: 2.8434\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6338 - val_loss: 2.8910\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6308 - val_loss: 2.8426\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6326 - val_loss: 2.8905\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6295 - val_loss: 2.8418\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6315 - val_loss: 2.8899\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6282 - val_loss: 2.8411\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6305 - val_loss: 2.8894\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6271 - val_loss: 2.8406\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6296 - val_loss: 2.8888\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6262 - val_loss: 2.8403\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6290 - val_loss: 2.8882\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6258 - val_loss: 2.8409\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6297 - val_loss: 2.8907\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6277 - val_loss: 2.8416\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6318 - val_loss: 2.8937\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6289 - val_loss: 2.8410\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6335 - val_loss: 2.8978\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6307 - val_loss: 2.8405\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6337 - val_loss: 2.8973\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6294 - val_loss: 2.8396\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6312 - val_loss: 2.8919\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6250 - val_loss: 2.8383\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6271 - val_loss: 2.8881\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6208 - val_loss: 2.8359\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6214 - val_loss: 2.8799\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6139 - val_loss: 2.8322\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6138 - val_loss: 2.8732\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6078 - val_loss: 2.8295\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6089 - val_loss: 2.8709\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.6045 - val_loss: 2.8290\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.6067 - val_loss: 2.8721\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6036 - val_loss: 2.8283\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6071 - val_loss: 2.8746\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6042 - val_loss: 2.8278\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6087 - val_loss: 2.8768\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6058 - val_loss: 2.8294\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6125 - val_loss: 2.8907\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6138 - val_loss: 2.8350\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6245 - val_loss: 2.9026\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6249 - val_loss: 2.8425\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6326 - val_loss: 2.9004\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6263 - val_loss: 2.8429\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6290 - val_loss: 2.8919\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6189 - val_loss: 2.8375\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6196 - val_loss: 2.8834\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6104 - val_loss: 2.8338\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6119 - val_loss: 2.8798\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6056 - val_loss: 2.8304\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 2.6075 - val_loss: 2.8784\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.6020 - val_loss: 2.8266\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6037 - val_loss: 2.8746\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.5975 - val_loss: 2.8233\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5987 - val_loss: 2.8703\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5936 - val_loss: 2.8218\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5952 - val_loss: 2.8683\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5908 - val_loss: 2.8207\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5926 - val_loss: 2.8672\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5889 - val_loss: 2.8203\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5913 - val_loss: 2.8688\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5894 - val_loss: 2.8209\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5937 - val_loss: 2.8782\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5945 - val_loss: 2.8240\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6020 - val_loss: 2.8809\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5980 - val_loss: 2.8299\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.6053 - val_loss: 2.8783\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6005 - val_loss: 2.8347\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6063 - val_loss: 2.8789\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6017 - val_loss: 2.8325\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6050 - val_loss: 2.8793\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5995 - val_loss: 2.8300\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6037 - val_loss: 2.8781\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5978 - val_loss: 2.8301\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6010 - val_loss: 2.8743\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5949 - val_loss: 2.8297\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5965 - val_loss: 2.8717\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5920 - val_loss: 2.8280\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5944 - val_loss: 2.8721\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5909 - val_loss: 2.8271\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5950 - val_loss: 2.8736\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5912 - val_loss: 2.8272\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5946 - val_loss: 2.8725\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5901 - val_loss: 2.8273\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5921 - val_loss: 2.8701\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5879 - val_loss: 2.8264\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5895 - val_loss: 2.8678\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5851 - val_loss: 2.8249\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5867 - val_loss: 2.8643\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5808 - val_loss: 2.8228\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5820 - val_loss: 2.8611\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5774 - val_loss: 2.8221\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5795 - val_loss: 2.8605\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5761 - val_loss: 2.8222\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5796 - val_loss: 2.8640\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5780 - val_loss: 2.8234\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5838 - val_loss: 2.8725\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5848 - val_loss: 2.8264\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5910 - val_loss: 2.8772\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5888 - val_loss: 2.8263\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5915 - val_loss: 2.8742\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5857 - val_loss: 2.8253\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5863 - val_loss: 2.8684\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5801 - val_loss: 2.8241\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5802 - val_loss: 2.8652\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5761 - val_loss: 2.8228\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5778 - val_loss: 2.8610\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5715 - val_loss: 2.8202\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5730 - val_loss: 2.8590\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5684 - val_loss: 2.8193\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5711 - val_loss: 2.8609\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5682 - val_loss: 2.8194\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5723 - val_loss: 2.8668\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5713 - val_loss: 2.8209\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5768 - val_loss: 2.8711\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5765 - val_loss: 2.8237\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5813 - val_loss: 2.8721\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5784 - val_loss: 2.8239\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5798 - val_loss: 2.8691\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5747 - val_loss: 2.8221\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5749 - val_loss: 2.8661\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5706 - val_loss: 2.8213\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5714 - val_loss: 2.8650\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5687 - val_loss: 2.8209\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5703 - val_loss: 2.8655\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5684 - val_loss: 2.8209\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5704 - val_loss: 2.8664\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5684 - val_loss: 2.8208\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5703 - val_loss: 2.8669\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5678 - val_loss: 2.8205\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.5694 - val_loss: 2.8667\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5664 - val_loss: 2.8201\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5677 - val_loss: 2.8659\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5642 - val_loss: 2.8194\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5655 - val_loss: 2.8633\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5608 - val_loss: 2.8182\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5617 - val_loss: 2.8522\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5527 - val_loss: 2.8159\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5527 - val_loss: 2.8419\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5445 - val_loss: 2.8159\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5438 - val_loss: 2.8350\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5385 - val_loss: 2.8156\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5368 - val_loss: 2.8301\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5332 - val_loss: 2.8190\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5321 - val_loss: 2.8313\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5304 - val_loss: 2.8182\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5301 - val_loss: 2.8329\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5289 - val_loss: 2.8167\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5290 - val_loss: 2.8347\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.5278 - val_loss: 2.8163\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5287 - val_loss: 2.8348\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5280 - val_loss: 2.8151\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5314 - val_loss: 2.8433\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5345 - val_loss: 2.8160\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5518 - val_loss: 2.8868\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5726 - val_loss: 2.8381\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6026 - val_loss: 2.9137\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5982 - val_loss: 2.8333\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6016 - val_loss: 2.8980\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5813 - val_loss: 2.8213\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5779 - val_loss: 2.8796\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 2.5636 - val_loss: 2.8168\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.5625 - val_loss: 2.8729\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5575 - val_loss: 2.8193\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5628 - val_loss: 2.8737\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5596 - val_loss: 2.8197\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5632 - val_loss: 2.8718\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5586 - val_loss: 2.8195\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.5610 - val_loss: 2.8689\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5552 - val_loss: 2.8181\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5571 - val_loss: 2.8659\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5500 - val_loss: 2.8150\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5523 - val_loss: 2.8601\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5438 - val_loss: 2.8127\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5455 - val_loss: 2.8533\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 2.5369 - val_loss: 2.8122\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5396 - val_loss: 2.8478\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5328 - val_loss: 2.8138\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5357 - val_loss: 2.8426\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.5287 - val_loss: 2.8151\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5324 - val_loss: 2.8427\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5278 - val_loss: 2.8155\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5341 - val_loss: 2.8505\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5329 - val_loss: 2.8164\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5446 - val_loss: 2.8654\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5439 - val_loss: 2.8179\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5562 - val_loss: 2.8821\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5575 - val_loss: 2.8197\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5657 - val_loss: 2.8800\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5547 - val_loss: 2.8134\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5541 - val_loss: 2.8716\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5453 - val_loss: 2.8120\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5477 - val_loss: 2.8672\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5411 - val_loss: 2.8122\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5462 - val_loss: 2.8637\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5385 - val_loss: 2.8119\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5434 - val_loss: 2.8574\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5334 - val_loss: 2.8111\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5365 - val_loss: 2.8526\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5282 - val_loss: 2.8117\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5326 - val_loss: 2.8499\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.525 - 0s 94ms/step - loss: 2.5251 - val_loss: 2.8114\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.5305 - val_loss: 2.8497\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.5239 - val_loss: 2.8114\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5311 - val_loss: 2.8546\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5266 - val_loss: 2.8119\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5359 - val_loss: 2.8613\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5316 - val_loss: 2.8125\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.5423 - val_loss: 2.8698\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5382 - val_loss: 2.8130\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5472 - val_loss: 2.8747\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5414 - val_loss: 2.8120\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5478 - val_loss: 2.8721\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5378 - val_loss: 2.8089\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5407 - val_loss: 2.8637\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5293 - val_loss: 2.8072\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5335 - val_loss: 2.8566\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5232 - val_loss: 2.8079\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5285 - val_loss: 2.8531\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5205 - val_loss: 2.8109\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5272 - val_loss: 2.8497\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 57.0698 - val_loss: 53.9722\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 55.6314 - val_loss: 51.7080\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 53.3507 - val_loss: 48.7023\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 50.3241 - val_loss: 44.8134\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 46.4131 - val_loss: 39.9019\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 41.4801 - val_loss: 34.0521\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 35.5942 - val_loss: 27.8044\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 29.2371 - val_loss: 22.4576\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 23.5421 - val_loss: 19.3572\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 20.1734 - val_loss: 17.6774\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 18.3471 - val_loss: 16.6703\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 17.2188 - val_loss: 15.9423\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 16.4190 - val_loss: 15.2607\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 15.7123 - val_loss: 14.4927\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 14.9511 - val_loss: 13.5907\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 14.0753 - val_loss: 12.6712\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 13.1818 - val_loss: 11.7698\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.2852 - val_loss: 10.9307\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 11.4106 - val_loss: 10.2302\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.6369 - val_loss: 9.6272\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 9.9296 - val_loss: 9.1424\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 9.3268 - val_loss: 8.8391\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.9224 - val_loss: 8.7054\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.7085 - val_loss: 8.6151\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5673 - val_loss: 8.5511\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.4607 - val_loss: 8.4855\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3741 - val_loss: 8.3921\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.2903 - val_loss: 8.2902\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.2064 - val_loss: 8.1929\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.1225 - val_loss: 8.1008\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0402 - val_loss: 8.0091\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.9577 - val_loss: 7.9173\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.8727 - val_loss: 7.8266\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.7839 - val_loss: 7.7353\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.6910 - val_loss: 7.6394\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.5940 - val_loss: 7.5397\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.4934 - val_loss: 7.4366\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.3898 - val_loss: 7.3322\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.2839 - val_loss: 7.2274\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.1764 - val_loss: 7.1235\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.0684 - val_loss: 7.0217\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.9607 - val_loss: 6.9203\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.8542 - val_loss: 6.8201\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.7488 - val_loss: 6.7206\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.6443 - val_loss: 6.6200\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.5403 - val_loss: 6.5211\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.4363 - val_loss: 6.4197\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.3319 - val_loss: 6.3163\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.2269 - val_loss: 6.2115\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.1213 - val_loss: 6.1060\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 6.0154 - val_loss: 6.0003\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.9094 - val_loss: 5.8948\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.8039 - val_loss: 5.7906\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.6993 - val_loss: 5.6895\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.5960 - val_loss: 5.5905\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.4942 - val_loss: 5.4931\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.3937 - val_loss: 5.3984\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.2949 - val_loss: 5.3055\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.1980 - val_loss: 5.2144\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.1032 - val_loss: 5.1262\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.0108 - val_loss: 5.0397\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.9211 - val_loss: 4.9548\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.8341 - val_loss: 4.8729\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.7503 - val_loss: 4.7953\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.6697 - val_loss: 4.7212\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.5923 - val_loss: 4.6494\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.5181 - val_loss: 4.5803\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.4470 - val_loss: 4.5145\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.3790 - val_loss: 4.4509\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.3144 - val_loss: 4.3932\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.2528 - val_loss: 4.3382\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.1939 - val_loss: 4.2868\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.1378 - val_loss: 4.2397\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0844 - val_loss: 4.1970\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.0338 - val_loss: 4.1580\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.9861 - val_loss: 4.1222\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.9413 - val_loss: 4.0880\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.8996 - val_loss: 4.0569\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.8603 - val_loss: 4.0266\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.8231 - val_loss: 3.9989\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.7882 - val_loss: 3.9751\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.7551 - val_loss: 3.9486\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.7236 - val_loss: 3.9256\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6940 - val_loss: 3.9058\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6661 - val_loss: 3.8859\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.6395 - val_loss: 3.8651\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.6140 - val_loss: 3.8462\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.5895 - val_loss: 3.8287\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.5658 - val_loss: 3.8109\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.5431 - val_loss: 3.7927\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5214 - val_loss: 3.7761\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5006 - val_loss: 3.7603\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4809 - val_loss: 3.7445\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.4621 - val_loss: 3.7290\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4440 - val_loss: 3.7143\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4266 - val_loss: 3.7000\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4098 - val_loss: 3.6858\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.3936 - val_loss: 3.6722\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.3778 - val_loss: 3.6592\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.3626 - val_loss: 3.6466\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3477 - val_loss: 3.6342\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.3334 - val_loss: 3.6230\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3196 - val_loss: 3.6102\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.3063 - val_loss: 3.5995\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.2935 - val_loss: 3.5882\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2813 - val_loss: 3.5769\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2697 - val_loss: 3.5676\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.2586 - val_loss: 3.5573\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2480 - val_loss: 3.5477\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2378 - val_loss: 3.5385\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2280 - val_loss: 3.5302\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.2186 - val_loss: 3.5215\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2095 - val_loss: 3.5127\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2008 - val_loss: 3.5043\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.1923 - val_loss: 3.4959\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.1840 - val_loss: 3.4878\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1760 - val_loss: 3.4800\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.1681 - val_loss: 3.4723\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1605 - val_loss: 3.4648\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.1531 - val_loss: 3.4574\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1459 - val_loss: 3.4502\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1388 - val_loss: 3.4432\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1320 - val_loss: 3.4363\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1252 - val_loss: 3.4296\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1186 - val_loss: 3.4230\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1121 - val_loss: 3.4164\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1057 - val_loss: 3.4098\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.0995 - val_loss: 3.4035\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0934 - val_loss: 3.3978\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0875 - val_loss: 3.3897\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0818 - val_loss: 3.3872\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0764 - val_loss: 3.3754\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0712 - val_loss: 3.3786\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0664 - val_loss: 3.3618\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0615 - val_loss: 3.3702\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0573 - val_loss: 3.3504\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0525 - val_loss: 3.3612\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0483 - val_loss: 3.3406\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0433 - val_loss: 3.3507\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0391 - val_loss: 3.3315\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0342 - val_loss: 3.3405\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0303 - val_loss: 3.3228\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0258 - val_loss: 3.3304\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0219 - val_loss: 3.3144\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0175 - val_loss: 3.3205\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0134 - val_loss: 3.3068\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0092 - val_loss: 3.3108\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 3.0053 - val_loss: 3.2993\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0014 - val_loss: 3.3026\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9978 - val_loss: 3.2911\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9940 - val_loss: 3.2952\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9906 - val_loss: 3.2831\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9869 - val_loss: 3.2882\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9838 - val_loss: 3.2753\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9801 - val_loss: 3.2820\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9772 - val_loss: 3.2678\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9735 - val_loss: 3.2752\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9707 - val_loss: 3.2611\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9669 - val_loss: 3.2678\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9641 - val_loss: 3.2546\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9604 - val_loss: 3.2609\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9577 - val_loss: 3.2482\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9541 - val_loss: 3.2543\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9513 - val_loss: 3.2420\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9477 - val_loss: 3.2479\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9449 - val_loss: 3.2362\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.9415 - val_loss: 3.2416\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9386 - val_loss: 3.2305\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9353 - val_loss: 3.2353\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9324 - val_loss: 3.2250\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9292 - val_loss: 3.2292\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9262 - val_loss: 3.2196\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9231 - val_loss: 3.2231\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9201 - val_loss: 3.2143\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9170 - val_loss: 3.2172\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9140 - val_loss: 3.2092\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9110 - val_loss: 3.2115\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9080 - val_loss: 3.2046\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9051 - val_loss: 3.2062\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9022 - val_loss: 3.2008\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8994 - val_loss: 3.2008\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8965 - val_loss: 3.1966\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8937 - val_loss: 3.1961\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8909 - val_loss: 3.1927\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8882 - val_loss: 3.1914\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8855 - val_loss: 3.1889\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8828 - val_loss: 3.1869\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8801 - val_loss: 3.1848\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8774 - val_loss: 3.1827\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8747 - val_loss: 3.1807\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8720 - val_loss: 3.1786\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8694 - val_loss: 3.1766\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8667 - val_loss: 3.1745\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8641 - val_loss: 3.1726\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8615 - val_loss: 3.1705\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8589 - val_loss: 3.1686\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8563 - val_loss: 3.1665\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8537 - val_loss: 3.1645\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8511 - val_loss: 3.1623\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8486 - val_loss: 3.1604\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8460 - val_loss: 3.1581\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8435 - val_loss: 3.1563\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8410 - val_loss: 3.1536\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8385 - val_loss: 3.1525\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8361 - val_loss: 3.1488\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8337 - val_loss: 3.1499\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8314 - val_loss: 3.1428\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8292 - val_loss: 3.1500\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8273 - val_loss: 3.1374\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8251 - val_loss: 3.1477\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8230 - val_loss: 3.1342\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8204 - val_loss: 3.1431\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8181 - val_loss: 3.1309\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8156 - val_loss: 3.1391\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8133 - val_loss: 3.1272\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8109 - val_loss: 3.1355\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8087 - val_loss: 3.1238\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8064 - val_loss: 3.1319\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8042 - val_loss: 3.1206\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8019 - val_loss: 3.1284\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7997 - val_loss: 3.1174\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7975 - val_loss: 3.1251\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7953 - val_loss: 3.1143\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7931 - val_loss: 3.1224\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7911 - val_loss: 3.1108\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7890 - val_loss: 3.1197\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7870 - val_loss: 3.1079\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7849 - val_loss: 3.1163\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7829 - val_loss: 3.1055\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7811 - val_loss: 3.1123\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7799 - val_loss: 3.1085\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7818 - val_loss: 3.1135\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7875 - val_loss: 3.1416\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8047 - val_loss: 3.1203\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7927 - val_loss: 3.1280\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7963 - val_loss: 3.1191\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7817 - val_loss: 3.1123\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7827 - val_loss: 3.1082\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7734 - val_loss: 3.1100\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7753 - val_loss: 3.1019\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7679 - val_loss: 3.1024\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7680 - val_loss: 3.0997\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7633 - val_loss: 3.0989\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7643 - val_loss: 3.0967\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7603 - val_loss: 3.0991\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7622 - val_loss: 3.0941\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7571 - val_loss: 3.0970\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7584 - val_loss: 3.0912\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7531 - val_loss: 3.0919\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7531 - val_loss: 3.0869\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7488 - val_loss: 3.0882\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7487 - val_loss: 3.0831\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7453 - val_loss: 3.0870\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7455 - val_loss: 3.0801\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7421 - val_loss: 3.0858\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7423 - val_loss: 3.0776\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7387 - val_loss: 3.0834\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7388 - val_loss: 3.0748\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7351 - val_loss: 3.0808\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7352 - val_loss: 3.0719\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7317 - val_loss: 3.0786\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7318 - val_loss: 3.0691\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7285 - val_loss: 3.0768\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7286 - val_loss: 3.0666\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7253 - val_loss: 3.0749\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7253 - val_loss: 3.0641\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7222 - val_loss: 3.0727\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7221 - val_loss: 3.0616\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7190 - val_loss: 3.0705\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7188 - val_loss: 3.0592\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7159 - val_loss: 3.0684\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7157 - val_loss: 3.0568\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7129 - val_loss: 3.0664\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7126 - val_loss: 3.0545\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7099 - val_loss: 3.0644\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7095 - val_loss: 3.0522\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7070 - val_loss: 3.0625\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7065 - val_loss: 3.0499\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7041 - val_loss: 3.0606\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7036 - val_loss: 3.0477\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7012 - val_loss: 3.0589\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7007 - val_loss: 3.0454\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6984 - val_loss: 3.0575\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6980 - val_loss: 3.0433\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6958 - val_loss: 3.0564\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6955 - val_loss: 3.0412\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6933 - val_loss: 3.0555\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6931 - val_loss: 3.0393\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6910 - val_loss: 3.0545\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6908 - val_loss: 3.0374\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6885 - val_loss: 3.0532\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6883 - val_loss: 3.0353\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6860 - val_loss: 3.0515\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6856 - val_loss: 3.0333\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6834 - val_loss: 3.0498\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6830 - val_loss: 3.0313\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6808 - val_loss: 3.0482\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6804 - val_loss: 3.0293\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6784 - val_loss: 3.0471\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6782 - val_loss: 3.0272\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6765 - val_loss: 3.0470\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6765 - val_loss: 3.0250\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6753 - val_loss: 3.0482\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6757 - val_loss: 3.0227\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6757 - val_loss: 3.0569\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6798 - val_loss: 3.0243\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6842 - val_loss: 3.0777\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6963 - val_loss: 3.0348\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7039 - val_loss: 3.0914\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7122 - val_loss: 3.0337\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7083 - val_loss: 3.0956\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7171 - val_loss: 3.0321\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.7037 - val_loss: 3.0767\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6998 - val_loss: 3.0228\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6883 - val_loss: 3.0540\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6788 - val_loss: 3.0116\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6683 - val_loss: 3.0303\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6601 - val_loss: 3.0099\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6534 - val_loss: 3.0132\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6503 - val_loss: 3.0135\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6490 - val_loss: 3.0095\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6479 - val_loss: 3.0135\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6466 - val_loss: 3.0086\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6452 - val_loss: 3.0114\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6439 - val_loss: 3.0086\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6426 - val_loss: 3.0081\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6415 - val_loss: 3.0076\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6403 - val_loss: 3.0062\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6392 - val_loss: 3.0060\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6380 - val_loss: 3.0050\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6369 - val_loss: 3.0046\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6358 - val_loss: 3.0037\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6347 - val_loss: 3.0033\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6336 - val_loss: 3.0024\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6325 - val_loss: 3.0020\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6314 - val_loss: 3.0010\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6304 - val_loss: 3.0010\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6293 - val_loss: 2.9995\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6283 - val_loss: 3.0002\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6272 - val_loss: 2.9980\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6262 - val_loss: 2.9994\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6252 - val_loss: 2.9968\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6243 - val_loss: 2.9975\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6238 - val_loss: 3.0014\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6249 - val_loss: 2.9938\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6276 - val_loss: 3.0236\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6353 - val_loss: 2.9977\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6477 - val_loss: 3.0797\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6783 - val_loss: 3.0228\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6944 - val_loss: 3.1417\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7343 - val_loss: 3.0327\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7057 - val_loss: 3.1152\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7094 - val_loss: 3.0116\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6654 - val_loss: 3.0472\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6514 - val_loss: 2.9926\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6404 - val_loss: 3.0350\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6393 - val_loss: 2.9845\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6364 - val_loss: 3.0369\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6388 - val_loss: 2.9856\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6381 - val_loss: 3.0421\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6417 - val_loss: 2.9895\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6404 - val_loss: 3.0469\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6455 - val_loss: 2.9930\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6430 - val_loss: 3.0510\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6487 - val_loss: 2.9956\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6442 - val_loss: 3.0497\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6475 - val_loss: 2.9948\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6408 - val_loss: 3.0423\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6408 - val_loss: 2.9904\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6342 - val_loss: 3.0354\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6334 - val_loss: 2.9828\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6260 - val_loss: 3.0262\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6244 - val_loss: 2.9745\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6165 - val_loss: 3.0093\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6118 - val_loss: 2.9710\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6069 - val_loss: 2.9952\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6022 - val_loss: 2.9736\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6004 - val_loss: 2.9945\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6000 - val_loss: 2.9742\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6009 - val_loss: 2.9992\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6017 - val_loss: 2.9722\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6045 - val_loss: 3.0143\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6113 - val_loss: 2.9749\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6144 - val_loss: 3.0385\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6297 - val_loss: 2.9915\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6371 - val_loss: 3.0613\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6472 - val_loss: 2.9950\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6362 - val_loss: 3.0479\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6355 - val_loss: 2.9855\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6204 - val_loss: 3.0279\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6189 - val_loss: 2.9755\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6075 - val_loss: 3.0138\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6061 - val_loss: 2.9652\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5996 - val_loss: 3.0004\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5964 - val_loss: 2.9626\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5935 - val_loss: 2.9943\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5920 - val_loss: 2.9647\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5919 - val_loss: 3.0016\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5960 - val_loss: 2.9677\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.5978 - val_loss: 3.0194\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.6077 - val_loss: 2.9749\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.6085 - val_loss: 3.0385\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6219 - val_loss: 2.9869\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6213 - val_loss: 3.0489\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6300 - val_loss: 2.9882\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6196 - val_loss: 3.0392\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6209 - val_loss: 2.9795\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6058 - val_loss: 3.0197\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6043 - val_loss: 2.9669\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 2.5893 - val_loss: 2.9946\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5841 - val_loss: 2.9566\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5812 - val_loss: 2.9887\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5802 - val_loss: 2.9575\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5805 - val_loss: 2.9938\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5837 - val_loss: 2.9601\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5844 - val_loss: 3.0086\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5932 - val_loss: 2.9680\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5944 - val_loss: 3.0302\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6089 - val_loss: 2.9786\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6054 - val_loss: 3.0395\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6161 - val_loss: 2.9823\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6056 - val_loss: 3.0330\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6102 - val_loss: 2.9764\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5962 - val_loss: 3.0177\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5970 - val_loss: 2.9655\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5813 - val_loss: 2.9952\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5786 - val_loss: 2.9530\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5730 - val_loss: 2.9863\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5718 - val_loss: 2.9505\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5701 - val_loss: 2.9876\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5724 - val_loss: 2.9534\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5720 - val_loss: 3.0002\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5809 - val_loss: 2.9623\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5826 - val_loss: 3.0240\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5982 - val_loss: 2.9732\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5926 - val_loss: 3.0307\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6036 - val_loss: 2.9757\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5908 - val_loss: 3.0215\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5957 - val_loss: 2.9697\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5821 - val_loss: 3.0079\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5838 - val_loss: 2.9602\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5690 - val_loss: 2.9853\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5654 - val_loss: 2.9490\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5607 - val_loss: 2.9790\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5604 - val_loss: 2.9448\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5593 - val_loss: 2.9828\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5625 - val_loss: 2.9487\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5631 - val_loss: 3.0008\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5749 - val_loss: 2.9604\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5751 - val_loss: 3.0228\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5914 - val_loss: 2.9707\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5838 - val_loss: 3.0255\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5940 - val_loss: 2.9704\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5800 - val_loss: 3.0135\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5840 - val_loss: 2.9641\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5704 - val_loss: 3.0015\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.5731 - val_loss: 2.9562\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5601 - val_loss: 2.9816\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5574 - val_loss: 2.9444\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5518 - val_loss: 2.9728\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5505 - val_loss: 2.9405\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5483 - val_loss: 2.9738\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5502 - val_loss: 2.9444\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.5517 - val_loss: 2.9903\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.5614 - val_loss: 2.9542\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.5631 - val_loss: 3.0158\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5804 - val_loss: 2.9663\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 2.5744 - val_loss: 3.0237\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5867 - val_loss: 2.9671\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5734 - val_loss: 3.0142\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5792 - val_loss: 2.9617\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5645 - val_loss: 3.0006\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5681 - val_loss: 2.9564\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5538 - val_loss: 2.9836\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5536 - val_loss: 2.9466\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5461 - val_loss: 2.9709\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5441 - val_loss: 2.9398\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5399 - val_loss: 2.9669\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5405 - val_loss: 2.9411\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5408 - val_loss: 2.9766\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5461 - val_loss: 2.9467\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5483 - val_loss: 2.9997\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5625 - val_loss: 2.9571\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5589 - val_loss: 3.0141\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5728 - val_loss: 2.9595\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5621 - val_loss: 3.0119\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5709 - val_loss: 2.9568\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5575 - val_loss: 3.0024\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5629 - val_loss: 2.9521\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5481 - val_loss: 2.9858\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5488 - val_loss: 2.9452\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.5393 - val_loss: 2.9736\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5397 - val_loss: 2.9414\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5351 - val_loss: 2.9698\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5365 - val_loss: 2.9403\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5344 - val_loss: 2.9763\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5402 - val_loss: 2.9449\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5394 - val_loss: 2.9881\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5484 - val_loss: 2.9501\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5436 - val_loss: 2.9947\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5528 - val_loss: 2.9495\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5441 - val_loss: 2.9980\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5542 - val_loss: 2.9501\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5449 - val_loss: 2.9981\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5537 - val_loss: 2.9480\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5424 - val_loss: 2.9898\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5468 - val_loss: 2.9449\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5359 - val_loss: 2.9802\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5388 - val_loss: 2.9427\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5312 - val_loss: 2.9728\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5327 - val_loss: 2.9401\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5270 - val_loss: 2.9664\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5278 - val_loss: 2.9392\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5249 - val_loss: 2.9661\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5272 - val_loss: 2.9408\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5262 - val_loss: 2.9765\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5337 - val_loss: 2.9446\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5312 - val_loss: 2.9887\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5421 - val_loss: 2.9448\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5349 - val_loss: 2.9924\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5449 - val_loss: 2.9442\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5338 - val_loss: 2.9866\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5400 - val_loss: 2.9433\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5293 - val_loss: 2.9805\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5345 - val_loss: 2.9412\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5260 - val_loss: 2.9779\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5312 - val_loss: 2.9395\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5239 - val_loss: 2.9766\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5292 - val_loss: 2.9385\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5225 - val_loss: 2.9755\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5278 - val_loss: 2.9382\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5213 - val_loss: 2.9748\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5267 - val_loss: 2.9380\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5203 - val_loss: 2.9745\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5259 - val_loss: 2.9378\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5194 - val_loss: 2.9745\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5253 - val_loss: 2.9376\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5185 - val_loss: 2.9743\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5246 - val_loss: 2.9372\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5176 - val_loss: 2.9739\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5237 - val_loss: 2.9368\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5164 - val_loss: 2.9728\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 2.5223 - val_loss: 2.9362\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.5149 - val_loss: 2.9710\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5202 - val_loss: 2.9355\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.5131 - val_loss: 2.9682\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 2.5173 - val_loss: 2.9346\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5109 - val_loss: 2.9650\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5143 - val_loss: 2.9340\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5091 - val_loss: 2.9633\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5123 - val_loss: 2.9338\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5080 - val_loss: 2.9643\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.5124 - val_loss: 2.9339\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5079 - val_loss: 2.9673\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5141 - val_loss: 2.9342\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5083 - val_loss: 2.9702\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5160 - val_loss: 2.9347\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5089 - val_loss: 2.9719\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5175 - val_loss: 2.9366\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5097 - val_loss: 2.9717\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5185 - val_loss: 2.9407\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5112 - val_loss: 2.9708\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5194 - val_loss: 2.9436\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5113 - val_loss: 2.9682\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5190 - val_loss: 2.9458\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5112 - val_loss: 2.9630\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5155 - val_loss: 2.9447\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5072 - val_loss: 2.9521\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5066 - val_loss: 2.9434\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5043 - val_loss: 2.9532\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5064 - val_loss: 2.9405\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5033 - val_loss: 2.9579\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5084 - val_loss: 2.9365\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5019 - val_loss: 2.9594\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5066 - val_loss: 2.9327\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4987 - val_loss: 2.9575\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5025 - val_loss: 2.9289\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.4956 - val_loss: 2.9578\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.4996 - val_loss: 2.9241\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.4929 - val_loss: 2.9568\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.4966 - val_loss: 2.9225\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.4911 - val_loss: 2.9568\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.4954 - val_loss: 2.9223\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4904 - val_loss: 2.9580\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.4959 - val_loss: 2.9224\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.4904 - val_loss: 2.9592\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.4963 - val_loss: 2.9230\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.4899 - val_loss: 2.9597\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.4966 - val_loss: 2.9235\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.4895 - val_loss: 2.9581\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.4951 - val_loss: 2.9250\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.4879 - val_loss: 2.9558\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.4936 - val_loss: 2.9255\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.4875 - val_loss: 2.9574\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.4946 - val_loss: 2.9276\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.4879 - val_loss: 2.9573\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4959 - val_loss: 2.9338\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.4914 - val_loss: 2.9614\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.502 - 0s 81ms/step - loss: 2.5028 - val_loss: 2.9480\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 2.5008 - val_loss: 2.9637\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 55.6645 - val_loss: 56.9051\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 54.2737 - val_loss: 54.7448\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 52.1126 - val_loss: 51.9450\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 49.3061 - val_loss: 48.3691\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 45.7114 - val_loss: 43.8452\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 41.1615 - val_loss: 38.3681\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 35.6880 - val_loss: 32.2547\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 29.6964 - val_loss: 26.3515\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 24.2307 - val_loss: 22.0568\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 20.7623 - val_loss: 19.3978\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 18.7210 - val_loss: 17.6555\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 17.4662 - val_loss: 16.5303\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 16.6960 - val_loss: 15.7838\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.1814 - val_loss: 15.2176\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 15.7422 - val_loss: 14.6814\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 15.2491 - val_loss: 14.0824\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 14.6303 - val_loss: 13.3739\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 13.8544 - val_loss: 12.4221\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 12.7612 - val_loss: 11.2252\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 11.3969 - val_loss: 10.2379\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 10.3267 - val_loss: 9.3746\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.4525 - val_loss: 8.7310\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.8285 - val_loss: 8.4947\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.5519 - val_loss: 8.4757\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.4322 - val_loss: 8.4448\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.3258 - val_loss: 8.3812\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2253 - val_loss: 8.3068\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.1389 - val_loss: 8.2254\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.0535 - val_loss: 8.1342\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.9642 - val_loss: 8.0401\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.8754 - val_loss: 7.9478\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.7879 - val_loss: 7.8600\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.7012 - val_loss: 7.7764\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.6150 - val_loss: 7.6949\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.5287 - val_loss: 7.6122\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.4412 - val_loss: 7.5276\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.3519 - val_loss: 7.4414\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 7.2607 - val_loss: 7.3533\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.1676 - val_loss: 7.2635\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.0726 - val_loss: 7.1700\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 6.9757 - val_loss: 7.0739\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.8770 - val_loss: 6.9766\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.7768 - val_loss: 6.8787\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.6754 - val_loss: 6.7799\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.5730 - val_loss: 6.6808\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 6.4696 - val_loss: 6.5813\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 6.3653 - val_loss: 6.4811\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.2599 - val_loss: 6.3803\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 6.1531 - val_loss: 6.2790\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 6.0448 - val_loss: 6.1774\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 5.9345 - val_loss: 6.0754\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.8222 - val_loss: 5.9732\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.7080 - val_loss: 5.8709\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.5923 - val_loss: 5.7693\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.4765 - val_loss: 5.6680\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.3618 - val_loss: 5.5688\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.2494 - val_loss: 5.4715\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.1400 - val_loss: 5.3759\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.0341 - val_loss: 5.2824\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.9317 - val_loss: 5.1913\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.8331 - val_loss: 5.1031\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.7387 - val_loss: 5.0179\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.6489 - val_loss: 4.9361\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.5642 - val_loss: 4.8579\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.4851 - val_loss: 4.7839\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.4117 - val_loss: 4.7144\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.3437 - val_loss: 4.6500\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.2807 - val_loss: 4.5900\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 4.2226 - val_loss: 4.5336\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.1689 - val_loss: 4.4799\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.1187 - val_loss: 4.4283\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.0715 - val_loss: 4.3787\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.0267 - val_loss: 4.3309\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.9841 - val_loss: 4.2848\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.9436 - val_loss: 4.2409\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.9054 - val_loss: 4.1996\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.8691 - val_loss: 4.1602\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.8344 - val_loss: 4.1224\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.8012 - val_loss: 4.0862\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.7691 - val_loss: 4.0513\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.7383 - val_loss: 4.0170\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.7088 - val_loss: 3.9841\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.6810 - val_loss: 3.9526\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.6546 - val_loss: 3.9224\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.6296 - val_loss: 3.8929\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6058 - val_loss: 3.8644\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.5831 - val_loss: 3.8371\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 3.5613 - val_loss: 3.8106\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.5401 - val_loss: 3.7848\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.5197 - val_loss: 3.7600\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4999 - val_loss: 3.7366\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.4809 - val_loss: 3.7140\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4625 - val_loss: 3.6924\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4447 - val_loss: 3.6719\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.4274 - val_loss: 3.6520\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4107 - val_loss: 3.6328\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.3944 - val_loss: 3.6141\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3785 - val_loss: 3.5958\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3630 - val_loss: 3.5781\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.3478 - val_loss: 3.5607\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.3330 - val_loss: 3.5441\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3186 - val_loss: 3.5290\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.3045 - val_loss: 3.5133\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2908 - val_loss: 3.4995\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.2774 - val_loss: 3.4853\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2644 - val_loss: 3.4729\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.2518 - val_loss: 3.4597\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2395 - val_loss: 3.4483\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2276 - val_loss: 3.4364\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2162 - val_loss: 3.4262\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2052 - val_loss: 3.4164\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1947 - val_loss: 3.4074\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1845 - val_loss: 3.3985\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1747 - val_loss: 3.3906\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1652 - val_loss: 3.3815\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1562 - val_loss: 3.3753\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1476 - val_loss: 3.3634\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1396 - val_loss: 3.3643\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1323 - val_loss: 3.3463\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1253 - val_loss: 3.3560\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1196 - val_loss: 3.3312\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1123 - val_loss: 3.3452\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1061 - val_loss: 3.3184\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0977 - val_loss: 3.3299\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0908 - val_loss: 3.3063\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0824 - val_loss: 3.3148\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0757 - val_loss: 3.2952\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0685 - val_loss: 3.3021\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0628 - val_loss: 3.2832\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0565 - val_loss: 3.2915\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0517 - val_loss: 3.2724\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0455 - val_loss: 3.2802\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0405 - val_loss: 3.2624\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0346 - val_loss: 3.2695\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0300 - val_loss: 3.2532\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.0247 - val_loss: 3.2616\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0208 - val_loss: 3.2443\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0161 - val_loss: 3.2554\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0126 - val_loss: 3.2358\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0082 - val_loss: 3.2491\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0047 - val_loss: 3.2276\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0006 - val_loss: 3.2423\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9968 - val_loss: 3.2198\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9929 - val_loss: 3.2352\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9890 - val_loss: 3.2127\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9850 - val_loss: 3.2276\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9810 - val_loss: 3.2060\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9764 - val_loss: 3.2184\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9724 - val_loss: 3.1998\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9679 - val_loss: 3.2092\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9641 - val_loss: 3.1937\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9600 - val_loss: 3.1996\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9560 - val_loss: 3.1888\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9521 - val_loss: 3.1900\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9485 - val_loss: 3.1835\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9451 - val_loss: 3.1817\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9418 - val_loss: 3.1780\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9386 - val_loss: 3.1747\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9354 - val_loss: 3.1722\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.9322 - val_loss: 3.1687\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9291 - val_loss: 3.1663\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9260 - val_loss: 3.1631\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9229 - val_loss: 3.1606\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9198 - val_loss: 3.1575\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.9168 - val_loss: 3.1550\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9137 - val_loss: 3.1520\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9107 - val_loss: 3.1495\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9077 - val_loss: 3.1465\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9048 - val_loss: 3.1440\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9018 - val_loss: 3.1411\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8989 - val_loss: 3.1387\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8959 - val_loss: 3.1358\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.8930 - val_loss: 3.1335\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8901 - val_loss: 3.1306\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8873 - val_loss: 3.1285\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8844 - val_loss: 3.1255\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8816 - val_loss: 3.1236\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8788 - val_loss: 3.1204\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8760 - val_loss: 3.1189\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8732 - val_loss: 3.1153\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8704 - val_loss: 3.1149\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8677 - val_loss: 3.1101\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8651 - val_loss: 3.1121\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8627 - val_loss: 3.1048\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8604 - val_loss: 3.1103\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8583 - val_loss: 3.1000\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8568 - val_loss: 3.1106\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8553 - val_loss: 3.0960\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8545 - val_loss: 3.1103\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8528 - val_loss: 3.0920\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8518 - val_loss: 3.1073\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8491 - val_loss: 3.0879\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8470 - val_loss: 3.1022\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8435 - val_loss: 3.0836\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.8410 - val_loss: 3.0966\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8375 - val_loss: 3.0796\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8353 - val_loss: 3.0920\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8321 - val_loss: 3.0759\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8302 - val_loss: 3.0880\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8271 - val_loss: 3.0723\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8253 - val_loss: 3.0842\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.8223 - val_loss: 3.0688\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8205 - val_loss: 3.0804\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.8174 - val_loss: 3.0654\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8156 - val_loss: 3.0766\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8126 - val_loss: 3.0620\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8108 - val_loss: 3.0729\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8079 - val_loss: 3.0587\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8061 - val_loss: 3.0693\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8032 - val_loss: 3.0554\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8013 - val_loss: 3.0658\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7985 - val_loss: 3.0521\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7966 - val_loss: 3.0623\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7939 - val_loss: 3.0488\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7919 - val_loss: 3.0587\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7892 - val_loss: 3.0455\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7870 - val_loss: 3.0548\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7841 - val_loss: 3.0422\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7819 - val_loss: 3.0507\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7789 - val_loss: 3.0388\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7761 - val_loss: 3.0459\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7731 - val_loss: 3.0356\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7701 - val_loss: 3.0408\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7674 - val_loss: 3.0326\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7647 - val_loss: 3.0370\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7624 - val_loss: 3.0300\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7602 - val_loss: 3.0346\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7582 - val_loss: 3.0274\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7563 - val_loss: 3.0330\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7545 - val_loss: 3.0248\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7528 - val_loss: 3.0315\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7510 - val_loss: 3.0224\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7495 - val_loss: 3.0301\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7476 - val_loss: 3.0201\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7462 - val_loss: 3.0285\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7443 - val_loss: 3.0177\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7428 - val_loss: 3.0265\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7407 - val_loss: 3.0152\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7390 - val_loss: 3.0238\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7367 - val_loss: 3.0125\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7347 - val_loss: 3.0203\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7323 - val_loss: 3.0098\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7302 - val_loss: 3.0169\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7280 - val_loss: 3.0074\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7260 - val_loss: 3.0142\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7241 - val_loss: 3.0051\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7223 - val_loss: 3.0121\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7206 - val_loss: 3.0030\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7189 - val_loss: 3.0104\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7173 - val_loss: 3.0008\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7157 - val_loss: 3.0087\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7141 - val_loss: 2.9988\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7125 - val_loss: 3.0070\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7109 - val_loss: 2.9968\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7093 - val_loss: 3.0053\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7078 - val_loss: 2.9949\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7062 - val_loss: 3.0037\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7046 - val_loss: 2.9931\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7032 - val_loss: 3.0020\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7016 - val_loss: 2.9914\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7001 - val_loss: 3.0003\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6985 - val_loss: 2.9897\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6971 - val_loss: 2.9985\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6954 - val_loss: 2.9883\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6943 - val_loss: 2.9967\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6925 - val_loss: 2.9870\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6918 - val_loss: 2.9955\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6899 - val_loss: 2.9857\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6894 - val_loss: 2.9944\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6873 - val_loss: 2.9844\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6869 - val_loss: 2.9932\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6847 - val_loss: 2.9830\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6842 - val_loss: 2.9917\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6819 - val_loss: 2.9815\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6813 - val_loss: 2.9901\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6791 - val_loss: 2.9801\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6786 - val_loss: 2.9887\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6765 - val_loss: 2.9792\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6764 - val_loss: 2.9885\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6747 - val_loss: 2.9790\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6752 - val_loss: 2.9899\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6742 - val_loss: 2.9791\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6744 - val_loss: 2.9915\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6737 - val_loss: 2.9796\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6743 - val_loss: 2.9949\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6751 - val_loss: 2.9816\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6768 - val_loss: 3.0015\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6799 - val_loss: 2.9864\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6842 - val_loss: 3.0137\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6901 - val_loss: 2.9943\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6963 - val_loss: 3.0259\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7009 - val_loss: 2.9981\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7015 - val_loss: 3.0251\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6994 - val_loss: 2.9934\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6936 - val_loss: 3.0124\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6860 - val_loss: 2.9849\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6785 - val_loss: 2.9992\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6719 - val_loss: 2.9810\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6710 - val_loss: 2.9958\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6669 - val_loss: 2.9788\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6678 - val_loss: 2.9970\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6665 - val_loss: 2.9791\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6677 - val_loss: 2.9990\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6673 - val_loss: 2.9804\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6689 - val_loss: 3.0015\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6689 - val_loss: 2.9825\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6713 - val_loss: 3.0044\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6708 - val_loss: 2.9832\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6718 - val_loss: 3.0035\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6691 - val_loss: 2.9815\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6683 - val_loss: 2.9996\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6645 - val_loss: 2.9771\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6606 - val_loss: 2.9914\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6555 - val_loss: 2.9738\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6541 - val_loss: 2.9878\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6506 - val_loss: 2.9720\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6505 - val_loss: 2.9865\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6478 - val_loss: 2.9705\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6477 - val_loss: 2.9864\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6465 - val_loss: 2.9707\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6470 - val_loss: 2.9876\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6464 - val_loss: 2.9721\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6485 - val_loss: 2.9910\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6487 - val_loss: 2.9749\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6521 - val_loss: 2.9943\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6514 - val_loss: 2.9766\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6537 - val_loss: 2.9942\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6509 - val_loss: 2.9756\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6512 - val_loss: 2.9910\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6470 - val_loss: 2.9729\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6460 - val_loss: 2.9871\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6419 - val_loss: 2.9694\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6397 - val_loss: 2.9824\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6360 - val_loss: 2.9661\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6332 - val_loss: 2.9768\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6291 - val_loss: 2.9632\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6279 - val_loss: 2.9746\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6253 - val_loss: 2.9626\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6259 - val_loss: 2.9753\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6248 - val_loss: 2.9637\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6267 - val_loss: 2.9783\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6271 - val_loss: 2.9673\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6315 - val_loss: 2.9846\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6331 - val_loss: 2.9718\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6374 - val_loss: 2.9890\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6376 - val_loss: 2.9737\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6398 - val_loss: 2.9903\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6391 - val_loss: 2.9731\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6379 - val_loss: 2.9874\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6353 - val_loss: 2.9698\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6316 - val_loss: 2.9797\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6254 - val_loss: 2.9643\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6226 - val_loss: 2.9738\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6177 - val_loss: 2.9603\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6151 - val_loss: 2.9688\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6112 - val_loss: 2.9578\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6109 - val_loss: 2.9675\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6088 - val_loss: 2.9577\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6097 - val_loss: 2.9683\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6088 - val_loss: 2.9586\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6102 - val_loss: 2.9696\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6095 - val_loss: 2.9606\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6123 - val_loss: 2.9727\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6122 - val_loss: 2.9644\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6167 - val_loss: 2.9778\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6168 - val_loss: 2.9672\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6199 - val_loss: 2.9831\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6219 - val_loss: 2.9697\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6219 - val_loss: 2.9843\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6228 - val_loss: 2.9691\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6200 - val_loss: 2.9809\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6184 - val_loss: 2.9665\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6152 - val_loss: 2.9744\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6103 - val_loss: 2.9623\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6088 - val_loss: 2.9689\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6030 - val_loss: 2.9579\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6019 - val_loss: 2.9654\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5980 - val_loss: 2.9560\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5982 - val_loss: 2.9640\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5956 - val_loss: 2.9551\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5959 - val_loss: 2.9630\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5937 - val_loss: 2.9547\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5943 - val_loss: 2.9627\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5928 - val_loss: 2.9557\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5948 - val_loss: 2.9645\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5941 - val_loss: 2.9574\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5963 - val_loss: 2.9667\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5955 - val_loss: 2.9603\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5994 - val_loss: 2.9723\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6009 - val_loss: 2.9644\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6036 - val_loss: 2.9771\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6055 - val_loss: 2.9657\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6045 - val_loss: 2.9760\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6038 - val_loss: 2.9639\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6011 - val_loss: 2.9720\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5987 - val_loss: 2.9616\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5969 - val_loss: 2.9683\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5938 - val_loss: 2.9595\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5932 - val_loss: 2.9647\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5890 - val_loss: 2.9564\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5888 - val_loss: 2.9609\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5834 - val_loss: 2.9524\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5827 - val_loss: 2.9576\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5785 - val_loss: 2.9507\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5791 - val_loss: 2.9570\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5766 - val_loss: 2.9511\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5782 - val_loss: 2.9584\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5776 - val_loss: 2.9542\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5816 - val_loss: 2.9639\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5835 - val_loss: 2.9591\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5868 - val_loss: 2.9687\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5878 - val_loss: 2.9634\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5909 - val_loss: 2.9715\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.5900 - val_loss: 2.9629\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5890 - val_loss: 2.9691\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5864 - val_loss: 2.9599\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5843 - val_loss: 2.9646\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5806 - val_loss: 2.9572\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5800 - val_loss: 2.9610\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5759 - val_loss: 2.9536\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5750 - val_loss: 2.9580\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5719 - val_loss: 2.9524\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5727 - val_loss: 2.9583\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5714 - val_loss: 2.9536\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5733 - val_loss: 2.9599\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5723 - val_loss: 2.9552\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5741 - val_loss: 2.9609\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5725 - val_loss: 2.9558\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5737 - val_loss: 2.9611\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5717 - val_loss: 2.9559\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5727 - val_loss: 2.9610\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5705 - val_loss: 2.9556\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5713 - val_loss: 2.9603\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5687 - val_loss: 2.9546\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5692 - val_loss: 2.9588\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5661 - val_loss: 2.9532\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5664 - val_loss: 2.9576\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5635 - val_loss: 2.9521\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5642 - val_loss: 2.9569\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5615 - val_loss: 2.9519\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5633 - val_loss: 2.9578\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5616 - val_loss: 2.9535\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5642 - val_loss: 2.9599\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5636 - val_loss: 2.9576\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5682 - val_loss: 2.9646\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5685 - val_loss: 2.9596\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5693 - val_loss: 2.9632\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5661 - val_loss: 2.9572\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5650 - val_loss: 2.9603\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5615 - val_loss: 2.9542\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5600 - val_loss: 2.9560\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5557 - val_loss: 2.9505\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5547 - val_loss: 2.9531\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5510 - val_loss: 2.9484\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5514 - val_loss: 2.9524\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5492 - val_loss: 2.9488\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5511 - val_loss: 2.9537\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5499 - val_loss: 2.9507\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5525 - val_loss: 2.9560\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5522 - val_loss: 2.9539\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5551 - val_loss: 2.9589\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5544 - val_loss: 2.9555\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5558 - val_loss: 2.9598\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5540 - val_loss: 2.9557\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5547 - val_loss: 2.9597\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5526 - val_loss: 2.9551\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5528 - val_loss: 2.9581\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5499 - val_loss: 2.9525\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5486 - val_loss: 2.9536\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5436 - val_loss: 2.9484\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5430 - val_loss: 2.9516\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5402 - val_loss: 2.9479\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5416 - val_loss: 2.9524\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5402 - val_loss: 2.9495\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5425 - val_loss: 2.9541\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5415 - val_loss: 2.9519\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5443 - val_loss: 2.9563\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5433 - val_loss: 2.9535\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5450 - val_loss: 2.9576\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5436 - val_loss: 2.9541\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5445 - val_loss: 2.9580\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5428 - val_loss: 2.9540\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.5434 - val_loss: 2.9577\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5413 - val_loss: 2.9525\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5406 - val_loss: 2.9546\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5366 - val_loss: 2.9501\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5370 - val_loss: 2.9526\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5333 - val_loss: 2.9479\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5333 - val_loss: 2.9510\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5309 - val_loss: 2.9480\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5325 - val_loss: 2.9525\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5320 - val_loss: 2.9506\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5344 - val_loss: 2.9548\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.5336 - val_loss: 2.9522\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5352 - val_loss: 2.9558\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5335 - val_loss: 2.9524\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5342 - val_loss: 2.9555\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5319 - val_loss: 2.9515\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5321 - val_loss: 2.9539\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5289 - val_loss: 2.9496\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5289 - val_loss: 2.9523\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5260 - val_loss: 2.9487\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5268 - val_loss: 2.9521\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5249 - val_loss: 2.9492\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5265 - val_loss: 2.9529\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5249 - val_loss: 2.9504\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5270 - val_loss: 2.9541\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5255 - val_loss: 2.9515\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5272 - val_loss: 2.9555\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5263 - val_loss: 2.9525\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5274 - val_loss: 2.9566\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5265 - val_loss: 2.9531\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5272 - val_loss: 2.9571\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5260 - val_loss: 2.9523\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5252 - val_loss: 2.9556\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5231 - val_loss: 2.9508\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5230 - val_loss: 2.9545\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5211 - val_loss: 2.9503\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5214 - val_loss: 2.9542\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5199 - val_loss: 2.9502\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5204 - val_loss: 2.9538\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5190 - val_loss: 2.9502\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5193 - val_loss: 2.9522\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5164 - val_loss: 2.9485\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5160 - val_loss: 2.9508\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5143 - val_loss: 2.9483\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5145 - val_loss: 2.9512\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5135 - val_loss: 2.9489\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5141 - val_loss: 2.9520\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5129 - val_loss: 2.9492\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5134 - val_loss: 2.9520\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5115 - val_loss: 2.9487\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5118 - val_loss: 2.9515\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5099 - val_loss: 2.9485\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5105 - val_loss: 2.9515\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5090 - val_loss: 2.9489\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5101 - val_loss: 2.9521\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5088 - val_loss: 2.9496\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5099 - val_loss: 2.9527\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5087 - val_loss: 2.9500\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5095 - val_loss: 2.9532\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5082 - val_loss: 2.9502\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5086 - val_loss: 2.9535\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5075 - val_loss: 2.9502\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5077 - val_loss: 2.9539\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5071 - val_loss: 2.9505\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5071 - val_loss: 2.9548\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5071 - val_loss: 2.9510\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5069 - val_loss: 2.9549\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5063 - val_loss: 2.9506\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5060 - val_loss: 2.9547\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5053 - val_loss: 2.9501\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5047 - val_loss: 2.9541\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5038 - val_loss: 2.9496\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5032 - val_loss: 2.9533\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5021 - val_loss: 2.9491\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5014 - val_loss: 2.9522\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5000 - val_loss: 2.9487\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4999 - val_loss: 2.9511\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.4983 - val_loss: 2.9479\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4977 - val_loss: 2.9501\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.4967 - val_loss: 2.9476\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4959 - val_loss: 2.9498\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.4952 - val_loss: 2.9472\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.4939 - val_loss: 2.9496\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.4931 - val_loss: 2.9473\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.4929 - val_loss: 2.9506\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4928 - val_loss: 2.9486\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.4939 - val_loss: 2.9525\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.4940 - val_loss: 2.9502\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.4951 - val_loss: 2.9557\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4968 - val_loss: 2.9520\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.4972 - val_loss: 2.9582\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.4995 - val_loss: 2.9527\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4988 - val_loss: 2.9576\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4980 - val_loss: 2.9506\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.4951 - val_loss: 2.9548\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.4934 - val_loss: 2.9484\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.4906 - val_loss: 2.9523\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4892 - val_loss: 2.9473\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4875 - val_loss: 2.9502\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4862 - val_loss: 2.9461\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4850 - val_loss: 2.9485\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.4846 - val_loss: 2.9462\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4833 - val_loss: 2.9480\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4835 - val_loss: 2.9463\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4816 - val_loss: 2.9468\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.4802 - val_loss: 2.9451\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4781 - val_loss: 2.9444\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.4758 - val_loss: 2.9429\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.4744 - val_loss: 2.9432\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.4729 - val_loss: 2.9420\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4724 - val_loss: 2.9438\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.4722 - val_loss: 2.9430\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.4726 - val_loss: 2.9454\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.4733 - val_loss: 2.9454\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.4750 - val_loss: 2.9501\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4787 - val_loss: 2.9510\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.4826 - val_loss: 2.9583\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4884 - val_loss: 2.9573\n",
      "Epoch 1/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 57.2636WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 57.0502\n",
      "Epoch 2/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 53.7925WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 53.1710\n",
      "Epoch 3/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 47.7835WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 46.5508\n",
      "Epoch 4/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 38.3521WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 36.9947\n",
      "Epoch 5/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 26.9777WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 26.0404\n",
      "Epoch 6/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19.8701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.5686\n",
      "Epoch 7/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 17.6173WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.2299\n",
      "Epoch 8/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16.3252WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.2043\n",
      "Epoch 9/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15.3706WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.2428\n",
      "Epoch 10/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13.5820WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.1601\n",
      "Epoch 11/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11.1396WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.0155\n",
      "Epoch 12/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9.8455WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.6770\n",
      "Epoch 13/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.7728WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 8.8436\n",
      "Epoch 14/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.5092WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5039\n",
      "Epoch 15/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.2870WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.2527\n",
      "Epoch 16/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.7298WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0205\n",
      "Epoch 17/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.7734WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.7970\n",
      "Epoch 18/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.6880WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.6155\n",
      "Epoch 19/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.2910WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4406\n",
      "Epoch 20/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.3999WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2832\n",
      "Epoch 21/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.1021WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0635\n",
      "Epoch 22/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.9160WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.8493\n",
      "Epoch 23/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.7195WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6672\n",
      "Epoch 24/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.4297WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4521\n",
      "Epoch 25/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.2578WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2388\n",
      "Epoch 26/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.0858WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.0370\n",
      "Epoch 27/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.8393WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8314\n",
      "Epoch 28/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.6825WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6330\n",
      "Epoch 29/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.4668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.4348\n",
      "Epoch 30/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.2732WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 5.2461\n",
      "Epoch 31/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.1100WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.1104\n",
      "Epoch 32/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.7828WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.8974\n",
      "Epoch 33/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.6742WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.7485\n",
      "Epoch 34/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.6350WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5839\n",
      "Epoch 35/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.4923WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4740\n",
      "Epoch 36/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.3696WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3735\n",
      "Epoch 37/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.2023WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1995\n",
      "Epoch 38/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.1318WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1092\n",
      "Epoch 39/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.9965WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0191\n",
      "Epoch 40/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.0213WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.9533\n",
      "Epoch 41/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.9614WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.8835\n",
      "Epoch 42/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8734WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.8180\n",
      "Epoch 43/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8113WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.7839\n",
      "Epoch 44/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7505WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.7304\n",
      "Epoch 45/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.6933WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.6615\n",
      "Epoch 46/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.6158WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6320\n",
      "Epoch 47/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5910\n",
      "Epoch 48/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.6040WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5550\n",
      "Epoch 49/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5223WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5440\n",
      "Epoch 50/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4853WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4926\n",
      "Epoch 51/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4881WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4572\n",
      "Epoch 52/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4631WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4436\n",
      "Epoch 53/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3711WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4259\n",
      "Epoch 54/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3077WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3897\n",
      "Epoch 55/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3025WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3691\n",
      "Epoch 56/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3623WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3648\n",
      "Epoch 57/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3144WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.3354\n",
      "Epoch 58/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2834WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2871\n",
      "Epoch 59/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2635WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2670\n",
      "Epoch 60/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3036\n",
      "Epoch 61/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2621WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2740\n",
      "Epoch 62/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2540WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2514\n",
      "Epoch 63/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2267WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1994\n",
      "Epoch 64/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2088WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1997\n",
      "Epoch 65/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1849WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1604\n",
      "Epoch 66/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0778WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1327\n",
      "Epoch 67/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1255WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1446\n",
      "Epoch 68/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1255WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1038\n",
      "Epoch 69/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0380WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0938\n",
      "Epoch 70/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1627WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1228\n",
      "Epoch 71/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2172WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1738\n",
      "Epoch 72/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0867WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0668\n",
      "Epoch 73/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0793WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0545\n",
      "Epoch 74/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0302WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0352\n",
      "Epoch 75/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1762WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1369\n",
      "Epoch 76/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1289WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0777\n",
      "Epoch 77/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1034WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0535\n",
      "Epoch 78/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0245WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0029\n",
      "Epoch 79/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1099WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0242\n",
      "Epoch 80/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0834WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0468\n",
      "Epoch 81/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9853WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9981\n",
      "Epoch 82/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0004WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9722\n",
      "Epoch 83/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0009WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9551\n",
      "Epoch 84/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9868WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9652\n",
      "Epoch 85/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9662WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9787\n",
      "Epoch 86/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9965\n",
      "Epoch 87/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9932WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9376\n",
      "Epoch 88/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9806WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9963\n",
      "Epoch 89/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9435\n",
      "Epoch 90/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8627WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9113\n",
      "Epoch 91/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9391WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9433\n",
      "Epoch 92/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9897WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9749\n",
      "Epoch 93/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8771WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9256\n",
      "Epoch 94/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9911WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9912\n",
      "Epoch 95/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8818WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8906\n",
      "Epoch 96/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9546WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9404\n",
      "Epoch 97/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8710WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8861\n",
      "Epoch 98/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8929WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9037\n",
      "Epoch 99/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8666WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8931\n",
      "Epoch 100/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8439WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8742\n",
      "Epoch 101/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8997WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8784\n",
      "Epoch 102/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8675WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9336\n",
      "Epoch 103/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8367WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8590\n",
      "Epoch 104/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9577WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9240\n",
      "Epoch 105/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9451WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9099\n",
      "Epoch 106/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8628WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9164\n",
      "Epoch 107/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8985WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8796\n",
      "Epoch 108/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8452WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8538\n",
      "Epoch 109/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8514WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8332\n",
      "Epoch 110/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9316WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9193\n",
      "Epoch 111/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7755WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8434\n",
      "Epoch 112/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8748WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8624\n",
      "Epoch 113/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8231\n",
      "Epoch 114/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7799WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8077\n",
      "Epoch 115/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8431\n",
      "Epoch 116/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8555WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8273\n",
      "Epoch 117/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8398WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8469\n",
      "Epoch 118/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8983WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8945\n",
      "Epoch 119/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0061WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9450\n",
      "Epoch 120/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0165WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9506\n",
      "Epoch 121/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8009WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7909\n",
      "Epoch 122/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8044\n",
      "Epoch 123/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8364WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7834\n",
      "Epoch 124/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7507WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7866\n",
      "Epoch 125/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8145WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8150\n",
      "Epoch 126/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8069WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7990\n",
      "Epoch 127/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8233WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8086\n",
      "Epoch 128/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7838WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7982\n",
      "Epoch 129/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8121WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8228\n",
      "Epoch 130/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8420WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8069\n",
      "Epoch 131/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8154WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7612\n",
      "Epoch 132/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9300WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9184\n",
      "Epoch 133/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0211WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9504\n",
      "Epoch 134/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8018WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7821\n",
      "Epoch 135/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7864WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8032\n",
      "Epoch 136/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8120WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7749\n",
      "Epoch 137/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8017WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8147\n",
      "Epoch 138/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7648WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7762\n",
      "Epoch 139/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8843WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7917\n",
      "Epoch 140/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7538WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7902\n",
      "Epoch 141/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6980WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7427\n",
      "Epoch 142/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7869WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7640\n",
      "Epoch 143/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7973WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7867\n",
      "Epoch 144/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8650WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8937\n",
      "Epoch 145/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7411WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7849\n",
      "Epoch 146/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7584WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7301\n",
      "Epoch 147/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7618WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7399\n",
      "Epoch 148/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7074WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7574\n",
      "Epoch 149/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8651WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8429\n",
      "Epoch 150/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7669WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7963\n",
      "Epoch 151/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7656WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7611\n",
      "Epoch 152/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7416WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7532\n",
      "Epoch 153/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7613WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7607\n",
      "Epoch 154/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7957WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7721\n",
      "Epoch 155/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7251WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7520\n",
      "Epoch 156/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7617WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7333\n",
      "Epoch 157/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7336WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7490\n",
      "Epoch 158/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7734WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7272\n",
      "Epoch 159/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7812\n",
      "Epoch 160/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9716WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8805\n",
      "Epoch 161/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8536WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8001\n",
      "Epoch 162/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9825WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0057\n",
      "Epoch 163/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7194WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7084\n",
      "Epoch 164/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7073\n",
      "Epoch 165/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7220WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6970\n",
      "Epoch 166/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7399WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7265\n",
      "Epoch 167/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7549WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7632\n",
      "Epoch 168/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0281WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9523\n",
      "Epoch 169/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8330WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8158\n",
      "Epoch 170/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7578WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8197\n",
      "Epoch 171/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7582WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7724\n",
      "Epoch 172/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6968WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7322\n",
      "Epoch 173/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7299WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7042\n",
      "Epoch 174/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6236WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6871\n",
      "Epoch 175/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7409WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7589\n",
      "Epoch 176/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7172WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7122\n",
      "Epoch 177/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7117WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7005\n",
      "Epoch 178/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7174WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7091\n",
      "Epoch 179/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6570WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6948\n",
      "Epoch 180/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7976WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7356\n",
      "Epoch 181/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7534WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6973\n",
      "Epoch 182/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6855WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6854\n",
      "Epoch 183/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7118WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6791\n",
      "Epoch 184/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6342WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6662\n",
      "Epoch 185/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6397WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6673\n",
      "Epoch 186/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6894WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6673\n",
      "Epoch 187/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6690WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6863\n",
      "Epoch 188/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6601\n",
      "Epoch 189/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7084WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6900\n",
      "Epoch 190/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6880\n",
      "Epoch 191/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6965WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6810\n",
      "Epoch 192/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7240WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7227\n",
      "Epoch 193/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6866WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6894\n",
      "Epoch 194/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7565WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7193\n",
      "Epoch 195/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6651\n",
      "Epoch 196/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7178WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6716\n",
      "Epoch 197/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6981WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7038\n",
      "Epoch 198/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6304WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6571\n",
      "Epoch 199/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7526WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7324\n",
      "Epoch 200/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7028WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6807\n",
      "Epoch 201/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6016WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6382\n",
      "Epoch 202/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7060WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6373\n",
      "Epoch 203/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6806WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7056\n",
      "Epoch 204/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7032WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6589\n",
      "Epoch 205/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6177WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6345\n",
      "Epoch 206/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6665\n",
      "Epoch 207/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5716WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6361\n",
      "Epoch 208/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6833WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6736\n",
      "Epoch 209/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6246WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6483\n",
      "Epoch 210/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6216WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6397\n",
      "Epoch 211/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6440WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6364\n",
      "Epoch 212/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6654\n",
      "Epoch 213/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7075WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6381\n",
      "Epoch 214/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6112WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6251\n",
      "Epoch 215/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6491WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6438\n",
      "Epoch 216/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6182WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6228\n",
      "Epoch 217/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7280WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7294\n",
      "Epoch 218/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6987WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6875\n",
      "Epoch 219/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6168WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6292\n",
      "Epoch 220/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6484\n",
      "Epoch 221/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6634WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6329\n",
      "Epoch 222/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6118\n",
      "Epoch 223/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6081WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6171\n",
      "Epoch 224/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6353WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6758\n",
      "Epoch 225/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6324WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6203\n",
      "Epoch 226/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6881WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6306\n",
      "Epoch 227/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7442WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7337\n",
      "Epoch 228/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7259WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6803\n",
      "Epoch 229/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7184WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6983\n",
      "Epoch 230/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6576WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6431\n",
      "Epoch 231/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6204WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6441\n",
      "Epoch 232/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5951WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6134\n",
      "Epoch 233/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8436WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8217\n",
      "Epoch 234/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7280WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6677\n",
      "Epoch 235/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6261WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6085\n",
      "Epoch 236/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8247WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8243\n",
      "Epoch 237/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6329WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6022\n",
      "Epoch 238/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6862WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6567\n",
      "Epoch 239/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5690WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6060\n",
      "Epoch 240/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6126WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5932\n",
      "Epoch 241/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5711WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5928\n",
      "Epoch 242/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5911WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6036\n",
      "Epoch 243/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6298WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6240\n",
      "Epoch 244/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6465WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6117\n",
      "Epoch 245/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6871WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7230\n",
      "Epoch 246/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6411WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6233\n",
      "Epoch 247/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5452WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5788\n",
      "Epoch 248/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6484\n",
      "Epoch 249/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5841WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6435\n",
      "Epoch 250/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5648WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5862\n",
      "Epoch 251/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6129WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6124\n",
      "Epoch 252/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6015WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5904\n",
      "Epoch 253/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5810WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5827\n",
      "Epoch 254/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6677WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6200\n",
      "Epoch 255/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5490WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5737\n",
      "Epoch 256/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5620WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5828\n",
      "Epoch 257/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5468WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5928\n",
      "Epoch 258/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5820WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6097\n",
      "Epoch 259/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6663WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6702\n",
      "Epoch 260/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7613WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7283\n",
      "Epoch 261/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6524WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6432\n",
      "Epoch 262/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6123WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6490\n",
      "Epoch 263/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6099WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6192\n",
      "Epoch 264/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6467WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6521\n",
      "Epoch 265/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6357WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6140\n",
      "Epoch 266/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5112WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5667\n",
      "Epoch 267/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5774WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5722\n",
      "Epoch 268/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7419\n",
      "Epoch 269/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7093WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6840\n",
      "Epoch 270/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5966WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5770\n",
      "Epoch 271/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6190WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5816\n",
      "Epoch 272/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5888WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6060\n",
      "Epoch 273/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6678WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6339\n",
      "Epoch 274/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5484WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5812\n",
      "Epoch 275/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6051WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5860\n",
      "Epoch 276/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5784WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5661\n",
      "Epoch 277/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6352WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6488\n",
      "Epoch 278/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5675WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5960\n",
      "Epoch 279/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6630WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6513\n",
      "Epoch 280/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5110WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5694\n",
      "Epoch 281/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5047WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5501\n",
      "Epoch 282/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6739WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6250\n",
      "Epoch 283/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6237WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6041\n",
      "Epoch 284/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7004WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6803\n",
      "Epoch 285/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5587WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5628\n",
      "Epoch 286/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6092WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5928\n",
      "Epoch 287/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6285WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6206\n",
      "Epoch 288/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6219\n",
      "Epoch 289/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6234WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6333\n",
      "Epoch 290/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5109WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5555\n",
      "Epoch 291/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5297WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5547\n",
      "Epoch 292/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5116WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5365\n",
      "Epoch 293/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5476WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5759\n",
      "Epoch 294/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5782WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5560\n",
      "Epoch 295/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5476WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5595\n",
      "Epoch 296/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4801WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5481\n",
      "Epoch 297/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5405WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5522\n",
      "Epoch 298/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6033WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5322\n",
      "Epoch 299/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5555WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5352\n",
      "Epoch 300/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5740WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5484\n",
      "Epoch 301/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6219WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5998\n",
      "Epoch 302/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5175WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5434\n",
      "Epoch 303/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6483WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6684\n",
      "Epoch 304/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6765\n",
      "Epoch 305/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5566\n",
      "Epoch 306/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5545WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5247\n",
      "Epoch 307/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5034WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5315\n",
      "Epoch 308/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5112WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5281\n",
      "Epoch 309/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5332WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5718\n",
      "Epoch 310/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6061WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5640\n",
      "Epoch 311/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6039WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6150\n",
      "Epoch 312/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5565WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5402\n",
      "Epoch 313/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5611WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5386\n",
      "Epoch 314/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6791WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6498\n",
      "Epoch 315/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6100WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5684\n",
      "Epoch 316/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5306WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5363\n",
      "Epoch 317/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5565WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5251\n",
      "Epoch 318/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5405WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5171\n",
      "Epoch 319/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5618WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5523\n",
      "Epoch 320/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5268WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5511\n",
      "Epoch 321/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6679WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6005\n",
      "Epoch 322/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5242WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5350\n",
      "Epoch 323/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5712\n",
      "Epoch 324/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4800WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5226\n",
      "Epoch 325/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5642WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5342\n",
      "Epoch 326/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5152\n",
      "Epoch 327/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5319WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5116\n",
      "Epoch 328/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5267WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5105\n",
      "Epoch 329/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5181WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5221\n",
      "Epoch 330/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5765WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5253\n",
      "Epoch 331/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4974WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5277\n",
      "Epoch 332/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5338WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5605\n",
      "Epoch 333/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5294WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5245\n",
      "Epoch 334/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5251WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5441\n",
      "Epoch 335/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5038WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5080\n",
      "Epoch 336/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5742WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5611\n",
      "Epoch 337/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6253WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5552\n",
      "Epoch 338/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4975WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5314\n",
      "Epoch 339/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6064WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6021\n",
      "Epoch 340/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5807WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5352\n",
      "Epoch 341/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5480WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5030\n",
      "Epoch 342/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4992WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4981\n",
      "Epoch 343/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4934WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5173\n",
      "Epoch 344/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5751WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5494\n",
      "Epoch 345/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5224WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5829\n",
      "Epoch 346/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4920WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5117\n",
      "Epoch 347/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4876WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5105\n",
      "Epoch 348/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5194WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5570\n",
      "Epoch 349/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4721WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4996\n",
      "Epoch 350/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5450WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5667\n",
      "Epoch 351/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6266\n",
      "Epoch 352/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6110WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5868\n",
      "Epoch 353/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4395WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5080\n",
      "Epoch 354/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5732WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5967\n",
      "Epoch 355/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5629WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5396\n",
      "Epoch 356/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4992WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4946\n",
      "Epoch 357/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5316WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5296\n",
      "Epoch 358/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4837WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4932\n",
      "Epoch 359/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4882\n",
      "Epoch 360/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4824WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5221\n",
      "Epoch 361/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5544WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5374\n",
      "Epoch 362/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4908WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5148\n",
      "Epoch 363/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4734WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5245\n",
      "Epoch 364/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5461WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5728\n",
      "Epoch 365/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5429WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5280\n",
      "Epoch 366/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5027WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4844\n",
      "Epoch 367/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5296WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5157\n",
      "Epoch 368/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4675WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4787\n",
      "Epoch 369/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5617WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5145\n",
      "Epoch 370/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5238WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.4998\n",
      "Epoch 371/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4669WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4914\n",
      "Epoch 372/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4388WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4921\n",
      "Epoch 373/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4998WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4779\n",
      "Epoch 374/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4841\n",
      "Epoch 375/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4792WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4843\n",
      "Epoch 376/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5347WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5297\n",
      "Epoch 377/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6581WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6199\n",
      "Epoch 378/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4848WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4877\n",
      "Epoch 379/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4801\n",
      "Epoch 380/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5743WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5431\n",
      "Epoch 381/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4826WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4820\n",
      "Epoch 382/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5648WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5338\n",
      "Epoch 383/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5238WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4893\n",
      "Epoch 384/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6453WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5897\n",
      "Epoch 385/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5957WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5565\n",
      "Epoch 386/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5137\n",
      "Epoch 387/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5179WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5712\n",
      "Epoch 388/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5231WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5091\n",
      "Epoch 389/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5741WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5389\n",
      "Epoch 390/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5133WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4792\n",
      "Epoch 391/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5969WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5537\n",
      "Epoch 392/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5421WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4795\n",
      "Epoch 393/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4947WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5294\n",
      "Epoch 394/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5062WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4975\n",
      "Epoch 395/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4937WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4976\n",
      "Epoch 396/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6326WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5999\n",
      "Epoch 397/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5859WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5757\n",
      "Epoch 398/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5826WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6114\n",
      "Epoch 399/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5066WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4690\n",
      "Epoch 400/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4808WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4658\n",
      "Epoch 401/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5027WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4718\n",
      "Epoch 402/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4979WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4788\n",
      "Epoch 403/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5162WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4781\n",
      "Epoch 404/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5543WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5242\n",
      "Epoch 405/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5352WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5245\n",
      "Epoch 406/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4808WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4619\n",
      "Epoch 407/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4945WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4987\n",
      "Epoch 408/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4753WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4751\n",
      "Epoch 409/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4639\n",
      "Epoch 410/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4368WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4520\n",
      "Epoch 411/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5102WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5143\n",
      "Epoch 412/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4324WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4573\n",
      "Epoch 413/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4515WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4690\n",
      "Epoch 414/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6283WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6140\n",
      "Epoch 415/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4916WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4588\n",
      "Epoch 416/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4875WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4615\n",
      "Epoch 417/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4692WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4765\n",
      "Epoch 418/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5487WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5230\n",
      "Epoch 419/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5655WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5018\n",
      "Epoch 420/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4302WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4516\n",
      "Epoch 421/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4122WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4424\n",
      "Epoch 422/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4324WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4502\n",
      "Epoch 423/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4915WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4585\n",
      "Epoch 424/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5340WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4921\n",
      "Epoch 425/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4666WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4846\n",
      "Epoch 426/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4363WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4657\n",
      "Epoch 427/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5344WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5064\n",
      "Epoch 428/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5186WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4665\n",
      "Epoch 429/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5439WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4915\n",
      "Epoch 430/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4541WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4570\n",
      "Epoch 431/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4752WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4737\n",
      "Epoch 432/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5180\n",
      "Epoch 433/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5059\n",
      "Epoch 434/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4413\n",
      "Epoch 435/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5089WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4778\n",
      "Epoch 436/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4582WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5007\n",
      "Epoch 437/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6370WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5993\n",
      "Epoch 438/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5176WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5256\n",
      "Epoch 439/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4779WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4551\n",
      "Epoch 440/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5108WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5167\n",
      "Epoch 441/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4786WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5045\n",
      "Epoch 442/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6138WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5360\n",
      "Epoch 443/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4433WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4358\n",
      "Epoch 444/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3922WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4405\n",
      "Epoch 445/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5454WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5261\n",
      "Epoch 446/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5028WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4822\n",
      "Epoch 447/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4757WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4404\n",
      "Epoch 448/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4994WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4822\n",
      "Epoch 449/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4686\n",
      "Epoch 450/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4225WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4417\n",
      "Epoch 451/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4228WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4230\n",
      "Epoch 452/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5091WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4683\n",
      "Epoch 453/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4662WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4712\n",
      "Epoch 454/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4209WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4313\n",
      "Epoch 455/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4513WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4656\n",
      "Epoch 456/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4742WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4759\n",
      "Epoch 457/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4072WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4317\n",
      "Epoch 458/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5396WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5080\n",
      "Epoch 459/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4617WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4787\n",
      "Epoch 460/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3915WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4497\n",
      "Epoch 461/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4601WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4244\n",
      "Epoch 462/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4431WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4612\n",
      "Epoch 463/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4833WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4643\n",
      "Epoch 464/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4883WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4950\n",
      "Epoch 465/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5293WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4910\n",
      "Epoch 466/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4265WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4232\n",
      "Epoch 467/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4241WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4304\n",
      "Epoch 468/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4403WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4462\n",
      "Epoch 469/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4380WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4908\n",
      "Epoch 470/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4875WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4482\n",
      "Epoch 471/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4062WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4448\n",
      "Epoch 472/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4396WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4315\n",
      "Epoch 473/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5106WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4447\n",
      "Epoch 474/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4988WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4557\n",
      "Epoch 475/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4874WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4706\n",
      "Epoch 476/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3953WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4182\n",
      "Epoch 477/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4379WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4732\n",
      "Epoch 478/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3553WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4462\n",
      "Epoch 479/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4784WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4306\n",
      "Epoch 480/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4346WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4408\n",
      "Epoch 481/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4318WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4157\n",
      "Epoch 482/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4392WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4166\n",
      "Epoch 483/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5161WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5004\n",
      "Epoch 484/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4531WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4703\n",
      "Epoch 485/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3802WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4185\n",
      "Epoch 486/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5711WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4813\n",
      "Epoch 487/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4663WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5177\n",
      "Epoch 488/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4558WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4312\n",
      "Epoch 489/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4237WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4037\n",
      "Epoch 490/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4531WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4468\n",
      "Epoch 491/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4236WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4488\n",
      "Epoch 492/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4480WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4358\n",
      "Epoch 493/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4219\n",
      "Epoch 494/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3972WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4069\n",
      "Epoch 495/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4997WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4695\n",
      "Epoch 496/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4925WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4582\n",
      "Epoch 497/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4337WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4019\n",
      "Epoch 498/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3855WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4001\n",
      "Epoch 499/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4373WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4028\n",
      "Epoch 500/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4155WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3959\n",
      "Epoch 501/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4591WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4840\n",
      "Epoch 502/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3956WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4258\n",
      "Epoch 503/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4820WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4299\n",
      "Epoch 504/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3872WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4513\n",
      "Epoch 505/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5407WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4529\n",
      "Epoch 506/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6076WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6014\n",
      "Epoch 507/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4360WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4174\n",
      "Epoch 508/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4228WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4114\n",
      "Epoch 509/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4026WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4492\n",
      "Epoch 510/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4281WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4700\n",
      "Epoch 511/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4063\n",
      "Epoch 512/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4965WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4556\n",
      "Epoch 513/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5499WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.5541\n",
      "Epoch 514/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3962\n",
      "Epoch 515/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3959WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4013\n",
      "Epoch 516/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4505WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4372\n",
      "Epoch 517/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3400WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3938\n",
      "Epoch 518/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4629WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4456\n",
      "Epoch 519/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4209WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4253\n",
      "Epoch 520/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4077WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3907\n",
      "Epoch 521/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4699WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4606\n",
      "Epoch 522/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4293WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4280\n",
      "Epoch 523/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4096WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4148\n",
      "Epoch 524/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.373 - ETA: 0s - loss: 2.3903WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 2.3903\n",
      "Epoch 525/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4665WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5100\n",
      "Epoch 526/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3718WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4082\n",
      "Epoch 527/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4902WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4976\n",
      "Epoch 528/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5099WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4564\n",
      "Epoch 529/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4008WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3915\n",
      "Epoch 530/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4652WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4592\n",
      "Epoch 531/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4188WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4151\n",
      "Epoch 532/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4406WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4109\n",
      "Epoch 533/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6153WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5701\n",
      "Epoch 534/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3594WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4058\n",
      "Epoch 535/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4157WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4132\n",
      "Epoch 536/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3676WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3993\n",
      "Epoch 537/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5433WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5327\n",
      "Epoch 538/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3777WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4023\n",
      "Epoch 539/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4592WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4077\n",
      "Epoch 540/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4364WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4279\n",
      "Epoch 541/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4742WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4583\n",
      "Epoch 542/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3979WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3921\n",
      "Epoch 543/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4640WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4125\n",
      "Epoch 544/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3999\n",
      "Epoch 545/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3286WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3830\n",
      "Epoch 546/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4320WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4278\n",
      "Epoch 547/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4203WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3981\n",
      "Epoch 548/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5171WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5235\n",
      "Epoch 549/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4441WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4356\n",
      "Epoch 550/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4062WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4079\n",
      "Epoch 551/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4406WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4341\n",
      "Epoch 552/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3916WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3870\n",
      "Epoch 553/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4060WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4120\n",
      "Epoch 554/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3738WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3959\n",
      "Epoch 555/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4883WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4511\n",
      "Epoch 556/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3646WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3821\n",
      "Epoch 557/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4730WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4290\n",
      "Epoch 558/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3686WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3801\n",
      "Epoch 559/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4151WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4156\n",
      "Epoch 560/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4664WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4729\n",
      "Epoch 561/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3623WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3739\n",
      "Epoch 562/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3927WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.4110\n",
      "Epoch 563/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4919WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4456\n",
      "Epoch 564/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3557WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3752\n",
      "Epoch 565/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4216\n",
      "Epoch 566/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4916WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5268\n",
      "Epoch 567/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3991\n",
      "Epoch 568/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4020WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4077\n",
      "Epoch 569/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3518WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4129\n",
      "Epoch 570/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4036\n",
      "Epoch 571/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3475WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3867\n",
      "Epoch 572/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3912WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3757\n",
      "Epoch 573/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5158WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4288\n",
      "Epoch 574/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4438WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4447\n",
      "Epoch 575/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4730WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4314\n",
      "Epoch 576/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4173WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3952\n",
      "Epoch 577/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3874\n",
      "Epoch 578/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3777WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3757\n",
      "Epoch 579/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3641WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3886\n",
      "Epoch 580/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4858WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.4375\n",
      "Epoch 581/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4408WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.3862\n",
      "Epoch 582/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3453WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.3776\n",
      "Epoch 583/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3867WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3852\n",
      "Epoch 584/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3720WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4017\n",
      "Epoch 585/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4393WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3990\n",
      "Epoch 586/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4660WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4453\n",
      "Epoch 587/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4794WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4478\n",
      "Epoch 588/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4025WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3915\n",
      "Epoch 589/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4369WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3710\n",
      "Epoch 590/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4056WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3817\n",
      "Epoch 591/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3871WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4028\n",
      "Epoch 592/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3908WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3931\n",
      "Epoch 593/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5376WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4963\n",
      "Epoch 594/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3988WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3608\n",
      "Epoch 595/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3007WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3776\n",
      "Epoch 596/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4246WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4154\n",
      "Epoch 597/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4249WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3823\n",
      "Epoch 598/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3775WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3880\n",
      "Epoch 599/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4493WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4156\n",
      "Epoch 600/600\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3960WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [57.05015563964844,\n",
       "  53.171043395996094,\n",
       "  46.5507926940918,\n",
       "  36.99473190307617,\n",
       "  26.040374755859375,\n",
       "  19.568592071533203,\n",
       "  17.22986602783203,\n",
       "  16.204267501831055,\n",
       "  15.242791175842285,\n",
       "  13.160062789916992,\n",
       "  11.015542030334473,\n",
       "  9.67701530456543,\n",
       "  8.843560218811035,\n",
       "  8.503936767578125,\n",
       "  8.25266170501709,\n",
       "  8.02049446105957,\n",
       "  7.79696798324585,\n",
       "  7.6155219078063965,\n",
       "  7.440563201904297,\n",
       "  7.283231735229492,\n",
       "  7.063515663146973,\n",
       "  6.849289417266846,\n",
       "  6.667182445526123,\n",
       "  6.452054977416992,\n",
       "  6.238813400268555,\n",
       "  6.037026405334473,\n",
       "  5.831389427185059,\n",
       "  5.63300085067749,\n",
       "  5.434781551361084,\n",
       "  5.246070861816406,\n",
       "  5.110358715057373,\n",
       "  4.897441864013672,\n",
       "  4.748498439788818,\n",
       "  4.5839056968688965,\n",
       "  4.473978042602539,\n",
       "  4.373469352722168,\n",
       "  4.199521064758301,\n",
       "  4.109156131744385,\n",
       "  4.019137859344482,\n",
       "  3.9532527923583984,\n",
       "  3.8834526538848877,\n",
       "  3.818004608154297,\n",
       "  3.7838854789733887,\n",
       "  3.730419635772705,\n",
       "  3.661461591720581,\n",
       "  3.632031202316284,\n",
       "  3.590956687927246,\n",
       "  3.5550239086151123,\n",
       "  3.5440127849578857,\n",
       "  3.4925546646118164,\n",
       "  3.4572091102600098,\n",
       "  3.443572759628296,\n",
       "  3.425926446914673,\n",
       "  3.3896894454956055,\n",
       "  3.369121789932251,\n",
       "  3.3647656440734863,\n",
       "  3.335360527038574,\n",
       "  3.287108898162842,\n",
       "  3.266954183578491,\n",
       "  3.303637742996216,\n",
       "  3.2739531993865967,\n",
       "  3.2514090538024902,\n",
       "  3.199395179748535,\n",
       "  3.199702024459839,\n",
       "  3.1604068279266357,\n",
       "  3.132741689682007,\n",
       "  3.144596576690674,\n",
       "  3.1038286685943604,\n",
       "  3.0938284397125244,\n",
       "  3.122774362564087,\n",
       "  3.1738085746765137,\n",
       "  3.066840410232544,\n",
       "  3.054506540298462,\n",
       "  3.0351626873016357,\n",
       "  3.136894941329956,\n",
       "  3.077684164047241,\n",
       "  3.0535356998443604,\n",
       "  3.002920150756836,\n",
       "  3.024195909500122,\n",
       "  3.046754837036133,\n",
       "  2.998119354248047,\n",
       "  2.9722111225128174,\n",
       "  2.9551191329956055,\n",
       "  2.9651708602905273,\n",
       "  2.978671073913574,\n",
       "  2.9965498447418213,\n",
       "  2.9375855922698975,\n",
       "  2.996260643005371,\n",
       "  2.9435112476348877,\n",
       "  2.9113099575042725,\n",
       "  2.9433491230010986,\n",
       "  2.9748549461364746,\n",
       "  2.9255576133728027,\n",
       "  2.9912426471710205,\n",
       "  2.8905813694000244,\n",
       "  2.9403810501098633,\n",
       "  2.88608980178833,\n",
       "  2.9036879539489746,\n",
       "  2.893118143081665,\n",
       "  2.8741626739501953,\n",
       "  2.8783984184265137,\n",
       "  2.933594226837158,\n",
       "  2.85904598236084,\n",
       "  2.92401123046875,\n",
       "  2.9098846912384033,\n",
       "  2.916360855102539,\n",
       "  2.8796164989471436,\n",
       "  2.8537614345550537,\n",
       "  2.8332161903381348,\n",
       "  2.9192755222320557,\n",
       "  2.8434388637542725,\n",
       "  2.862433671951294,\n",
       "  2.823143720626831,\n",
       "  2.807666540145874,\n",
       "  2.8430659770965576,\n",
       "  2.8272688388824463,\n",
       "  2.8469247817993164,\n",
       "  2.8944969177246094,\n",
       "  2.9450111389160156,\n",
       "  2.9506006240844727,\n",
       "  2.7909486293792725,\n",
       "  2.8043887615203857,\n",
       "  2.7833847999572754,\n",
       "  2.7865631580352783,\n",
       "  2.815028667449951,\n",
       "  2.7990024089813232,\n",
       "  2.8086183071136475,\n",
       "  2.798243284225464,\n",
       "  2.8228249549865723,\n",
       "  2.806852340698242,\n",
       "  2.7611923217773438,\n",
       "  2.918436050415039,\n",
       "  2.95039439201355,\n",
       "  2.7820615768432617,\n",
       "  2.803156614303589,\n",
       "  2.774945020675659,\n",
       "  2.8147330284118652,\n",
       "  2.776195526123047,\n",
       "  2.791710615158081,\n",
       "  2.790222644805908,\n",
       "  2.742671012878418,\n",
       "  2.763984203338623,\n",
       "  2.7866978645324707,\n",
       "  2.8937408924102783,\n",
       "  2.784947633743286,\n",
       "  2.730142116546631,\n",
       "  2.7399022579193115,\n",
       "  2.7573938369750977,\n",
       "  2.842893600463867,\n",
       "  2.7963461875915527,\n",
       "  2.761106252670288,\n",
       "  2.7531845569610596,\n",
       "  2.760744333267212,\n",
       "  2.772127628326416,\n",
       "  2.7519936561584473,\n",
       "  2.7332825660705566,\n",
       "  2.748987913131714,\n",
       "  2.7272112369537354,\n",
       "  2.781172037124634,\n",
       "  2.8805084228515625,\n",
       "  2.8001110553741455,\n",
       "  3.0057003498077393,\n",
       "  2.708376884460449,\n",
       "  2.7072784900665283,\n",
       "  2.6970157623291016,\n",
       "  2.7265353202819824,\n",
       "  2.7632088661193848,\n",
       "  2.952293634414673,\n",
       "  2.815783739089966,\n",
       "  2.819659471511841,\n",
       "  2.772444725036621,\n",
       "  2.73219633102417,\n",
       "  2.7041821479797363,\n",
       "  2.6870670318603516,\n",
       "  2.7588846683502197,\n",
       "  2.712233304977417,\n",
       "  2.700457811355591,\n",
       "  2.709089756011963,\n",
       "  2.6948423385620117,\n",
       "  2.7355825901031494,\n",
       "  2.697323799133301,\n",
       "  2.685359001159668,\n",
       "  2.67913818359375,\n",
       "  2.6662113666534424,\n",
       "  2.6673214435577393,\n",
       "  2.6673333644866943,\n",
       "  2.686296224594116,\n",
       "  2.660104274749756,\n",
       "  2.690028429031372,\n",
       "  2.688021183013916,\n",
       "  2.6810355186462402,\n",
       "  2.722681999206543,\n",
       "  2.689366102218628,\n",
       "  2.71928334236145,\n",
       "  2.665102481842041,\n",
       "  2.6715919971466064,\n",
       "  2.703810930252075,\n",
       "  2.657120943069458,\n",
       "  2.7323503494262695,\n",
       "  2.6807339191436768,\n",
       "  2.6381518840789795,\n",
       "  2.6373472213745117,\n",
       "  2.705605983734131,\n",
       "  2.658881187438965,\n",
       "  2.6344809532165527,\n",
       "  2.666494369506836,\n",
       "  2.6360983848571777,\n",
       "  2.673619031906128,\n",
       "  2.64833664894104,\n",
       "  2.6397104263305664,\n",
       "  2.636373281478882,\n",
       "  2.665415048599243,\n",
       "  2.638094902038574,\n",
       "  2.6251354217529297,\n",
       "  2.6438169479370117,\n",
       "  2.622788667678833,\n",
       "  2.7294015884399414,\n",
       "  2.6875059604644775,\n",
       "  2.6291606426239014,\n",
       "  2.648378610610962,\n",
       "  2.632855176925659,\n",
       "  2.611820936203003,\n",
       "  2.6170942783355713,\n",
       "  2.6757586002349854,\n",
       "  2.6202573776245117,\n",
       "  2.630568027496338,\n",
       "  2.733743190765381,\n",
       "  2.680302619934082,\n",
       "  2.6983211040496826,\n",
       "  2.643141746520996,\n",
       "  2.6441092491149902,\n",
       "  2.613419771194458,\n",
       "  2.8217220306396484,\n",
       "  2.667710542678833,\n",
       "  2.6085286140441895,\n",
       "  2.8243210315704346,\n",
       "  2.6022167205810547,\n",
       "  2.6566882133483887,\n",
       "  2.6060421466827393,\n",
       "  2.5932281017303467,\n",
       "  2.592820882797241,\n",
       "  2.603624105453491,\n",
       "  2.623983860015869,\n",
       "  2.611696720123291,\n",
       "  2.723045587539673,\n",
       "  2.62325119972229,\n",
       "  2.578836679458618,\n",
       "  2.648418664932251,\n",
       "  2.643479824066162,\n",
       "  2.5862088203430176,\n",
       "  2.6124157905578613,\n",
       "  2.590397357940674,\n",
       "  2.582683563232422,\n",
       "  2.6199636459350586,\n",
       "  2.5736706256866455,\n",
       "  2.582836627960205,\n",
       "  2.592820644378662,\n",
       "  2.6096673011779785,\n",
       "  2.6702303886413574,\n",
       "  2.72831130027771,\n",
       "  2.643195867538452,\n",
       "  2.64898681640625,\n",
       "  2.619241714477539,\n",
       "  2.652068853378296,\n",
       "  2.613999605178833,\n",
       "  2.566650867462158,\n",
       "  2.572200298309326,\n",
       "  2.7419140338897705,\n",
       "  2.68395733833313,\n",
       "  2.5770485401153564,\n",
       "  2.581599235534668,\n",
       "  2.6059634685516357,\n",
       "  2.633909225463867,\n",
       "  2.581220865249634,\n",
       "  2.5860049724578857,\n",
       "  2.566133499145508,\n",
       "  2.6487598419189453,\n",
       "  2.595963478088379,\n",
       "  2.6512770652770996,\n",
       "  2.569364309310913,\n",
       "  2.550092935562134,\n",
       "  2.6250102519989014,\n",
       "  2.604055881500244,\n",
       "  2.6803088188171387,\n",
       "  2.562800645828247,\n",
       "  2.5927631855010986,\n",
       "  2.6205735206604004,\n",
       "  2.6218655109405518,\n",
       "  2.633284330368042,\n",
       "  2.555542230606079,\n",
       "  2.554687261581421,\n",
       "  2.536480188369751,\n",
       "  2.575895071029663,\n",
       "  2.556037425994873,\n",
       "  2.5595316886901855,\n",
       "  2.5481109619140625,\n",
       "  2.5522379875183105,\n",
       "  2.53224778175354,\n",
       "  2.5352261066436768,\n",
       "  2.548434257507324,\n",
       "  2.599816083908081,\n",
       "  2.543426275253296,\n",
       "  2.6683592796325684,\n",
       "  2.6764748096466064,\n",
       "  2.55656361579895,\n",
       "  2.5246739387512207,\n",
       "  2.5315358638763428,\n",
       "  2.5280909538269043,\n",
       "  2.5718092918395996,\n",
       "  2.5639941692352295,\n",
       "  2.6150033473968506,\n",
       "  2.5402004718780518,\n",
       "  2.5385589599609375,\n",
       "  2.6498239040374756,\n",
       "  2.5684192180633545,\n",
       "  2.5363383293151855,\n",
       "  2.5250606536865234,\n",
       "  2.5170717239379883,\n",
       "  2.5523083209991455,\n",
       "  2.5510687828063965,\n",
       "  2.6005489826202393,\n",
       "  2.5349771976470947,\n",
       "  2.571152687072754,\n",
       "  2.522552967071533,\n",
       "  2.534198522567749,\n",
       "  2.515211343765259,\n",
       "  2.5116305351257324,\n",
       "  2.5105197429656982,\n",
       "  2.5220820903778076,\n",
       "  2.525291919708252,\n",
       "  2.527719736099243,\n",
       "  2.5604870319366455,\n",
       "  2.5244596004486084,\n",
       "  2.5440807342529297,\n",
       "  2.508033514022827,\n",
       "  2.5611233711242676,\n",
       "  2.555194139480591,\n",
       "  2.531423807144165,\n",
       "  2.60207462310791,\n",
       "  2.535169839859009,\n",
       "  2.5030364990234375,\n",
       "  2.4981274604797363,\n",
       "  2.517324447631836,\n",
       "  2.5494470596313477,\n",
       "  2.58286714553833,\n",
       "  2.511728286743164,\n",
       "  2.5105299949645996,\n",
       "  2.5569915771484375,\n",
       "  2.499582290649414,\n",
       "  2.56667160987854,\n",
       "  2.626641273498535,\n",
       "  2.5868148803710938,\n",
       "  2.5080082416534424,\n",
       "  2.596684694290161,\n",
       "  2.5395617485046387,\n",
       "  2.4946277141571045,\n",
       "  2.5295655727386475,\n",
       "  2.493165969848633,\n",
       "  2.488191843032837,\n",
       "  2.522106170654297,\n",
       "  2.5373613834381104,\n",
       "  2.5148158073425293,\n",
       "  2.5244596004486084,\n",
       "  2.572838544845581,\n",
       "  2.5279533863067627,\n",
       "  2.4844000339508057,\n",
       "  2.5156733989715576,\n",
       "  2.478691816329956,\n",
       "  2.5145273208618164,\n",
       "  2.499807596206665,\n",
       "  2.4914212226867676,\n",
       "  2.4921135902404785,\n",
       "  2.477893829345703,\n",
       "  2.48410701751709,\n",
       "  2.4843268394470215,\n",
       "  2.529676675796509,\n",
       "  2.6198744773864746,\n",
       "  2.4877490997314453,\n",
       "  2.480064868927002,\n",
       "  2.5431339740753174,\n",
       "  2.481954336166382,\n",
       "  2.5337677001953125,\n",
       "  2.4893381595611572,\n",
       "  2.5897088050842285,\n",
       "  2.5564732551574707,\n",
       "  2.513730049133301,\n",
       "  2.5711886882781982,\n",
       "  2.5090560913085938,\n",
       "  2.5388898849487305,\n",
       "  2.4792027473449707,\n",
       "  2.553745985031128,\n",
       "  2.4794602394104004,\n",
       "  2.529370069503784,\n",
       "  2.4975478649139404,\n",
       "  2.497626543045044,\n",
       "  2.5998504161834717,\n",
       "  2.5756685733795166,\n",
       "  2.6113834381103516,\n",
       "  2.4689602851867676,\n",
       "  2.465780019760132,\n",
       "  2.4717540740966797,\n",
       "  2.478799343109131,\n",
       "  2.4781014919281006,\n",
       "  2.5241832733154297,\n",
       "  2.5244593620300293,\n",
       "  2.461851119995117,\n",
       "  2.498656988143921,\n",
       "  2.4750986099243164,\n",
       "  2.463900089263916,\n",
       "  2.4520010948181152,\n",
       "  2.514326572418213,\n",
       "  2.457253932952881,\n",
       "  2.4689855575561523,\n",
       "  2.613985538482666,\n",
       "  2.4588253498077393,\n",
       "  2.461470127105713,\n",
       "  2.4764842987060547,\n",
       "  2.522975444793701,\n",
       "  2.5017917156219482,\n",
       "  2.4516327381134033,\n",
       "  2.4424140453338623,\n",
       "  2.4502289295196533,\n",
       "  2.4584600925445557,\n",
       "  2.492105007171631,\n",
       "  2.4845917224884033,\n",
       "  2.4656662940979004,\n",
       "  2.506370782852173,\n",
       "  2.46653151512146,\n",
       "  2.4915127754211426,\n",
       "  2.456967353820801,\n",
       "  2.47367787361145,\n",
       "  2.5179543495178223,\n",
       "  2.5058672428131104,\n",
       "  2.441317558288574,\n",
       "  2.477764129638672,\n",
       "  2.5007200241088867,\n",
       "  2.599250316619873,\n",
       "  2.5255820751190186,\n",
       "  2.4551477432250977,\n",
       "  2.5167300701141357,\n",
       "  2.5044705867767334,\n",
       "  2.5359511375427246,\n",
       "  2.4357526302337646,\n",
       "  2.4405300617218018,\n",
       "  2.5261154174804688,\n",
       "  2.4822299480438232,\n",
       "  2.4404003620147705,\n",
       "  2.4822096824645996,\n",
       "  2.468590497970581,\n",
       "  2.441699981689453,\n",
       "  2.422950267791748,\n",
       "  2.4682624340057373,\n",
       "  2.471165895462036,\n",
       "  2.4312636852264404,\n",
       "  2.4656264781951904,\n",
       "  2.47591233253479,\n",
       "  2.4316587448120117,\n",
       "  2.507988214492798,\n",
       "  2.4787425994873047,\n",
       "  2.449662208557129,\n",
       "  2.4243698120117188,\n",
       "  2.4611892700195312,\n",
       "  2.464306354522705,\n",
       "  2.4950015544891357,\n",
       "  2.490992546081543,\n",
       "  2.4232099056243896,\n",
       "  2.4303698539733887,\n",
       "  2.4461896419525146,\n",
       "  2.4907708168029785,\n",
       "  2.4482181072235107,\n",
       "  2.444761037826538,\n",
       "  2.4314582347869873,\n",
       "  2.4446613788604736,\n",
       "  2.455726385116577,\n",
       "  2.4706318378448486,\n",
       "  2.4181737899780273,\n",
       "  2.4732205867767334,\n",
       "  2.446167469024658,\n",
       "  2.4306414127349854,\n",
       "  2.44084095954895,\n",
       "  2.4156765937805176,\n",
       "  2.41664981842041,\n",
       "  2.500387191772461,\n",
       "  2.470303773880005,\n",
       "  2.418515920639038,\n",
       "  2.481311798095703,\n",
       "  2.517693042755127,\n",
       "  2.4311554431915283,\n",
       "  2.403721809387207,\n",
       "  2.446835517883301,\n",
       "  2.4487531185150146,\n",
       "  2.4358208179473877,\n",
       "  2.421880006790161,\n",
       "  2.4068703651428223,\n",
       "  2.469491958618164,\n",
       "  2.4581661224365234,\n",
       "  2.4019243717193604,\n",
       "  2.400050401687622,\n",
       "  2.4027867317199707,\n",
       "  2.3959460258483887,\n",
       "  2.483964681625366,\n",
       "  2.4257876873016357,\n",
       "  2.4299066066741943,\n",
       "  2.4512531757354736,\n",
       "  2.452897071838379,\n",
       "  2.60141921043396,\n",
       "  2.4174036979675293,\n",
       "  2.411419630050659,\n",
       "  2.4492299556732178,\n",
       "  2.4699697494506836,\n",
       "  2.4063403606414795,\n",
       "  2.4556145668029785,\n",
       "  2.5541133880615234,\n",
       "  2.396237373352051,\n",
       "  2.4013164043426514,\n",
       "  2.437208890914917,\n",
       "  2.3938181400299072,\n",
       "  2.445596694946289,\n",
       "  2.4253101348876953,\n",
       "  2.390693187713623,\n",
       "  2.460620880126953,\n",
       "  2.428041458129883,\n",
       "  2.4148447513580322,\n",
       "  2.3902761936187744,\n",
       "  2.5099525451660156,\n",
       "  2.4082143306732178,\n",
       "  2.4976046085357666,\n",
       "  2.4563913345336914,\n",
       "  2.391467332839966,\n",
       "  2.4592373371124268,\n",
       "  2.4151158332824707,\n",
       "  2.4108738899230957,\n",
       "  2.570080518722534,\n",
       "  2.405839681625366,\n",
       "  2.4132022857666016,\n",
       "  2.3993184566497803,\n",
       "  2.532680034637451,\n",
       "  2.4023170471191406,\n",
       "  2.407700777053833,\n",
       "  2.4278979301452637,\n",
       "  2.458268642425537,\n",
       "  2.3921070098876953,\n",
       "  2.412468433380127,\n",
       "  2.39992094039917,\n",
       "  2.383014678955078,\n",
       "  2.427793264389038,\n",
       "  2.3981404304504395,\n",
       "  2.523536205291748,\n",
       "  2.4355645179748535,\n",
       "  2.4078991413116455,\n",
       "  2.434112548828125,\n",
       "  2.3870468139648438,\n",
       "  2.411977767944336,\n",
       "  2.3958654403686523,\n",
       "  2.451101779937744,\n",
       "  2.3821067810058594,\n",
       "  2.428974151611328,\n",
       "  2.380136251449585,\n",
       "  2.415635108947754,\n",
       "  2.4728591442108154,\n",
       "  2.3738927841186523,\n",
       "  2.4109716415405273,\n",
       "  2.445601224899292,\n",
       "  2.3751959800720215,\n",
       "  2.4216432571411133,\n",
       "  2.5268425941467285,\n",
       "  2.39912486076355,\n",
       "  2.407675266265869,\n",
       "  2.4128808975219727,\n",
       "  2.403636932373047,\n",
       "  2.386660099029541,\n",
       "  2.375674247741699,\n",
       "  2.4288485050201416,\n",
       "  2.4447245597839355,\n",
       "  2.4313933849334717,\n",
       "  2.3952136039733887,\n",
       "  2.3873865604400635,\n",
       "  2.375685453414917,\n",
       "  2.3886284828186035,\n",
       "  2.4375498294830322,\n",
       "  2.3862059116363525,\n",
       "  2.3775744438171387,\n",
       "  2.3852174282073975,\n",
       "  2.401655673980713,\n",
       "  2.3989510536193848,\n",
       "  2.4452836513519287,\n",
       "  2.447842836380005,\n",
       "  2.391528844833374,\n",
       "  2.3710358142852783,\n",
       "  2.381739377975464,\n",
       "  2.4027602672576904,\n",
       "  2.3930864334106445,\n",
       "  2.4962868690490723,\n",
       "  2.3608474731445312,\n",
       "  2.37760066986084,\n",
       "  2.4153809547424316,\n",
       "  2.382272481918335,\n",
       "  2.3879776000976562,\n",
       "  2.4155914783477783,\n",
       "  2.355572462081909]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal = []\n",
    "historyTr = []\n",
    "\n",
    "mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_modelLC()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal.append(history['val_loss'])\n",
    "    historyTr.append(history['loss'])\n",
    "model=create_modelLC()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036, callbacks=[mc]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_mean=np.mean(historyVal, axis=0)\n",
    "historyTr_mean=np.mean(historyTr, axis=0)\n",
    "\n",
    "historyVal_sd=np.std(historyVal, axis=0)\n",
    "historyTr_sd=np.std(historyTr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_mean)),historyTr_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_mean)), historyTr_mean - historyTr_sd, historyTr_mean + historyTr_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_mean)), historyVal_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_mean)), historyVal_mean - historyVal_sd, \n",
    "                 historyVal_mean + historyVal_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model result:\n",
      "MEE on the validation 2.8200313091278075 with standard deviation 0.1331580348681592\n",
      "MEE on the training 2.520401620864868 with standard deviation 0.023011841736000863\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model result:\")\n",
    "print(\"MEE on the validation\",historyVal_mean[-1],\"with standard deviation\",historyVal_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_mean[-1],\"with standard deviation\",historyTr_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_NoReg():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    ))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    ))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 57.1453 - val_loss: 55.7275\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 55.4388 - val_loss: 53.2448\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 52.9588 - val_loss: 50.2234\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 49.9416 - val_loss: 46.5826\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 46.3073 - val_loss: 42.1462\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 41.8832 - val_loss: 36.8767\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 36.6409 - val_loss: 31.0294\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 30.8615 - val_loss: 25.3492\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 25.2869 - val_loss: 21.2234\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 21.1953 - val_loss: 18.9719\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 19.0143 - val_loss: 17.6863\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.7592 - val_loss: 16.9829\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.0660 - val_loss: 16.6424\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 16.7153 - val_loss: 16.4907\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.5473 - val_loss: 16.4056\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 16.4489 - val_loss: 16.3170\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 16.3526 - val_loss: 16.1918\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 16.2250 - val_loss: 16.0144\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 16.0489 - val_loss: 15.7744\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 15.8123 - val_loss: 15.4643\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 15.5060 - val_loss: 15.0693\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 15.1137 - val_loss: 14.5496\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 14.5944 - val_loss: 13.8566\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13.8978 - val_loss: 12.9675\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 12.9974 - val_loss: 11.9410\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 11.9473 - val_loss: 10.9029\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 10.8846 - val_loss: 9.9452\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.9083 - val_loss: 9.2175\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.1840 - val_loss: 8.8367\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8128 - val_loss: 8.6941\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.6895 - val_loss: 8.6030\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.6147 - val_loss: 8.5323\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.5545 - val_loss: 8.4855\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.5111 - val_loss: 8.4511\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.4758 - val_loss: 8.4124\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.4378 - val_loss: 8.3686\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3955 - val_loss: 8.3232\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.3513 - val_loss: 8.2769\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3052 - val_loss: 8.2293\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.2567 - val_loss: 8.1804\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.2059 - val_loss: 8.1291\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1524 - val_loss: 8.0746\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.0955 - val_loss: 8.0167\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.0348 - val_loss: 7.9553\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.9703 - val_loss: 7.8903\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.9020 - val_loss: 7.8219\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.8300 - val_loss: 7.7504\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.7547 - val_loss: 7.6759\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.6766 - val_loss: 7.5990\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.5960 - val_loss: 7.5200\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5137 - val_loss: 7.4393\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.4300 - val_loss: 7.3574\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.3456 - val_loss: 7.2746\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.2609 - val_loss: 7.1914\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.1761 - val_loss: 7.1079\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.0916 - val_loss: 7.0243\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.0074 - val_loss: 6.9408\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.9238 - val_loss: 6.8576\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.8406 - val_loss: 6.7747\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6.7579 - val_loss: 6.6923\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.6758 - val_loss: 6.6102\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.5942 - val_loss: 6.5284\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.5129 - val_loss: 6.4467\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 6.4319 - val_loss: 6.3653\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.3511 - val_loss: 6.2840\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.2705 - val_loss: 6.2027\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.1900 - val_loss: 6.1214\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.1096 - val_loss: 6.0401\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.0293 - val_loss: 5.9588\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 5.9489 - val_loss: 5.8775\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 5.8686 - val_loss: 5.7961\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.7883 - val_loss: 5.7146\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.7079 - val_loss: 5.6330\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.6274 - val_loss: 5.5512\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.5467 - val_loss: 5.4692\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 5.4657 - val_loss: 5.3870\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.3844 - val_loss: 5.3042\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.3025 - val_loss: 5.2209\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 5.2198 - val_loss: 5.1369\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.1363 - val_loss: 5.0522\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 5.0520 - val_loss: 4.9668\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.9671 - val_loss: 4.8811\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.8820 - val_loss: 4.7956\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.7974 - val_loss: 4.7112\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.7139 - val_loss: 4.6278\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.6319 - val_loss: 4.5454\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.5516 - val_loss: 4.4610\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.4740 - val_loss: 4.3825\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.3989 - val_loss: 4.3080\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.3269 - val_loss: 4.2355\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.2589 - val_loss: 4.1692\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.1944 - val_loss: 4.1057\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.1340 - val_loss: 4.0511\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.0776 - val_loss: 4.0003\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.0252 - val_loss: 3.9521\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.9760 - val_loss: 3.9074\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.9299 - val_loss: 3.8661\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.8866 - val_loss: 3.8282\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.8463 - val_loss: 3.7916\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.8087 - val_loss: 3.7593\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.7741 - val_loss: 3.7291\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.7420 - val_loss: 3.7006\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.7121 - val_loss: 3.6749\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6843 - val_loss: 3.6486\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.6586 - val_loss: 3.6308\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.6348 - val_loss: 3.6096\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6124 - val_loss: 3.5940\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.5913 - val_loss: 3.5739\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.5715 - val_loss: 3.5638\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.5527 - val_loss: 3.5424\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.5352 - val_loss: 3.5425\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.5191 - val_loss: 3.5111\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.5045 - val_loss: 3.5295\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.4923 - val_loss: 3.4880\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.4839 - val_loss: 3.5392\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.4824 - val_loss: 3.4919\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.4933 - val_loss: 3.5851\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.5056 - val_loss: 3.5075\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.5195 - val_loss: 3.6106\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.5156 - val_loss: 3.5015\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5119 - val_loss: 3.5821\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4843 - val_loss: 3.4550\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4618 - val_loss: 3.5264\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.4325 - val_loss: 3.4162\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.4153 - val_loss: 3.4861\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.3948 - val_loss: 3.3893\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3856 - val_loss: 3.4640\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.3698 - val_loss: 3.3708\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3634 - val_loss: 3.4499\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3520 - val_loss: 3.3524\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3510 - val_loss: 3.4405\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3397 - val_loss: 3.3392\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.3386 - val_loss: 3.4264\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3241 - val_loss: 3.3217\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3189 - val_loss: 3.4048\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3036 - val_loss: 3.3029\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.2964 - val_loss: 3.3807\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2828 - val_loss: 3.2856\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.2734 - val_loss: 3.3544\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.2613 - val_loss: 3.2707\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2508 - val_loss: 3.3301\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2409 - val_loss: 3.2552\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2313 - val_loss: 3.3042\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.2216 - val_loss: 3.2398\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2124 - val_loss: 3.2784\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2036 - val_loss: 3.2258\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.1967 - val_loss: 3.2655\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.1911 - val_loss: 3.2155\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1872 - val_loss: 3.2583\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.1832 - val_loss: 3.2077\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1813 - val_loss: 3.2544\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1776 - val_loss: 3.2013\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1777 - val_loss: 3.2575\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1756 - val_loss: 3.1975\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1780 - val_loss: 3.2583\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1736 - val_loss: 3.1922\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1743 - val_loss: 3.2504\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1661 - val_loss: 3.1842\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1639 - val_loss: 3.2335\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1530 - val_loss: 3.1733\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1502 - val_loss: 3.2152\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.1388 - val_loss: 3.1631\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1355 - val_loss: 3.1972\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1246 - val_loss: 3.1538\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1231 - val_loss: 3.1871\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1151 - val_loss: 3.1474\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1154 - val_loss: 3.1806\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1087 - val_loss: 3.1419\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1092 - val_loss: 3.1750\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.1027 - val_loss: 3.1362\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1029 - val_loss: 3.1685\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0963 - val_loss: 3.1304\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0961 - val_loss: 3.1613\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0895 - val_loss: 3.1242\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.0890 - val_loss: 3.1536\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0825 - val_loss: 3.1180\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0818 - val_loss: 3.1436\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0744 - val_loss: 3.1105\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0734 - val_loss: 3.1317\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0657 - val_loss: 3.1028\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0648 - val_loss: 3.1238\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0581 - val_loss: 3.0969\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0574 - val_loss: 3.1170\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0515 - val_loss: 3.0916\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.0510 - val_loss: 3.1125\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0455 - val_loss: 3.0866\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0451 - val_loss: 3.1078\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0402 - val_loss: 3.0823\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0402 - val_loss: 3.1032\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0352 - val_loss: 3.0778\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0353 - val_loss: 3.0983\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0302 - val_loss: 3.0733\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0303 - val_loss: 3.0933\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0251 - val_loss: 3.0687\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0251 - val_loss: 3.0877\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0197 - val_loss: 3.0643\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0195 - val_loss: 3.0812\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0138 - val_loss: 3.0598\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0134 - val_loss: 3.0744\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0078 - val_loss: 3.0551\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0075 - val_loss: 3.0687\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0025 - val_loss: 3.0507\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0026 - val_loss: 3.0652\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9985 - val_loss: 3.0472\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9992 - val_loss: 3.0639\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9959 - val_loss: 3.0444\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9973 - val_loss: 3.0640\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9946 - val_loss: 3.0436\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9975 - val_loss: 3.0675\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9962 - val_loss: 3.0434\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0002 - val_loss: 3.0726\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9997 - val_loss: 3.0425\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0036 - val_loss: 3.0819\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0059 - val_loss: 3.0453\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0111 - val_loss: 3.0888\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0113 - val_loss: 3.0436\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0127 - val_loss: 3.0834\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0070 - val_loss: 3.0349\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0024 - val_loss: 3.0679\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9927 - val_loss: 3.0239\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9874 - val_loss: 3.0483\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9771 - val_loss: 3.0151\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9728 - val_loss: 3.0350\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9664 - val_loss: 3.0083\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9643 - val_loss: 3.0296\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9611 - val_loss: 3.0034\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9611 - val_loss: 3.0281\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9593 - val_loss: 3.0011\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9601 - val_loss: 3.0274\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9586 - val_loss: 2.9991\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9596 - val_loss: 3.0264\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9575 - val_loss: 2.9967\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.9582 - val_loss: 3.0236\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9553 - val_loss: 2.9936\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9573 - val_loss: 3.0237\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9543 - val_loss: 2.9905\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9555 - val_loss: 3.0196\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9512 - val_loss: 2.9866\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9514 - val_loss: 3.0141\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9466 - val_loss: 2.9817\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9465 - val_loss: 3.0089\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9419 - val_loss: 2.9773\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9420 - val_loss: 3.0042\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9379 - val_loss: 2.9734\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9385 - val_loss: 3.0007\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9347 - val_loss: 2.9701\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9359 - val_loss: 2.9980\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9321 - val_loss: 2.9671\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9336 - val_loss: 2.9953\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9296 - val_loss: 2.9642\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9312 - val_loss: 2.9923\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9270 - val_loss: 2.9612\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9277 - val_loss: 2.9874\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9234 - val_loss: 2.9573\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9240 - val_loss: 2.9836\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9199 - val_loss: 2.9534\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9208 - val_loss: 2.9804\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9169 - val_loss: 2.9502\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9183 - val_loss: 2.9776\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9144 - val_loss: 2.9471\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9160 - val_loss: 2.9747\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9118 - val_loss: 2.9439\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9133 - val_loss: 2.9713\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9086 - val_loss: 2.9403\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9102 - val_loss: 2.9679\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9053 - val_loss: 2.9367\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9072 - val_loss: 2.9650\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9026 - val_loss: 2.9338\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9047 - val_loss: 2.9627\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9005 - val_loss: 2.9312\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9024 - val_loss: 2.9603\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8983 - val_loss: 2.9284\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8999 - val_loss: 2.9574\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8957 - val_loss: 2.9254\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8971 - val_loss: 2.9541\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8927 - val_loss: 2.9224\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8940 - val_loss: 2.9508\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8896 - val_loss: 2.9194\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8908 - val_loss: 2.9473\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8865 - val_loss: 2.9165\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8875 - val_loss: 2.9437\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8833 - val_loss: 2.9135\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8841 - val_loss: 2.9396\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8798 - val_loss: 2.9106\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8804 - val_loss: 2.9352\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8761 - val_loss: 2.9077\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8765 - val_loss: 2.9305\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8723 - val_loss: 2.9049\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8726 - val_loss: 2.9259\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8686 - val_loss: 2.9025\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8690 - val_loss: 2.9222\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8656 - val_loss: 2.9005\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8663 - val_loss: 2.9199\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8634 - val_loss: 2.8985\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8642 - val_loss: 2.9178\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8614 - val_loss: 2.8966\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8620 - val_loss: 2.9153\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8591 - val_loss: 2.8945\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8596 - val_loss: 2.9129\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8568 - val_loss: 2.8923\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8571 - val_loss: 2.9105\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8545 - val_loss: 2.8901\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8546 - val_loss: 2.9081\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8522 - val_loss: 2.8881\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8521 - val_loss: 2.9058\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8500 - val_loss: 2.8861\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8497 - val_loss: 2.9035\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8478 - val_loss: 2.8842\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8473 - val_loss: 2.9013\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8456 - val_loss: 2.8824\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8450 - val_loss: 2.8993\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8435 - val_loss: 2.8806\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.8429 - val_loss: 2.8973\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8415 - val_loss: 2.8788\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8407 - val_loss: 2.8952\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8394 - val_loss: 2.8769\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8384 - val_loss: 2.8922\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8367 - val_loss: 2.8747\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8355 - val_loss: 2.8866\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8332 - val_loss: 2.8727\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8318 - val_loss: 2.8811\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8291 - val_loss: 2.8689\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8258 - val_loss: 2.8760\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8245 - val_loss: 2.8655\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8200 - val_loss: 2.8675\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8192 - val_loss: 2.8663\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8161 - val_loss: 2.8617\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8160 - val_loss: 2.8664\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8133 - val_loss: 2.8604\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8135 - val_loss: 2.8633\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8110 - val_loss: 2.8604\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8115 - val_loss: 2.8609\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8091 - val_loss: 2.8597\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8100 - val_loss: 2.8601\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8079 - val_loss: 2.8587\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8089 - val_loss: 2.8594\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8068 - val_loss: 2.8576\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8074 - val_loss: 2.8578\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8052 - val_loss: 2.8564\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8055 - val_loss: 2.8558\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8032 - val_loss: 2.8548\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8034 - val_loss: 2.8537\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8011 - val_loss: 2.8532\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8013 - val_loss: 2.8517\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7989 - val_loss: 2.8514\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7991 - val_loss: 2.8498\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7967 - val_loss: 2.8497\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7970 - val_loss: 2.8479\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7946 - val_loss: 2.8480\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7949 - val_loss: 2.8461\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7925 - val_loss: 2.8463\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7927 - val_loss: 2.8442\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7903 - val_loss: 2.8446\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7906 - val_loss: 2.8424\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7882 - val_loss: 2.8429\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7885 - val_loss: 2.8406\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7861 - val_loss: 2.8412\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7864 - val_loss: 2.8390\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7840 - val_loss: 2.8394\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7843 - val_loss: 2.8375\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7819 - val_loss: 2.8373\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7821 - val_loss: 2.8361\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7795 - val_loss: 2.8348\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7796 - val_loss: 2.8347\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7768 - val_loss: 2.8318\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7768 - val_loss: 2.8329\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7737 - val_loss: 2.8281\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7733 - val_loss: 2.8306\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7700 - val_loss: 2.8230\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7688 - val_loss: 2.8307\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7651 - val_loss: 2.8146\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7637 - val_loss: 2.8307\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7595 - val_loss: 2.8109\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7575 - val_loss: 2.8235\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7550 - val_loss: 2.8114\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7535 - val_loss: 2.8188\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7518 - val_loss: 2.8110\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7508 - val_loss: 2.8164\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7496 - val_loss: 2.8108\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7488 - val_loss: 2.8150\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7478 - val_loss: 2.8103\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7472 - val_loss: 2.8141\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7463 - val_loss: 2.8095\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7458 - val_loss: 2.8139\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7449 - val_loss: 2.8086\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7447 - val_loss: 2.8138\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7440 - val_loss: 2.8084\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7440 - val_loss: 2.8118\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7437 - val_loss: 2.8094\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7436 - val_loss: 2.8101\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7436 - val_loss: 2.8094\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7429 - val_loss: 2.8099\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7425 - val_loss: 2.8073\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7415 - val_loss: 2.8108\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7411 - val_loss: 2.8042\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7406 - val_loss: 2.8160\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7402 - val_loss: 2.8006\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7416 - val_loss: 2.8188\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7409 - val_loss: 2.8016\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7429 - val_loss: 2.8180\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7427 - val_loss: 2.8040\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7441 - val_loss: 2.8174\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7442 - val_loss: 2.8061\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7450 - val_loss: 2.8179\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7449 - val_loss: 2.8077\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7456 - val_loss: 2.8175\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7454 - val_loss: 2.8101\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7461 - val_loss: 2.8167\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7476 - val_loss: 2.8181\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7502 - val_loss: 2.8196\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7575 - val_loss: 2.8381\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7657 - val_loss: 2.8369\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7875 - val_loss: 2.8743\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8073 - val_loss: 2.8653\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8246 - val_loss: 2.8751\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8064 - val_loss: 2.8434\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7992 - val_loss: 2.8572\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7869 - val_loss: 2.8405\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7877 - val_loss: 2.8503\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7817 - val_loss: 2.8385\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7815 - val_loss: 2.8445\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7744 - val_loss: 2.8290\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7716 - val_loss: 2.8330\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7596 - val_loss: 2.8103\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7512 - val_loss: 2.8184\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7418 - val_loss: 2.7936\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7327 - val_loss: 2.8053\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7250 - val_loss: 2.7835\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7204 - val_loss: 2.7975\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7144 - val_loss: 2.7824\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7108 - val_loss: 2.7845\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7074 - val_loss: 2.7906\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7060 - val_loss: 2.7814\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7051 - val_loss: 2.7881\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7041 - val_loss: 2.7831\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7032 - val_loss: 2.7843\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7024 - val_loss: 2.7839\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7016 - val_loss: 2.7827\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7009 - val_loss: 2.7830\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7001 - val_loss: 2.7827\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6994 - val_loss: 2.7810\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6987 - val_loss: 2.7826\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6979 - val_loss: 2.7790\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6973 - val_loss: 2.7822\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6966 - val_loss: 2.7781\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6960 - val_loss: 2.7807\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6953 - val_loss: 2.7778\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6948 - val_loss: 2.7793\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6941 - val_loss: 2.7774\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6937 - val_loss: 2.7781\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6931 - val_loss: 2.7772\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6927 - val_loss: 2.7769\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6921 - val_loss: 2.7771\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6918 - val_loss: 2.7757\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6911 - val_loss: 2.7769\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6908 - val_loss: 2.7745\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6904 - val_loss: 2.7769\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6902 - val_loss: 2.7732\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6905 - val_loss: 2.7778\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6908 - val_loss: 2.7723\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6911 - val_loss: 2.7796\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6916 - val_loss: 2.7716\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6919 - val_loss: 2.7818\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6926 - val_loss: 2.7723\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6934 - val_loss: 2.7828\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6946 - val_loss: 2.7748\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6956 - val_loss: 2.7841\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6976 - val_loss: 2.7779\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6986 - val_loss: 2.7875\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7021 - val_loss: 2.7850\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7056 - val_loss: 2.7942\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7138 - val_loss: 2.8100\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7264 - val_loss: 2.8074\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7420 - val_loss: 2.8306\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7470 - val_loss: 2.8199\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7618 - val_loss: 2.8352\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7537 - val_loss: 2.8202\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7553 - val_loss: 2.8248\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7423 - val_loss: 2.8113\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7400 - val_loss: 2.8128\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7268 - val_loss: 2.7946\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7196 - val_loss: 2.8024\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7127 - val_loss: 2.7767\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7049 - val_loss: 2.7939\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6984 - val_loss: 2.7663\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6950 - val_loss: 2.7907\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6926 - val_loss: 2.7640\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6923 - val_loss: 2.7900\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6921 - val_loss: 2.7657\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6924 - val_loss: 2.7886\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6925 - val_loss: 2.7689\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6922 - val_loss: 2.7858\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6921 - val_loss: 2.7709\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6908 - val_loss: 2.7830\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6903 - val_loss: 2.7703\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6886 - val_loss: 2.7811\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6882 - val_loss: 2.7684\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6863 - val_loss: 2.7796\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6860 - val_loss: 2.7671\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6845 - val_loss: 2.7785\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6845 - val_loss: 2.7666\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6835 - val_loss: 2.7780\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6838 - val_loss: 2.7665\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6831 - val_loss: 2.7781\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6836 - val_loss: 2.7669\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6835 - val_loss: 2.7789\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6842 - val_loss: 2.7683\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6846 - val_loss: 2.7798\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6853 - val_loss: 2.7691\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6851 - val_loss: 2.7796\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6849 - val_loss: 2.7685\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6837 - val_loss: 2.7779\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6827 - val_loss: 2.7668\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6813 - val_loss: 2.7761\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6802 - val_loss: 2.7647\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6786 - val_loss: 2.7745\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6778 - val_loss: 2.7628\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6764 - val_loss: 2.7737\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6763 - val_loss: 2.7621\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6754 - val_loss: 2.7739\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6759 - val_loss: 2.7624\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6757 - val_loss: 2.7740\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6759 - val_loss: 2.7636\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6760 - val_loss: 2.7734\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6758 - val_loss: 2.7645\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6756 - val_loss: 2.7725\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6751 - val_loss: 2.7644\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6746 - val_loss: 2.7718\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6738 - val_loss: 2.7630\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6731 - val_loss: 2.7715\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6724 - val_loss: 2.7609\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6715 - val_loss: 2.7710\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6707 - val_loss: 2.7591\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6699 - val_loss: 2.7700\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6690 - val_loss: 2.7584\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6685 - val_loss: 2.7687\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6676 - val_loss: 2.7585\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6674 - val_loss: 2.7676\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6667 - val_loss: 2.7594\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6670 - val_loss: 2.7673\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6676 - val_loss: 2.7629\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6686 - val_loss: 2.7683\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6700 - val_loss: 2.7651\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6700 - val_loss: 2.7684\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6698 - val_loss: 2.7637\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6684 - val_loss: 2.7674\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6674 - val_loss: 2.7610\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6660 - val_loss: 2.7665\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6646 - val_loss: 2.7572\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6631 - val_loss: 2.7650\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6607 - val_loss: 2.7529\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.6592 - val_loss: 2.7627\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6571 - val_loss: 2.7515\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6561 - val_loss: 2.7608\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6552 - val_loss: 2.7520\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6549 - val_loss: 2.7597\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6547 - val_loss: 2.7533\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6549 - val_loss: 2.7600\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6553 - val_loss: 2.7555\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6570 - val_loss: 2.7624\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6597 - val_loss: 2.7613\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6627 - val_loss: 2.7642\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6629 - val_loss: 2.7628\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6632 - val_loss: 2.7626\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6609 - val_loss: 2.7601\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6591 - val_loss: 2.7609\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6573 - val_loss: 2.7565\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6555 - val_loss: 2.7603\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6544 - val_loss: 2.7536\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6537 - val_loss: 2.7603\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6527 - val_loss: 2.7514\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6524 - val_loss: 2.7595\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6511 - val_loss: 2.7507\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6510 - val_loss: 2.7587\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6503 - val_loss: 2.7520\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6509 - val_loss: 2.7586\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6512 - val_loss: 2.7546\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6519 - val_loss: 2.7584\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6519 - val_loss: 2.7562\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6520 - val_loss: 2.7578\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6515 - val_loss: 2.7558\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6509 - val_loss: 2.7572\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6500 - val_loss: 2.7539\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6490 - val_loss: 2.7567\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6481 - val_loss: 2.7514\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6471 - val_loss: 2.7564\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6460 - val_loss: 2.7488\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6449 - val_loss: 2.7557\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6437 - val_loss: 2.7464\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6425 - val_loss: 2.7547\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6415 - val_loss: 2.7448\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6404 - val_loss: 2.7539\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6401 - val_loss: 2.7446\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6396 - val_loss: 2.7538\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6402 - val_loss: 2.7473\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6412 - val_loss: 2.7543\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6425 - val_loss: 2.7526\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6443 - val_loss: 2.7543\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6448 - val_loss: 2.7553\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6442 - val_loss: 2.7529\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 56.4193 - val_loss: 55.2372\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 54.4026 - val_loss: 52.1052\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 51.2717 - val_loss: 48.0485\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 47.2156 - val_loss: 43.0888\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 42.2560 - val_loss: 37.3998\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 36.5669 - val_loss: 31.3896\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 30.5563 - val_loss: 25.7312\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 24.9138 - val_loss: 21.5073\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 20.9336 - val_loss: 19.2160\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18.7985 - val_loss: 17.9123\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 17.5837 - val_loss: 17.2097\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 16.9319 - val_loss: 16.8659\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 16.6173 - val_loss: 16.7055\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 16.4739 - val_loss: 16.6107\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 16.3883 - val_loss: 16.5166\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.2986 - val_loss: 16.3951\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 16.1779 - val_loss: 16.2330\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 16.0140 - val_loss: 16.0187\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.7955 - val_loss: 15.7396\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 15.5107 - val_loss: 15.3876\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 15.1529 - val_loss: 14.9478\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 14.7095 - val_loss: 14.3726\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 14.1346 - val_loss: 13.5959\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.3626 - val_loss: 12.5576\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 12.3295 - val_loss: 11.3330\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 11.1057 - val_loss: 10.2465\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.0674 - val_loss: 9.4040\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.2328 - val_loss: 8.9219\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.7320 - val_loss: 8.7713\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.5907 - val_loss: 8.7103\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.5261 - val_loss: 8.6669\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.4779 - val_loss: 8.6341\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.4465 - val_loss: 8.6219\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.4299 - val_loss: 8.6087\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.4144 - val_loss: 8.5880\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3948 - val_loss: 8.5679\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.3760 - val_loss: 8.5499\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.3575 - val_loss: 8.5313\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.3386 - val_loss: 8.5115\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.3192 - val_loss: 8.4917\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.2992 - val_loss: 8.4702\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2780 - val_loss: 8.4465\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2552 - val_loss: 8.4209\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.2306 - val_loss: 8.3932\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.2039 - val_loss: 8.3631\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.1751 - val_loss: 8.3306\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.1441 - val_loss: 8.2956\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1109 - val_loss: 8.2582\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0753 - val_loss: 8.2183\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0373 - val_loss: 8.1759\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.9969 - val_loss: 8.1311\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9541 - val_loss: 8.0836\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.9086 - val_loss: 8.0334\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.8604 - val_loss: 7.9803\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.8094 - val_loss: 7.9241\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.7553 - val_loss: 7.8646\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.6981 - val_loss: 7.8018\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.6376 - val_loss: 7.7360\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.5737 - val_loss: 7.6659\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.5064 - val_loss: 7.5927\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.4354 - val_loss: 7.5159\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.3607 - val_loss: 7.4355\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.2825 - val_loss: 7.3523\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.2010 - val_loss: 7.2663\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.1167 - val_loss: 7.1785\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.0301 - val_loss: 7.0896\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 6.9417 - val_loss: 6.9997\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.8519 - val_loss: 6.9094\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 6.7608 - val_loss: 6.8178\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.6685 - val_loss: 6.7256\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6.5747 - val_loss: 6.6317\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.4796 - val_loss: 6.5371\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.3829 - val_loss: 6.4403\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6.2845 - val_loss: 6.3427\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.1842 - val_loss: 6.2428\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.0820 - val_loss: 6.1418\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.9776 - val_loss: 6.0374\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.8714 - val_loss: 5.9349\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.7632 - val_loss: 5.8273\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.6533 - val_loss: 5.7231\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.5422 - val_loss: 5.6136\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.4306 - val_loss: 5.5124\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.3195 - val_loss: 5.4033\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.2100 - val_loss: 5.3084\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.1036 - val_loss: 5.1996\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.0027 - val_loss: 5.1205\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.9089 - val_loss: 5.0139\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.8303 - val_loss: 4.9895\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.7757 - val_loss: 4.9054\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.7647 - val_loss: 5.0198\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.8101 - val_loss: 4.9502\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.8565 - val_loss: 5.1007\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.9089 - val_loss: 4.8049\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.7213 - val_loss: 4.7243\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.5582 - val_loss: 4.5021\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.3989 - val_loss: 4.4314\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 4.2928 - val_loss: 4.3225\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.2139 - val_loss: 4.2769\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.1532 - val_loss: 4.2180\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.1028 - val_loss: 4.1730\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.0571 - val_loss: 4.1258\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.0140 - val_loss: 4.0822\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.9726 - val_loss: 4.0415\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.9327 - val_loss: 3.9987\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.8946 - val_loss: 3.9621\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.8586 - val_loss: 3.9203\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.8250 - val_loss: 3.8986\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7957 - val_loss: 3.8551\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7752 - val_loss: 3.8728\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.7731 - val_loss: 3.8623\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.8087 - val_loss: 3.9808\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.8836 - val_loss: 3.9846\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.9564 - val_loss: 4.0731\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.9789 - val_loss: 3.9887\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.9619 - val_loss: 3.9144\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.8349 - val_loss: 3.7650\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.7166 - val_loss: 3.7017\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6271 - val_loss: 3.6360\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.5665 - val_loss: 3.6236\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.5398 - val_loss: 3.5913\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.5190 - val_loss: 3.5970\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5097 - val_loss: 3.5714\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5002 - val_loss: 3.5860\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.4990 - val_loss: 3.5659\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4964 - val_loss: 3.5831\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4904 - val_loss: 3.5562\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4855 - val_loss: 3.5636\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4670 - val_loss: 3.5327\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.4565 - val_loss: 3.5356\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.4324 - val_loss: 3.5040\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4201 - val_loss: 3.5087\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.3988 - val_loss: 3.4808\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3884 - val_loss: 3.4852\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.3673 - val_loss: 3.4581\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.3574 - val_loss: 3.4635\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.3381 - val_loss: 3.4388\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.3295 - val_loss: 3.4477\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.3147 - val_loss: 3.4244\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3083 - val_loss: 3.4373\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.2961 - val_loss: 3.4127\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.2904 - val_loss: 3.4288\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2794 - val_loss: 3.4039\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.2756 - val_loss: 3.4236\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.2658 - val_loss: 3.4016\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.2677 - val_loss: 3.4250\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.259 - 0s 77ms/step - loss: 3.2591 - val_loss: 3.3988\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2611 - val_loss: 3.4257\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2514 - val_loss: 3.3932\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.2502 - val_loss: 3.4186\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.2365 - val_loss: 3.3823\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2333 - val_loss: 3.4068\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.2170 - val_loss: 3.3699\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2146 - val_loss: 3.3950\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1982 - val_loss: 3.3551\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1940 - val_loss: 3.3795\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1762 - val_loss: 3.3360\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.1671 - val_loss: 3.3572\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.1501 - val_loss: 3.3199\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1378 - val_loss: 3.3322\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.1257 - val_loss: 3.3126\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1177 - val_loss: 3.3168\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1110 - val_loss: 3.3104\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.1063 - val_loss: 3.3097\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1022 - val_loss: 3.3106\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.0984 - val_loss: 3.3065\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0945 - val_loss: 3.3073\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0908 - val_loss: 3.3042\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0871 - val_loss: 3.3044\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0835 - val_loss: 3.3017\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0799 - val_loss: 3.3017\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0765 - val_loss: 3.2989\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0731 - val_loss: 3.2996\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0699 - val_loss: 3.2950\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0669 - val_loss: 3.2987\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.0639 - val_loss: 3.2920\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0611 - val_loss: 3.2979\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0583 - val_loss: 3.2892\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0554 - val_loss: 3.2964\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0525 - val_loss: 3.2867\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0494 - val_loss: 3.2938\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0464 - val_loss: 3.2842\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0433 - val_loss: 3.2906\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0403 - val_loss: 3.2818\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0372 - val_loss: 3.2870\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0342 - val_loss: 3.2795\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0311 - val_loss: 3.2833\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0282 - val_loss: 3.2773\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0252 - val_loss: 3.2796\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.0224 - val_loss: 3.2750\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0196 - val_loss: 3.2761\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0168 - val_loss: 3.2727\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0140 - val_loss: 3.2728\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0113 - val_loss: 3.2702\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0087 - val_loss: 3.2696\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0060 - val_loss: 3.2676\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0034 - val_loss: 3.2666\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0008 - val_loss: 3.2648\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9982 - val_loss: 3.2636\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9956 - val_loss: 3.2619\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9930 - val_loss: 3.2606\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9905 - val_loss: 3.2590\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9880 - val_loss: 3.2575\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9855 - val_loss: 3.2559\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9830 - val_loss: 3.2545\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9805 - val_loss: 3.2529\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9780 - val_loss: 3.2514\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9756 - val_loss: 3.2499\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9731 - val_loss: 3.2484\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9707 - val_loss: 3.2468\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9683 - val_loss: 3.2454\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9659 - val_loss: 3.2438\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9635 - val_loss: 3.2426\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9611 - val_loss: 3.2406\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9588 - val_loss: 3.2407\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9565 - val_loss: 3.2361\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9543 - val_loss: 3.2393\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9525 - val_loss: 3.2320\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9515 - val_loss: 3.2446\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9527 - val_loss: 3.2293\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9583 - val_loss: 3.2719\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9741 - val_loss: 3.2600\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0197 - val_loss: 3.3882\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1040 - val_loss: 3.4381\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2470 - val_loss: 3.6207\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3647 - val_loss: 3.4366\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.2629 - val_loss: 3.4075\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1343 - val_loss: 3.2502\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0387 - val_loss: 3.2654\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9799 - val_loss: 3.2031\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9554 - val_loss: 3.2274\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9367 - val_loss: 3.2010\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9270 - val_loss: 3.2077\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9226 - val_loss: 3.2031\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9195 - val_loss: 3.2056\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9167 - val_loss: 3.2051\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9142 - val_loss: 3.2039\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9118 - val_loss: 3.2044\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9095 - val_loss: 3.2016\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9073 - val_loss: 3.2031\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9051 - val_loss: 3.1971\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9032 - val_loss: 3.2040\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9015 - val_loss: 3.1929\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9001 - val_loss: 3.2058\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8997 - val_loss: 3.1898\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8993 - val_loss: 3.2124\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9020 - val_loss: 3.1880\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9074 - val_loss: 3.2285\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9163 - val_loss: 3.1992\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9347 - val_loss: 3.2644\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9580 - val_loss: 3.2355\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0017 - val_loss: 3.3400\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0404 - val_loss: 3.2826\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0702 - val_loss: 3.3512\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0533 - val_loss: 3.2396\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0203 - val_loss: 3.2694\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9657 - val_loss: 3.1755\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9232 - val_loss: 3.2030\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8940 - val_loss: 3.1644\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8806 - val_loss: 3.1781\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8733 - val_loss: 3.1632\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8684 - val_loss: 3.1743\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8650 - val_loss: 3.1631\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8622 - val_loss: 3.1697\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8600 - val_loss: 3.1640\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8581 - val_loss: 3.1676\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8563 - val_loss: 3.1608\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8547 - val_loss: 3.1696\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8537 - val_loss: 3.1574\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8527 - val_loss: 3.1727\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8534 - val_loss: 3.1545\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8543 - val_loss: 3.1790\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8571 - val_loss: 3.1527\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8620 - val_loss: 3.1932\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8706 - val_loss: 3.1640\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8878 - val_loss: 3.2305\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9125 - val_loss: 3.1963\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9488 - val_loss: 3.2884\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9737 - val_loss: 3.2140\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9797 - val_loss: 3.2736\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9580 - val_loss: 3.1739\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9249 - val_loss: 3.2118\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8922 - val_loss: 3.1451\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8708 - val_loss: 3.1739\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8505 - val_loss: 3.1334\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8413 - val_loss: 3.1601\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8363 - val_loss: 3.1315\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8310 - val_loss: 3.1528\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8288 - val_loss: 3.1312\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8257 - val_loss: 3.1501\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8258 - val_loss: 3.1290\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8242 - val_loss: 3.1519\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8257 - val_loss: 3.1274\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8254 - val_loss: 3.1566\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8281 - val_loss: 3.1289\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8313 - val_loss: 3.1701\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8420 - val_loss: 3.1456\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8628 - val_loss: 3.2074\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8806 - val_loss: 3.1640\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8998 - val_loss: 3.2340\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9074 - val_loss: 3.1589\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8964 - val_loss: 3.2093\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8796 - val_loss: 3.1353\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8570 - val_loss: 3.1714\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8389 - val_loss: 3.1242\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8316 - val_loss: 3.1558\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8248 - val_loss: 3.1224\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8246 - val_loss: 3.1586\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8266 - val_loss: 3.1277\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8331 - val_loss: 3.1701\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8368 - val_loss: 3.1323\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8434 - val_loss: 3.1794\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8435 - val_loss: 3.1318\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8431 - val_loss: 3.1746\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8380 - val_loss: 3.1264\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8344 - val_loss: 3.1629\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8254 - val_loss: 3.1191\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8217 - val_loss: 3.1549\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8172 - val_loss: 3.1177\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8150 - val_loss: 3.1498\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8119 - val_loss: 3.1155\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8108 - val_loss: 3.1485\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8099 - val_loss: 3.1153\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8095 - val_loss: 3.1492\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8094 - val_loss: 3.1139\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8081 - val_loss: 3.1488\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8071 - val_loss: 3.1112\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8044 - val_loss: 3.1454\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8028 - val_loss: 3.1088\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7997 - val_loss: 3.1418\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7989 - val_loss: 3.1072\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7964 - val_loss: 3.1406\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7969 - val_loss: 3.1060\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7946 - val_loss: 3.1403\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7957 - val_loss: 3.1048\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7928 - val_loss: 3.1390\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7935 - val_loss: 3.1030\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7900 - val_loss: 3.1370\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7908 - val_loss: 3.1011\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7869 - val_loss: 3.1350\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7880 - val_loss: 3.0994\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7840 - val_loss: 3.1332\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7855 - val_loss: 3.0977\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7813 - val_loss: 3.1313\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7830 - val_loss: 3.0953\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7779 - val_loss: 3.1280\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7788 - val_loss: 3.0906\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7703 - val_loss: 3.1192\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7702 - val_loss: 3.0870\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7610 - val_loss: 3.1021\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7577 - val_loss: 3.0814\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7460 - val_loss: 3.0849\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7416 - val_loss: 3.0748\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7353 - val_loss: 3.0754\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7336 - val_loss: 3.0742\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7318 - val_loss: 3.0716\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7319 - val_loss: 3.0724\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7305 - val_loss: 3.0720\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7308 - val_loss: 3.0692\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7292 - val_loss: 3.0726\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7294 - val_loss: 3.0674\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7276 - val_loss: 3.0716\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7278 - val_loss: 3.0672\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7269 - val_loss: 3.0718\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7278 - val_loss: 3.0653\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7262 - val_loss: 3.0730\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7269 - val_loss: 3.0636\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7249 - val_loss: 3.0727\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7256 - val_loss: 3.0623\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7235 - val_loss: 3.0720\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7243 - val_loss: 3.0612\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7222 - val_loss: 3.0719\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7236 - val_loss: 3.0604\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7216 - val_loss: 3.0734\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7243 - val_loss: 3.0621\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7245 - val_loss: 3.0812\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7306 - val_loss: 3.0662\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7326 - val_loss: 3.0978\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7435 - val_loss: 3.0702\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7451 - val_loss: 3.1138\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7550 - val_loss: 3.0720\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7573 - val_loss: 3.1227\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7618 - val_loss: 3.0709\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7556 - val_loss: 3.1122\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7527 - val_loss: 3.0649\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7444 - val_loss: 3.1018\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7424 - val_loss: 3.0615\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7366 - val_loss: 3.0949\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7358 - val_loss: 3.0606\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7298 - val_loss: 3.0868\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7286 - val_loss: 3.0526\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7161 - val_loss: 3.0670\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7113 - val_loss: 3.0457\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7023 - val_loss: 3.0490\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6976 - val_loss: 3.0405\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6928 - val_loss: 3.0418\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.6902 - val_loss: 3.0378\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6889 - val_loss: 3.0377\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6878 - val_loss: 3.0373\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6868 - val_loss: 3.0356\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6859 - val_loss: 3.0346\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6849 - val_loss: 3.0340\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6842 - val_loss: 3.0327\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6836 - val_loss: 3.0358\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6841 - val_loss: 3.0343\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6857 - val_loss: 3.0342\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6851 - val_loss: 3.0410\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6863 - val_loss: 3.0319\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6857 - val_loss: 3.0448\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6877 - val_loss: 3.0336\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6876 - val_loss: 3.0521\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6929 - val_loss: 3.0376\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6938 - val_loss: 3.0615\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6998 - val_loss: 3.0388\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6991 - val_loss: 3.0700\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7054 - val_loss: 3.0417\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7097 - val_loss: 3.0861\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7191 - val_loss: 3.0497\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7246 - val_loss: 3.1014\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7310 - val_loss: 3.0504\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7268 - val_loss: 3.0957\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7245 - val_loss: 3.0441\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7166 - val_loss: 3.0834\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7128 - val_loss: 3.0398\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7074 - val_loss: 3.0740\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7043 - val_loss: 3.0342\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6964 - val_loss: 3.0584\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6905 - val_loss: 3.0225\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6780 - val_loss: 3.0341\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6703 - val_loss: 3.0202\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6648 - val_loss: 3.0248\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6626 - val_loss: 3.0186\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6609 - val_loss: 3.0231\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6598 - val_loss: 3.0170\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6586 - val_loss: 3.0207\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6576 - val_loss: 3.0172\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6567 - val_loss: 3.0172\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6558 - val_loss: 3.0170\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6550 - val_loss: 3.0159\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6542 - val_loss: 3.0158\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6534 - val_loss: 3.0152\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6526 - val_loss: 3.0148\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6519 - val_loss: 3.0147\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6512 - val_loss: 3.0141\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6508 - val_loss: 3.0144\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6507 - val_loss: 3.0163\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6514 - val_loss: 3.0145\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6536 - val_loss: 3.0297\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6592 - val_loss: 3.0162\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6627 - val_loss: 3.0474\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6708 - val_loss: 3.0200\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6769 - val_loss: 3.0661\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6869 - val_loss: 3.0308\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6957 - val_loss: 3.0920\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.7109 - val_loss: 3.0418\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7124 - val_loss: 3.0931\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7109 - val_loss: 3.0307\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6972 - val_loss: 3.0740\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6932 - val_loss: 3.0254\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6861 - val_loss: 3.0619\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6813 - val_loss: 3.0157\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6736 - val_loss: 3.0454\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6659 - val_loss: 3.0025\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6551 - val_loss: 3.0222\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6476 - val_loss: 3.0031\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6416 - val_loss: 3.0103\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6387 - val_loss: 3.0011\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6364 - val_loss: 3.0082\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6356 - val_loss: 3.0005\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6348 - val_loss: 3.0075\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6346 - val_loss: 2.9998\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6342 - val_loss: 3.0089\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6343 - val_loss: 2.9986\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6343 - val_loss: 3.0116\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6348 - val_loss: 2.9985\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6353 - val_loss: 3.0154\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6370 - val_loss: 2.9988\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6371 - val_loss: 3.0216\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6413 - val_loss: 3.0012\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6426 - val_loss: 3.0315\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6481 - val_loss: 3.0002\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 2.6490 - val_loss: 3.0411\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6534 - val_loss: 3.0003\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6540 - val_loss: 3.0484\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6585 - val_loss: 3.0025\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6578 - val_loss: 3.0523\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6612 - val_loss: 3.0040\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6593 - val_loss: 3.0526\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6612 - val_loss: 3.0031\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6576 - val_loss: 3.0486\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6575 - val_loss: 2.9976\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6497 - val_loss: 3.0366\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6471 - val_loss: 2.9929\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6391 - val_loss: 3.0202\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6351 - val_loss: 2.9910\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6284 - val_loss: 3.0047\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6235 - val_loss: 2.9876\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6199 - val_loss: 2.9976\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6171 - val_loss: 2.9854\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6154 - val_loss: 2.9928\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6143 - val_loss: 2.9874\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6136 - val_loss: 2.9917\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6130 - val_loss: 2.9863\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6126 - val_loss: 2.9927\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6122 - val_loss: 2.9842\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6121 - val_loss: 2.9948\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6121 - val_loss: 2.9825\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6124 - val_loss: 2.9983\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6128 - val_loss: 2.9819\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6134 - val_loss: 3.0024\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6143 - val_loss: 2.9829\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6163 - val_loss: 3.0076\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6184 - val_loss: 2.9828\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6199 - val_loss: 3.0189\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6280 - val_loss: 2.9891\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6347 - val_loss: 3.0460\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6497 - val_loss: 3.0036\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6610 - val_loss: 3.0723\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6724 - val_loss: 3.0073\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6636 - val_loss: 3.0590\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6608 - val_loss: 2.9992\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6499 - val_loss: 3.0410\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6443 - val_loss: 2.9846\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6337 - val_loss: 3.0298\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6313 - val_loss: 2.9791\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6226 - val_loss: 3.0095\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6159 - val_loss: 2.9750\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6089 - val_loss: 2.9932\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6033 - val_loss: 2.9727\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5996 - val_loss: 2.9813\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5976 - val_loss: 2.9747\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5966 - val_loss: 2.9794\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5959 - val_loss: 2.9736\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5953 - val_loss: 2.9805\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5950 - val_loss: 2.9721\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5947 - val_loss: 2.9829\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5947 - val_loss: 2.9707\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5947 - val_loss: 2.9871\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5953 - val_loss: 2.9703\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5965 - val_loss: 2.9928\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5983 - val_loss: 2.9701\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6022 - val_loss: 3.0046\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6061 - val_loss: 2.9714\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6106 - val_loss: 3.0186\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6180 - val_loss: 2.9776\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6227 - val_loss: 3.0442\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6393 - val_loss: 2.9930\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6460 - val_loss: 3.0572\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6526 - val_loss: 2.9919\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6458 - val_loss: 3.0465\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6418 - val_loss: 2.9835\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6325 - val_loss: 3.0330\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6281 - val_loss: 2.9724\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6157 - val_loss: 3.0163\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6131 - val_loss: 2.9660\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6031 - val_loss: 2.9949\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5959 - val_loss: 2.9619\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5903 - val_loss: 2.9823\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5867 - val_loss: 2.9612\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5842 - val_loss: 2.9741\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5828 - val_loss: 2.9633\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5818 - val_loss: 2.9719\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5811 - val_loss: 2.9619\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5807 - val_loss: 2.9742\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5807 - val_loss: 2.9604\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5808 - val_loss: 2.9781\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5815 - val_loss: 2.9584\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5829 - val_loss: 2.9873\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5854 - val_loss: 2.9587\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5875 - val_loss: 2.9940\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5906 - val_loss: 2.9610\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5948 - val_loss: 3.0088\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6029 - val_loss: 2.9655\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6063 - val_loss: 3.0291\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6190 - val_loss: 2.9711\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6231 - val_loss: 3.0438\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6330 - val_loss: 2.9800\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6326 - val_loss: 3.0409\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6314 - val_loss: 2.9762\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6242 - val_loss: 3.0296\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6199 - val_loss: 2.9641\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6088 - val_loss: 3.0125\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6026 - val_loss: 2.9555\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5886 - val_loss: 2.9833\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5804 - val_loss: 2.9517\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5749 - val_loss: 2.9706\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5718 - val_loss: 2.9514\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5698 - val_loss: 2.9662\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5691 - val_loss: 2.9526\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5685 - val_loss: 2.9663\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5685 - val_loss: 2.9512\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5688 - val_loss: 2.9708\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5696 - val_loss: 2.9492\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5712 - val_loss: 2.9807\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5739 - val_loss: 2.9501\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5764 - val_loss: 2.9890\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5801 - val_loss: 2.9514\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5831 - val_loss: 3.0020\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5912 - val_loss: 2.9561\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5930 - val_loss: 3.0145\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6004 - val_loss: 2.9556\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 57.4309 - val_loss: 56.7938\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 55.5341 - val_loss: 54.1409\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 52.8781 - val_loss: 50.9584\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 49.6918 - val_loss: 46.9603\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 45.6892 - val_loss: 41.8863\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 40.6077 - val_loss: 35.9497\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 34.6521 - val_loss: 29.8347\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 28.5010 - val_loss: 24.5083\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 23.2754 - val_loss: 21.1185\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 20.2227 - val_loss: 19.0540\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 18.5166 - val_loss: 17.7973\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.5288 - val_loss: 17.0669\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.9923 - val_loss: 16.6659\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 16.7276 - val_loss: 16.4477\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.5983 - val_loss: 16.3107\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.5130 - val_loss: 16.1976\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 16.4243 - val_loss: 16.0860\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.3193 - val_loss: 15.9700\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.2006 - val_loss: 15.8370\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 16.0623 - val_loss: 15.6673\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 15.8865 - val_loss: 15.4419\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 15.6519 - val_loss: 15.1323\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 15.3249 - val_loss: 14.6819\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.8410 - val_loss: 14.0047\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 14.1037 - val_loss: 13.0786\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 13.0945 - val_loss: 12.0989\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 12.0319 - val_loss: 11.0680\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 10.9244 - val_loss: 10.0339\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 9.8714 - val_loss: 9.2151\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.1116 - val_loss: 8.7966\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.7429 - val_loss: 8.7089\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.6354 - val_loss: 8.6695\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.5901 - val_loss: 8.6154\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.5408 - val_loss: 8.5778\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.5063 - val_loss: 8.5576\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.4813 - val_loss: 8.5309\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.4555 - val_loss: 8.4970\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.4262 - val_loss: 8.4650\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.3956 - val_loss: 8.4332\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3645 - val_loss: 8.3991\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3324 - val_loss: 8.3642\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2995 - val_loss: 8.3291\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.2660 - val_loss: 8.2919\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.2312 - val_loss: 8.2523\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.1943 - val_loss: 8.2102\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1548 - val_loss: 8.1649\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.1123 - val_loss: 8.1163\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.0667 - val_loss: 8.0646\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.0181 - val_loss: 8.0098\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.9668 - val_loss: 7.9523\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.9130 - val_loss: 7.8922\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.8569 - val_loss: 7.8300\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.7987 - val_loss: 7.7659\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.7387 - val_loss: 7.7001\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6769 - val_loss: 7.6328\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.6137 - val_loss: 7.5639\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 7.5490 - val_loss: 7.4936\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.4830 - val_loss: 7.4218\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.4158 - val_loss: 7.3487\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.3474 - val_loss: 7.2755\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.2781 - val_loss: 7.1989\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.2079 - val_loss: 7.1223\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.1369 - val_loss: 7.0442\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.0651 - val_loss: 6.9648\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.9928 - val_loss: 6.8853\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 6.9200 - val_loss: 6.8050\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 6.8470 - val_loss: 6.7252\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.7740 - val_loss: 6.6467\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.7010 - val_loss: 6.5680\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 6.6280 - val_loss: 6.4901\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.5551 - val_loss: 6.4130\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.4824 - val_loss: 6.3364\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 6.4097 - val_loss: 6.2603\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.3372 - val_loss: 6.1846\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.2647 - val_loss: 6.1091\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.1921 - val_loss: 6.0337\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.1193 - val_loss: 5.9584\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.0464 - val_loss: 5.8830\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.9731 - val_loss: 5.8074\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5.8995 - val_loss: 5.7315\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.8252 - val_loss: 5.6551\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.7504 - val_loss: 5.5782\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.6747 - val_loss: 5.5004\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.5981 - val_loss: 5.4217\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.5203 - val_loss: 5.3421\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.4412 - val_loss: 5.2613\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.3607 - val_loss: 5.1794\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.2786 - val_loss: 5.0964\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.1951 - val_loss: 5.0127\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.1105 - val_loss: 4.9284\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.0249 - val_loss: 4.8438\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.9391 - val_loss: 4.7598\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.8539 - val_loss: 4.6772\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.7702 - val_loss: 4.5971\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.6889 - val_loss: 4.5204\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.6105 - val_loss: 4.4473\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.5354 - val_loss: 4.3791\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.4640 - val_loss: 4.3150\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.3968 - val_loss: 4.2550\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3335 - val_loss: 4.1986\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2736 - val_loss: 4.1452\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.2169 - val_loss: 4.0939\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.1633 - val_loss: 4.0465\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.1130 - val_loss: 4.0020\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.0659 - val_loss: 3.9591\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.0219 - val_loss: 3.9203\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 3.9801 - val_loss: 3.8846\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.9405 - val_loss: 3.8503\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.9027 - val_loss: 3.8194\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.8665 - val_loss: 3.7897\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.8318 - val_loss: 3.7637\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.7988 - val_loss: 3.7397\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.7673 - val_loss: 3.7176\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.7372 - val_loss: 3.6955\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.7086 - val_loss: 3.6756\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.6812 - val_loss: 3.6582\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.6558 - val_loss: 3.6438\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.6332 - val_loss: 3.6303\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6132 - val_loss: 3.6249\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5964 - val_loss: 3.6101\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5792 - val_loss: 3.6084\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.5625 - val_loss: 3.5901\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.5460 - val_loss: 3.5857\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5262 - val_loss: 3.5629\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5085 - val_loss: 3.5610\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.4883 - val_loss: 3.5336\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4699 - val_loss: 3.5346\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.4511 - val_loss: 3.5091\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.4353 - val_loss: 3.5130\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.4183 - val_loss: 3.4893\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4050 - val_loss: 3.4967\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.3897 - val_loss: 3.4717\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.3771 - val_loss: 3.4787\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.3620 - val_loss: 3.4536\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.3501 - val_loss: 3.4606\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.3356 - val_loss: 3.4364\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.3243 - val_loss: 3.4425\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.3107 - val_loss: 3.4203\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.2994 - val_loss: 3.4246\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2871 - val_loss: 3.4034\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.2769 - val_loss: 3.4093\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.2657 - val_loss: 3.3892\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.2561 - val_loss: 3.3966\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.2458 - val_loss: 3.3759\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2364 - val_loss: 3.3843\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2265 - val_loss: 3.3631\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.2171 - val_loss: 3.3719\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.2077 - val_loss: 3.3505\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.1984 - val_loss: 3.3602\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1896 - val_loss: 3.3387\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.1806 - val_loss: 3.3493\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1725 - val_loss: 3.3273\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.1642 - val_loss: 3.3395\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1568 - val_loss: 3.3160\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1495 - val_loss: 3.3283\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.1419 - val_loss: 3.3059\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1344 - val_loss: 3.3170\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.1267 - val_loss: 3.2945\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1205 - val_loss: 3.3066\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1133 - val_loss: 3.2839\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1079 - val_loss: 3.2967\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1013 - val_loss: 3.2754\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0977 - val_loss: 3.2919\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0926 - val_loss: 3.2688\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0922 - val_loss: 3.2942\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0896 - val_loss: 3.2647\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0915 - val_loss: 3.2974\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0878 - val_loss: 3.2602\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0895 - val_loss: 3.2984\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0841 - val_loss: 3.2546\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0851 - val_loss: 3.2934\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0771 - val_loss: 3.2468\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0756 - val_loss: 3.2814\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0652 - val_loss: 3.2368\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0626 - val_loss: 3.2680\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0522 - val_loss: 3.2258\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0485 - val_loss: 3.2560\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0400 - val_loss: 3.2179\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0378 - val_loss: 3.2476\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0307 - val_loss: 3.2109\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0294 - val_loss: 3.2409\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0225 - val_loss: 3.2042\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0211 - val_loss: 3.2341\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0142 - val_loss: 3.1979\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0128 - val_loss: 3.2281\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0064 - val_loss: 3.1919\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0056 - val_loss: 3.2233\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9996 - val_loss: 3.1867\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9990 - val_loss: 3.2190\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9932 - val_loss: 3.1819\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9925 - val_loss: 3.2148\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9867 - val_loss: 3.1771\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9859 - val_loss: 3.2101\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9800 - val_loss: 3.1724\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9784 - val_loss: 3.2044\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9727 - val_loss: 3.1675\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9698 - val_loss: 3.1974\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9646 - val_loss: 3.1631\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9610 - val_loss: 3.1909\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9569 - val_loss: 3.1600\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9528 - val_loss: 3.1842\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9493 - val_loss: 3.1567\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9456 - val_loss: 3.1796\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9428 - val_loss: 3.1538\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9396 - val_loss: 3.1769\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9372 - val_loss: 3.1513\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9347 - val_loss: 3.1754\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9326 - val_loss: 3.1489\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9307 - val_loss: 3.1754\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9291 - val_loss: 3.1470\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9284 - val_loss: 3.1771\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9267 - val_loss: 3.1454\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9277 - val_loss: 3.1809\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9262 - val_loss: 3.1445\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9299 - val_loss: 3.1867\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9275 - val_loss: 3.1441\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9348 - val_loss: 3.1936\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9304 - val_loss: 3.1437\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9363 - val_loss: 3.1916\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9276 - val_loss: 3.1416\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9302 - val_loss: 3.1883\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9217 - val_loss: 3.1383\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9238 - val_loss: 3.1827\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9141 - val_loss: 3.1341\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9154 - val_loss: 3.1763\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9069 - val_loss: 3.1316\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9089 - val_loss: 3.1711\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9006 - val_loss: 3.1297\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9024 - val_loss: 3.1679\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8949 - val_loss: 3.1279\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8972 - val_loss: 3.1655\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8896 - val_loss: 3.1246\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8924 - val_loss: 3.1646\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8860 - val_loss: 3.1233\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8899 - val_loss: 3.1656\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8841 - val_loss: 3.1228\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8889 - val_loss: 3.1679\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8834 - val_loss: 3.1227\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8884 - val_loss: 3.1695\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8821 - val_loss: 3.1222\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8861 - val_loss: 3.1685\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8786 - val_loss: 3.1205\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8819 - val_loss: 3.1664\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8744 - val_loss: 3.1188\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8776 - val_loss: 3.1650\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8707 - val_loss: 3.1174\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8742 - val_loss: 3.1644\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8677 - val_loss: 3.1163\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8713 - val_loss: 3.1641\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8648 - val_loss: 3.1152\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8684 - val_loss: 3.1637\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8620 - val_loss: 3.1141\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8656 - val_loss: 3.1634\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8593 - val_loss: 3.1130\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8634 - val_loss: 3.1639\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8573 - val_loss: 3.1120\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8628 - val_loss: 3.1663\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8570 - val_loss: 3.1114\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8625 - val_loss: 3.1693\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8569 - val_loss: 3.1109\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8611 - val_loss: 3.1676\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8539 - val_loss: 3.1094\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8566 - val_loss: 3.1623\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8482 - val_loss: 3.1073\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8497 - val_loss: 3.1576\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8423 - val_loss: 3.1056\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8433 - val_loss: 3.1557\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8378 - val_loss: 3.1045\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8403 - val_loss: 3.1573\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8364 - val_loss: 3.1042\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8426 - val_loss: 3.1621\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8385 - val_loss: 3.1042\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8426 - val_loss: 3.1611\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8365 - val_loss: 3.1034\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8383 - val_loss: 3.1569\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8316 - val_loss: 3.1017\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8311 - val_loss: 3.1517\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8251 - val_loss: 3.1002\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8253 - val_loss: 3.1499\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8211 - val_loss: 3.0997\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8221 - val_loss: 3.1502\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8191 - val_loss: 3.0994\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8206 - val_loss: 3.1515\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8181 - val_loss: 3.0991\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8202 - val_loss: 3.1531\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8176 - val_loss: 3.0984\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8195 - val_loss: 3.1531\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8162 - val_loss: 3.0974\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8164 - val_loss: 3.1497\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8124 - val_loss: 3.0967\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8104 - val_loss: 3.1452\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8072 - val_loss: 3.0960\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8064 - val_loss: 3.1453\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8050 - val_loss: 3.0952\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8056 - val_loss: 3.1470\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8047 - val_loss: 3.0947\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8053 - val_loss: 3.1473\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8035 - val_loss: 3.0940\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8033 - val_loss: 3.1459\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8011 - val_loss: 3.0931\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8007 - val_loss: 3.1446\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7987 - val_loss: 3.0923\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7985 - val_loss: 3.1440\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7968 - val_loss: 3.0915\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7971 - val_loss: 3.1437\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7953 - val_loss: 3.0908\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7955 - val_loss: 3.1430\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7934 - val_loss: 3.0901\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7934 - val_loss: 3.1420\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7913 - val_loss: 3.0895\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7913 - val_loss: 3.1411\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7892 - val_loss: 3.0888\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7893 - val_loss: 3.1404\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7873 - val_loss: 3.0882\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7874 - val_loss: 3.1396\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7853 - val_loss: 3.0876\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7853 - val_loss: 3.1387\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7833 - val_loss: 3.0871\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7833 - val_loss: 3.1379\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7813 - val_loss: 3.0865\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7813 - val_loss: 3.1370\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7794 - val_loss: 3.0860\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7794 - val_loss: 3.1362\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7774 - val_loss: 3.0854\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7775 - val_loss: 3.1353\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7755 - val_loss: 3.0849\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7755 - val_loss: 3.1344\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7735 - val_loss: 3.0843\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7736 - val_loss: 3.1335\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7716 - val_loss: 3.0838\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7717 - val_loss: 3.1326\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7696 - val_loss: 3.0832\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7698 - val_loss: 3.1317\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7677 - val_loss: 3.0827\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7678 - val_loss: 3.1307\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7656 - val_loss: 3.0822\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7658 - val_loss: 3.1298\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7635 - val_loss: 3.0816\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7637 - val_loss: 3.1287\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7612 - val_loss: 3.0810\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7614 - val_loss: 3.1273\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7583 - val_loss: 3.0805\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7581 - val_loss: 3.1248\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7546 - val_loss: 3.0802\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7538 - val_loss: 3.1199\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7497 - val_loss: 3.0816\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.7478 - val_loss: 3.1162\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7456 - val_loss: 3.0831\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7447 - val_loss: 3.1164\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7437 - val_loss: 3.0817\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7442 - val_loss: 3.1216\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7444 - val_loss: 3.0787\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7470 - val_loss: 3.1266\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7470 - val_loss: 3.0763\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7510 - val_loss: 3.1294\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7492 - val_loss: 3.0749\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7529 - val_loss: 3.1267\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7485 - val_loss: 3.0744\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7486 - val_loss: 3.1163\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7411 - val_loss: 3.0767\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7362 - val_loss: 3.1064\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7320 - val_loss: 3.0802\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7278 - val_loss: 3.0971\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7248 - val_loss: 3.0814\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7231 - val_loss: 3.0948\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7218 - val_loss: 3.0825\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7212 - val_loss: 3.0948\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7204 - val_loss: 3.0819\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7200 - val_loss: 3.0948\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7192 - val_loss: 3.0806\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7187 - val_loss: 3.0949\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7179 - val_loss: 3.0798\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7173 - val_loss: 3.0947\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7166 - val_loss: 3.0782\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7160 - val_loss: 3.0947\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7153 - val_loss: 3.0771\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7145 - val_loss: 3.0938\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7138 - val_loss: 3.0760\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7130 - val_loss: 3.0927\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7122 - val_loss: 3.0751\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7115 - val_loss: 3.0917\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7107 - val_loss: 3.0740\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7101 - val_loss: 3.0910\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7094 - val_loss: 3.0727\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7089 - val_loss: 3.0910\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7083 - val_loss: 3.0711\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7079 - val_loss: 3.0920\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7074 - val_loss: 3.0688\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7074 - val_loss: 3.0975\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7081 - val_loss: 3.0652\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7091 - val_loss: 3.1081\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7114 - val_loss: 3.0609\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7163 - val_loss: 3.1153\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7169 - val_loss: 3.0596\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7203 - val_loss: 3.1111\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7149 - val_loss: 3.0596\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7143 - val_loss: 3.0982\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7078 - val_loss: 3.0637\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7020 - val_loss: 3.0870\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7002 - val_loss: 3.0619\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6993 - val_loss: 3.0911\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6999 - val_loss: 3.0602\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6997 - val_loss: 3.0954\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6999 - val_loss: 3.0602\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6988 - val_loss: 3.0950\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6991 - val_loss: 3.0596\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6978 - val_loss: 3.0946\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6980 - val_loss: 3.0586\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6964 - val_loss: 3.0933\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6965 - val_loss: 3.0577\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6948 - val_loss: 3.0913\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6947 - val_loss: 3.0569\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6930 - val_loss: 3.0893\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6929 - val_loss: 3.0561\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6914 - val_loss: 3.0874\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6913 - val_loss: 3.0552\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6898 - val_loss: 3.0859\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6897 - val_loss: 3.0543\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6884 - val_loss: 3.0844\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6882 - val_loss: 3.0534\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6869 - val_loss: 3.0830\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6867 - val_loss: 3.0525\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6855 - val_loss: 3.0816\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6853 - val_loss: 3.0515\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6842 - val_loss: 3.0804\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6839 - val_loss: 3.0505\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6828 - val_loss: 3.0791\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6825 - val_loss: 3.0495\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6815 - val_loss: 3.0778\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6811 - val_loss: 3.0485\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6801 - val_loss: 3.0766\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6798 - val_loss: 3.0474\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6788 - val_loss: 3.0754\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6784 - val_loss: 3.0463\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6775 - val_loss: 3.0742\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6771 - val_loss: 3.0451\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6762 - val_loss: 3.0730\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6758 - val_loss: 3.0439\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6749 - val_loss: 3.0718\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6745 - val_loss: 3.0427\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6737 - val_loss: 3.0705\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6732 - val_loss: 3.0415\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6724 - val_loss: 3.0693\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6719 - val_loss: 3.0402\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6711 - val_loss: 3.0681\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6706 - val_loss: 3.0390\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6698 - val_loss: 3.0669\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6693 - val_loss: 3.0376\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6685 - val_loss: 3.0657\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6680 - val_loss: 3.0363\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6672 - val_loss: 3.0645\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6667 - val_loss: 3.0349\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6660 - val_loss: 3.0634\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6655 - val_loss: 3.0335\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6647 - val_loss: 3.0623\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6642 - val_loss: 3.0320\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6635 - val_loss: 3.0613\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6630 - val_loss: 3.0306\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6624 - val_loss: 3.0603\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6617 - val_loss: 3.0291\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6612 - val_loss: 3.0593\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6605 - val_loss: 3.0277\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6600 - val_loss: 3.0584\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6592 - val_loss: 3.0263\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6588 - val_loss: 3.0574\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6580 - val_loss: 3.0248\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6578 - val_loss: 3.0567\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6568 - val_loss: 3.0232\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6568 - val_loss: 3.0560\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6557 - val_loss: 3.0220\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6557 - val_loss: 3.0549\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6543 - val_loss: 3.0209\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6543 - val_loss: 3.0535\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6527 - val_loss: 3.0197\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6528 - val_loss: 3.0522\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6512 - val_loss: 3.0184\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6513 - val_loss: 3.0491\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6491 - val_loss: 3.0179\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6491 - val_loss: 3.0447\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6466 - val_loss: 3.0192\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6464 - val_loss: 3.0430\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6447 - val_loss: 3.0184\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6451 - val_loss: 3.0440\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6439 - val_loss: 3.0157\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6449 - val_loss: 3.0448\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6432 - val_loss: 3.0144\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6438 - val_loss: 3.0430\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6417 - val_loss: 3.0142\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6420 - val_loss: 3.0414\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6401 - val_loss: 3.0133\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6405 - val_loss: 3.0411\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6390 - val_loss: 3.0116\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6395 - val_loss: 3.0404\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6379 - val_loss: 3.0103\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6382 - val_loss: 3.0390\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6364 - val_loss: 3.0096\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6367 - val_loss: 3.0380\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6351 - val_loss: 3.0084\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6354 - val_loss: 3.0374\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6339 - val_loss: 3.0068\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6342 - val_loss: 3.0367\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6327 - val_loss: 3.0055\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6330 - val_loss: 3.0357\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6315 - val_loss: 3.0043\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6317 - val_loss: 3.0349\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6302 - val_loss: 3.0030\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6305 - val_loss: 3.0343\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6290 - val_loss: 3.0015\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6294 - val_loss: 3.0338\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6279 - val_loss: 3.0000\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6284 - val_loss: 3.0334\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6269 - val_loss: 2.9985\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6274 - val_loss: 3.0332\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6258 - val_loss: 2.9971\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6264 - val_loss: 3.0335\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6250 - val_loss: 2.9954\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6257 - val_loss: 3.0353\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6247 - val_loss: 2.9934\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6258 - val_loss: 3.0364\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6243 - val_loss: 2.9919\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6248 - val_loss: 3.0340\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6224 - val_loss: 2.9914\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6223 - val_loss: 3.0303\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6200 - val_loss: 2.9915\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6198 - val_loss: 3.0241\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6169 - val_loss: 2.9964\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6152 - val_loss: 3.0173\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6135 - val_loss: 3.0013\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6124 - val_loss: 3.0116\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6113 - val_loss: 3.0049\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6105 - val_loss: 3.0088\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6097 - val_loss: 3.0067\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6090 - val_loss: 3.0085\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6084 - val_loss: 3.0056\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6077 - val_loss: 3.0098\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6071 - val_loss: 3.0025\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6066 - val_loss: 3.0119\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6061 - val_loss: 2.9980\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6057 - val_loss: 3.0130\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6053 - val_loss: 2.9940\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6049 - val_loss: 3.0156\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6048 - val_loss: 2.9879\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6055 - val_loss: 3.0185\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6049 - val_loss: 2.9846\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6054 - val_loss: 3.0184\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6040 - val_loss: 2.9843\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6038 - val_loss: 3.0156\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6022 - val_loss: 2.9851\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6015 - val_loss: 3.0127\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6002 - val_loss: 2.9850\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5998 - val_loss: 3.0120\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5989 - val_loss: 2.9836\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5988 - val_loss: 3.0121\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5979 - val_loss: 2.9823\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5978 - val_loss: 3.0117\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5968 - val_loss: 2.9815\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5966 - val_loss: 3.0109\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5955 - val_loss: 2.9809\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5952 - val_loss: 3.0099\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5942 - val_loss: 2.9803\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5937 - val_loss: 3.0090\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5928 - val_loss: 2.9796\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5924 - val_loss: 3.0084\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5916 - val_loss: 2.9788\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5912 - val_loss: 3.0079\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5904 - val_loss: 2.9780\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5900 - val_loss: 3.0074\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5892 - val_loss: 2.9772\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5887 - val_loss: 3.0067\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5879 - val_loss: 2.9766\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5874 - val_loss: 3.0059\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5866 - val_loss: 2.9759\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5861 - val_loss: 3.0053\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5853 - val_loss: 2.9753\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5848 - val_loss: 3.0047\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5840 - val_loss: 2.9746\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5835 - val_loss: 3.0041\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5828 - val_loss: 2.9740\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5822 - val_loss: 3.0036\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.5815 - val_loss: 2.9734\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5809 - val_loss: 3.0030\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5803 - val_loss: 2.9728\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5797 - val_loss: 3.0025\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5790 - val_loss: 2.9722\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5784 - val_loss: 3.0020\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5778 - val_loss: 2.9716\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5771 - val_loss: 3.0016\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5765 - val_loss: 2.9710\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5759 - val_loss: 3.0012\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5753 - val_loss: 2.9703\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5747 - val_loss: 3.0009\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5742 - val_loss: 2.9696\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5735 - val_loss: 3.0007\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5730 - val_loss: 2.9688\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5724 - val_loss: 3.0006\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5719 - val_loss: 2.9680\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5713 - val_loss: 3.0006\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5708 - val_loss: 2.9669\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5704 - val_loss: 3.0011\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5699 - val_loss: 2.9649\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 56.2549 - val_loss: 54.1954\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 54.3856 - val_loss: 51.3050\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 51.4949 - val_loss: 47.5786\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 47.7688 - val_loss: 42.9622\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 43.1535 - val_loss: 37.4663\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 37.6591 - val_loss: 31.3560\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 31.5548 - val_loss: 25.3322\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 25.5921 - val_loss: 20.7799\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 21.1125 - val_loss: 18.5855\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 18.7665 - val_loss: 17.3863\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 17.5107 - val_loss: 16.7495\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 16.8778 - val_loss: 16.4409\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.5891 - val_loss: 16.2924\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 16.4569 - val_loss: 16.1894\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 16.3629 - val_loss: 16.0721\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 16.2485 - val_loss: 15.9157\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 16.0910 - val_loss: 15.7066\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 15.8787 - val_loss: 15.4267\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 15.5945 - val_loss: 15.0456\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.2084 - val_loss: 14.5182\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.6738 - val_loss: 13.7899\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 13.9316 - val_loss: 12.8387\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 12.9556 - val_loss: 11.7673\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 11.8693 - val_loss: 10.6473\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.7553 - val_loss: 9.6017\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.7045 - val_loss: 8.9318\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.9842 - val_loss: 8.6798\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.6824 - val_loss: 8.6132\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.5916 - val_loss: 8.5567\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.5247 - val_loss: 8.5030\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4768 - val_loss: 8.4682\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.4494 - val_loss: 8.4430\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.4239 - val_loss: 8.4153\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3934 - val_loss: 8.3858\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.3620 - val_loss: 8.3580\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.3315 - val_loss: 8.3309\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.3017 - val_loss: 8.3026\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.2719 - val_loss: 8.2721\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.2409 - val_loss: 8.2400\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2077 - val_loss: 8.2055\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.1717 - val_loss: 8.1677\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1327 - val_loss: 8.1264\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0905 - val_loss: 8.0818\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.0451 - val_loss: 8.0340\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9964 - val_loss: 7.9830\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9443 - val_loss: 7.9289\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.8890 - val_loss: 7.8718\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.8304 - val_loss: 7.8115\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7688 - val_loss: 7.7482\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.7041 - val_loss: 7.6821\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.6366 - val_loss: 7.6135\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.5664 - val_loss: 7.5425\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.4936 - val_loss: 7.4692\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.4183 - val_loss: 7.3936\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.3408 - val_loss: 7.3160\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.2614 - val_loss: 7.2366\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.1801 - val_loss: 7.1555\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.0973 - val_loss: 7.0732\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.0133 - val_loss: 6.9907\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.9286 - val_loss: 6.9070\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 6.8435 - val_loss: 6.8223\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.7579 - val_loss: 6.7375\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 6.6720 - val_loss: 6.6524\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.5858 - val_loss: 6.5669\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.4993 - val_loss: 6.4814\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.4125 - val_loss: 6.3958\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.3252 - val_loss: 6.3098\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.2373 - val_loss: 6.2237\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 6.1488 - val_loss: 6.1370\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.0593 - val_loss: 6.0498\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.9688 - val_loss: 5.9619\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.8770 - val_loss: 5.8733\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.7835 - val_loss: 5.7840\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.6883 - val_loss: 5.6931\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.5910 - val_loss: 5.6016\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.4916 - val_loss: 5.5086\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.3904 - val_loss: 5.4148\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.2877 - val_loss: 5.3200\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.1848 - val_loss: 5.2267\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.0834 - val_loss: 5.1328\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.9856 - val_loss: 5.0483\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.8919 - val_loss: 4.9570\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.8037 - val_loss: 4.8924\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7253 - val_loss: 4.8099\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.6629 - val_loss: 4.7986\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.6210 - val_loss: 4.7344\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.6049 - val_loss: 4.7947\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.6045 - val_loss: 4.7031\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.5787 - val_loss: 4.7337\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.5379 - val_loss: 4.5845\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.4531 - val_loss: 4.5724\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.3794 - val_loss: 4.4330\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.2932 - val_loss: 4.4160\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.2234 - val_loss: 4.3063\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.1567 - val_loss: 4.2845\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.0968 - val_loss: 4.2014\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.0448 - val_loss: 4.1820\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.9967 - val_loss: 4.1161\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.9568 - val_loss: 4.1008\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.9177 - val_loss: 4.0467\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.8872 - val_loss: 4.0412\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.8590 - val_loss: 4.0009\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.8467 - val_loss: 4.0196\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.8374 - val_loss: 4.0059\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.8597 - val_loss: 4.0462\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.8685 - val_loss: 4.0292\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.8914 - val_loss: 4.0314\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.8589 - val_loss: 3.9545\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.8214 - val_loss: 3.9168\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7510 - val_loss: 3.8483\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7179 - val_loss: 3.8355\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6755 - val_loss: 3.7923\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.6625 - val_loss: 3.7899\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.6365 - val_loss: 3.7577\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.6299 - val_loss: 3.7532\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6063 - val_loss: 3.7199\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5938 - val_loss: 3.7048\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.5652 - val_loss: 3.6728\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5478 - val_loss: 3.6518\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.5191 - val_loss: 3.6225\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4990 - val_loss: 3.5991\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4729 - val_loss: 3.5759\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.4542 - val_loss: 3.5545\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.4344 - val_loss: 3.5374\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.4183 - val_loss: 3.5181\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.4026 - val_loss: 3.5040\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3886 - val_loss: 3.4864\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3755 - val_loss: 3.4753\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3636 - val_loss: 3.4600\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3526 - val_loss: 3.4499\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3422 - val_loss: 3.4367\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3322 - val_loss: 3.4263\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3225 - val_loss: 3.4148\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3133 - val_loss: 3.4043\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.3043 - val_loss: 3.3938\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.2957 - val_loss: 3.3835\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.2873 - val_loss: 3.3736\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2791 - val_loss: 3.3638\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.2712 - val_loss: 3.3543\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2635 - val_loss: 3.3446\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.2562 - val_loss: 3.3362\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.2492 - val_loss: 3.3265\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2426 - val_loss: 3.3207\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2367 - val_loss: 3.3100\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2310 - val_loss: 3.3081\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2271 - val_loss: 3.2968\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.2230 - val_loss: 3.2989\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2207 - val_loss: 3.2866\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2174 - val_loss: 3.2909\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2158 - val_loss: 3.2755\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2114 - val_loss: 3.2798\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.2069 - val_loss: 3.2629\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2022 - val_loss: 3.2694\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1997 - val_loss: 3.2513\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1941 - val_loss: 3.2580\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1909 - val_loss: 3.2385\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1849 - val_loss: 3.2474\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1827 - val_loss: 3.2269\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1765 - val_loss: 3.2398\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1774 - val_loss: 3.2167\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1697 - val_loss: 3.2305\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1695 - val_loss: 3.2048\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1609 - val_loss: 3.2164\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1573 - val_loss: 3.1904\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1482 - val_loss: 3.1995\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.1429 - val_loss: 3.1752\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.1340 - val_loss: 3.1839\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1298 - val_loss: 3.1608\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.1212 - val_loss: 3.1685\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1162 - val_loss: 3.1492\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1106 - val_loss: 3.1592\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1087 - val_loss: 3.1419\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1053 - val_loss: 3.1548\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1050 - val_loss: 3.1360\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1015 - val_loss: 3.1510\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1014 - val_loss: 3.1302\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0972 - val_loss: 3.1467\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0973 - val_loss: 3.1240\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0920 - val_loss: 3.1407\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0916 - val_loss: 3.1166\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0853 - val_loss: 3.1324\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0837 - val_loss: 3.1085\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0775 - val_loss: 3.1231\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0752 - val_loss: 3.1004\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0698 - val_loss: 3.1148\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0677 - val_loss: 3.0929\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0628 - val_loss: 3.1076\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0610 - val_loss: 3.0859\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0564 - val_loss: 3.1010\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0548 - val_loss: 3.0792\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0502 - val_loss: 3.0946\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0489 - val_loss: 3.0727\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0443 - val_loss: 3.0884\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0431 - val_loss: 3.0665\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0386 - val_loss: 3.0827\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.0378 - val_loss: 3.0607\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0334 - val_loss: 3.0776\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.0329 - val_loss: 3.0553\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0287 - val_loss: 3.0731\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0285 - val_loss: 3.0504\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0243 - val_loss: 3.0689\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0243 - val_loss: 3.0457\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0203 - val_loss: 3.0648\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.0203 - val_loss: 3.0415\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.0166 - val_loss: 3.0612\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0166 - val_loss: 3.0374\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0131 - val_loss: 3.0574\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0126 - val_loss: 3.0323\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0081 - val_loss: 3.0516\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0071 - val_loss: 3.0255\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0010 - val_loss: 3.0435\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 3.0000 - val_loss: 3.0195\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9948 - val_loss: 3.0378\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9947 - val_loss: 3.0150\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9906 - val_loss: 3.0345\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9911 - val_loss: 3.0117\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9878 - val_loss: 3.0325\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9885 - val_loss: 3.0092\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9857 - val_loss: 3.0308\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9862 - val_loss: 3.0067\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 2.9835 - val_loss: 3.0288\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9836 - val_loss: 3.0042\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.9810 - val_loss: 3.0269\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9811 - val_loss: 3.0026\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9794 - val_loss: 3.0266\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9799 - val_loss: 3.0015\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9781 - val_loss: 3.0250\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9777 - val_loss: 2.9991\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9753 - val_loss: 3.0218\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9741 - val_loss: 2.9960\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9717 - val_loss: 3.0182\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9703 - val_loss: 2.9931\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9684 - val_loss: 3.0151\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9669 - val_loss: 2.9904\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9653 - val_loss: 3.0124\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9639 - val_loss: 2.9878\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9624 - val_loss: 3.0099\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9610 - val_loss: 2.9853\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9596 - val_loss: 3.0080\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9587 - val_loss: 2.9833\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.9573 - val_loss: 3.0077\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9576 - val_loss: 2.9823\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9557 - val_loss: 3.0062\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9555 - val_loss: 2.9796\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9522 - val_loss: 3.0008\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9500 - val_loss: 2.9744\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9463 - val_loss: 2.9928\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9422 - val_loss: 2.9667\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9379 - val_loss: 2.9820\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9323 - val_loss: 2.9592\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9304 - val_loss: 2.9759\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9262 - val_loss: 2.9553\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9263 - val_loss: 2.9744\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9240 - val_loss: 2.9545\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9256 - val_loss: 2.9764\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9248 - val_loss: 2.9552\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9263 - val_loss: 2.9778\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9252 - val_loss: 2.9545\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9250 - val_loss: 2.9750\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9220 - val_loss: 2.9506\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9205 - val_loss: 2.9684\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9156 - val_loss: 2.9450\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9144 - val_loss: 2.9622\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9094 - val_loss: 2.9407\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9096 - val_loss: 2.9586\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9056 - val_loss: 2.9383\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9070 - val_loss: 2.9571\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9035 - val_loss: 2.9371\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.9054 - val_loss: 2.9561\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9017 - val_loss: 2.9356\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9035 - val_loss: 2.9539\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8990 - val_loss: 2.9332\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9006 - val_loss: 2.9506\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8955 - val_loss: 2.9303\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8972 - val_loss: 2.9472\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8918 - val_loss: 2.9277\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8941 - val_loss: 2.9443\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8886 - val_loss: 2.9256\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.8914 - val_loss: 2.9419\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8858 - val_loss: 2.9237\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8891 - val_loss: 2.9396\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8830 - val_loss: 2.9220\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8867 - val_loss: 2.9373\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8803 - val_loss: 2.9202\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8844 - val_loss: 2.9350\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8776 - val_loss: 2.9183\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8819 - val_loss: 2.9327\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8748 - val_loss: 2.9165\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8794 - val_loss: 2.9304\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8721 - val_loss: 2.9146\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8768 - val_loss: 2.9282\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8694 - val_loss: 2.9129\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8743 - val_loss: 2.9263\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8670 - val_loss: 2.9118\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8723 - val_loss: 2.9256\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8656 - val_loss: 2.9116\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8711 - val_loss: 2.9249\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8641 - val_loss: 2.9110\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8694 - val_loss: 2.9230\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8615 - val_loss: 2.9091\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8666 - val_loss: 2.9206\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8586 - val_loss: 2.9074\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8639 - val_loss: 2.9186\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8560 - val_loss: 2.9093\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8647 - val_loss: 2.9198\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8562 - val_loss: 2.9091\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8634 - val_loss: 2.9180\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8534 - val_loss: 2.9066\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8600 - val_loss: 2.9154\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8502 - val_loss: 2.9050\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8575 - val_loss: 2.9143\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8485 - val_loss: 2.9048\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8563 - val_loss: 2.9142\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8475 - val_loss: 2.9045\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8549 - val_loss: 2.9136\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8459 - val_loss: 2.9038\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8532 - val_loss: 2.9133\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8445 - val_loss: 2.9039\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8521 - val_loss: 2.9147\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8450 - val_loss: 2.9063\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8534 - val_loss: 2.9183\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8473 - val_loss: 2.9076\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8537 - val_loss: 2.9180\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8455 - val_loss: 2.9060\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8509 - val_loss: 2.9151\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8419 - val_loss: 2.9044\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.8484 - val_loss: 2.9136\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8398 - val_loss: 2.9034\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8464 - val_loss: 2.9120\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8373 - val_loss: 2.9017\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8436 - val_loss: 2.9096\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8338 - val_loss: 2.8998\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8408 - val_loss: 2.9079\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8314 - val_loss: 2.8991\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8391 - val_loss: 2.9071\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8296 - val_loss: 2.8981\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8372 - val_loss: 2.9056\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8271 - val_loss: 2.8962\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8344 - val_loss: 2.9035\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8241 - val_loss: 2.8941\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8313 - val_loss: 2.9014\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8212 - val_loss: 2.8916\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8279 - val_loss: 2.8991\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8181 - val_loss: 2.8881\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8235 - val_loss: 2.8964\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8146 - val_loss: 2.8848\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8194 - val_loss: 2.8942\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8116 - val_loss: 2.8831\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8169 - val_loss: 2.8929\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8095 - val_loss: 2.8822\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8151 - val_loss: 2.8921\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8077 - val_loss: 2.8811\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8131 - val_loss: 2.8911\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8057 - val_loss: 2.8799\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8110 - val_loss: 2.8899\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8036 - val_loss: 2.8789\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8090 - val_loss: 2.8890\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8017 - val_loss: 2.8780\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8071 - val_loss: 2.8881\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7998 - val_loss: 2.8771\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8052 - val_loss: 2.8873\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7979 - val_loss: 2.8762\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8033 - val_loss: 2.8865\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7961 - val_loss: 2.8753\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8014 - val_loss: 2.8858\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7943 - val_loss: 2.8744\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7995 - val_loss: 2.8852\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7926 - val_loss: 2.8736\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7976 - val_loss: 2.8846\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7909 - val_loss: 2.8727\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7956 - val_loss: 2.8840\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7892 - val_loss: 2.8719\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7937 - val_loss: 2.8834\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7875 - val_loss: 2.8710\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7917 - val_loss: 2.8826\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7857 - val_loss: 2.8701\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7898 - val_loss: 2.8818\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7839 - val_loss: 2.8692\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7878 - val_loss: 2.8810\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7820 - val_loss: 2.8683\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7858 - val_loss: 2.8801\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7800 - val_loss: 2.8674\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7839 - val_loss: 2.8793\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7781 - val_loss: 2.8666\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7819 - val_loss: 2.8784\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7761 - val_loss: 2.8657\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7799 - val_loss: 2.8775\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7741 - val_loss: 2.8647\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7779 - val_loss: 2.8766\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7721 - val_loss: 2.8637\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7758 - val_loss: 2.8757\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7700 - val_loss: 2.8626\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7736 - val_loss: 2.8747\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7679 - val_loss: 2.8614\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7713 - val_loss: 2.8737\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7658 - val_loss: 2.8600\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7687 - val_loss: 2.8726\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7635 - val_loss: 2.8557\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7637 - val_loss: 2.8694\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7596 - val_loss: 2.8530\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7597 - val_loss: 2.8667\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7560 - val_loss: 2.8478\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7534 - val_loss: 2.8592\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7488 - val_loss: 2.8421\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7468 - val_loss: 2.8545\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7445 - val_loss: 2.8398\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7430 - val_loss: 2.8519\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7412 - val_loss: 2.8384\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7396 - val_loss: 2.8476\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7366 - val_loss: 2.8371\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7350 - val_loss: 2.8452\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7337 - val_loss: 2.8364\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7332 - val_loss: 2.8458\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7328 - val_loss: 2.8360\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7333 - val_loss: 2.8475\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7325 - val_loss: 2.8361\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7329 - val_loss: 2.8511\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7339 - val_loss: 2.8396\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7367 - val_loss: 2.8568\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7368 - val_loss: 2.8471\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7432 - val_loss: 2.8605\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7382 - val_loss: 2.8498\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7445 - val_loss: 2.8639\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7396 - val_loss: 2.8546\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7480 - val_loss: 2.8675\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7418 - val_loss: 2.8587\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7517 - val_loss: 2.8699\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7429 - val_loss: 2.8604\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7524 - val_loss: 2.8719\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7431 - val_loss: 2.8607\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7520 - val_loss: 2.8719\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7423 - val_loss: 2.8608\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7510 - val_loss: 2.8708\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7404 - val_loss: 2.8627\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7525 - val_loss: 2.8756\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7442 - val_loss: 2.8699\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7594 - val_loss: 2.8785\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7464 - val_loss: 2.8686\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7573 - val_loss: 2.8748\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7423 - val_loss: 2.8645\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7517 - val_loss: 2.8697\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7366 - val_loss: 2.8592\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7450 - val_loss: 2.8638\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7297 - val_loss: 2.8493\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7332 - val_loss: 2.8513\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7171 - val_loss: 2.8300\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7100 - val_loss: 2.8363\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7054 - val_loss: 2.8308\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7038 - val_loss: 2.8333\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7028 - val_loss: 2.8302\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7021 - val_loss: 2.8338\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7016 - val_loss: 2.8289\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7016 - val_loss: 2.8366\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7016 - val_loss: 2.8286\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7016 - val_loss: 2.8387\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7013 - val_loss: 2.8289\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7011 - val_loss: 2.8395\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7003 - val_loss: 2.8289\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7002 - val_loss: 2.8388\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6987 - val_loss: 2.8283\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6985 - val_loss: 2.8382\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6970 - val_loss: 2.8281\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6970 - val_loss: 2.8387\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6960 - val_loss: 2.8284\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6963 - val_loss: 2.8393\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6950 - val_loss: 2.8287\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6952 - val_loss: 2.8396\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6939 - val_loss: 2.8286\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6943 - val_loss: 2.8394\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6926 - val_loss: 2.8283\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6928 - val_loss: 2.8396\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6916 - val_loss: 2.8282\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6926 - val_loss: 2.8414\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6917 - val_loss: 2.8289\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6934 - val_loss: 2.8437\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6927 - val_loss: 2.8297\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6940 - val_loss: 2.8437\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6919 - val_loss: 2.8287\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6924 - val_loss: 2.8425\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6900 - val_loss: 2.8278\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6905 - val_loss: 2.8418\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6884 - val_loss: 2.8275\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6892 - val_loss: 2.8418\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6873 - val_loss: 2.8275\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6883 - val_loss: 2.8421\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6866 - val_loss: 2.8278\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6879 - val_loss: 2.8439\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6874 - val_loss: 2.8324\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6923 - val_loss: 2.8503\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6927 - val_loss: 2.8441\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7037 - val_loss: 2.8567\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6986 - val_loss: 2.8529\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7127 - val_loss: 2.8650\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7070 - val_loss: 2.8632\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7241 - val_loss: 2.8815\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7230 - val_loss: 2.8692\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7325 - val_loss: 2.8817\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7229 - val_loss: 2.8640\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7251 - val_loss: 2.8697\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7093 - val_loss: 2.8591\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7174 - val_loss: 2.8720\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7099 - val_loss: 2.8621\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7199 - val_loss: 2.8746\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7123 - val_loss: 2.8626\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7199 - val_loss: 2.8712\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7082 - val_loss: 2.8562\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7109 - val_loss: 2.8629\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6981 - val_loss: 2.8437\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6955 - val_loss: 2.8481\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6814 - val_loss: 2.8269\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6744 - val_loss: 2.8316\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6661 - val_loss: 2.8202\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6625 - val_loss: 2.8263\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6617 - val_loss: 2.8211\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6616 - val_loss: 2.8300\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6626 - val_loss: 2.8229\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.6646 - val_loss: 2.8340\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6647 - val_loss: 2.8242\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6671 - val_loss: 2.8351\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6647 - val_loss: 2.8236\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6658 - val_loss: 2.8335\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6623 - val_loss: 2.8218\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.6620 - val_loss: 2.8310\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6593 - val_loss: 2.8210\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6591 - val_loss: 2.8317\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6587 - val_loss: 2.8224\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6606 - val_loss: 2.8348\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6602 - val_loss: 2.8251\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6637 - val_loss: 2.8392\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6628 - val_loss: 2.8276\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6653 - val_loss: 2.8419\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6638 - val_loss: 2.8284\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6654 - val_loss: 2.8424\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6634 - val_loss: 2.8284\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6646 - val_loss: 2.8432\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6631 - val_loss: 2.8284\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6646 - val_loss: 2.8436\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6627 - val_loss: 2.8287\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6650 - val_loss: 2.8430\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6619 - val_loss: 2.8280\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6639 - val_loss: 2.8424\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6607 - val_loss: 2.8270\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6620 - val_loss: 2.8411\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6588 - val_loss: 2.8258\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6598 - val_loss: 2.8399\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6569 - val_loss: 2.8248\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6578 - val_loss: 2.8389\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6552 - val_loss: 2.8242\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6565 - val_loss: 2.8387\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6543 - val_loss: 2.8243\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6559 - val_loss: 2.8392\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6540 - val_loss: 2.8248\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6558 - val_loss: 2.8396\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6536 - val_loss: 2.8255\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6561 - val_loss: 2.8404\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6540 - val_loss: 2.8273\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6580 - val_loss: 2.8448\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6580 - val_loss: 2.8327\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6622 - val_loss: 2.8482\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6606 - val_loss: 2.8320\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6601 - val_loss: 2.8441\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6558 - val_loss: 2.8252\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6525 - val_loss: 2.8334\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6449 - val_loss: 2.8182\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6434 - val_loss: 2.8294\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6402 - val_loss: 2.8175\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6397 - val_loss: 2.8283\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6383 - val_loss: 2.8190\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6419 - val_loss: 2.8325\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6416 - val_loss: 2.8224\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6464 - val_loss: 2.8387\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6477 - val_loss: 2.8262\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6497 - val_loss: 2.8411\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6491 - val_loss: 2.8249\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6472 - val_loss: 2.8366\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6434 - val_loss: 2.8216\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6427 - val_loss: 2.8324\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6386 - val_loss: 2.8191\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6386 - val_loss: 2.8299\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6353 - val_loss: 2.8188\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6370 - val_loss: 2.8309\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6352 - val_loss: 2.8201\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6387 - val_loss: 2.8372\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6408 - val_loss: 2.8250\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6443 - val_loss: 2.8407\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6437 - val_loss: 2.8249\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6433 - val_loss: 2.8384\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6405 - val_loss: 2.8211\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6387 - val_loss: 2.8344\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6360 - val_loss: 2.8189\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6354 - val_loss: 2.8310\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6316 - val_loss: 2.8176\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6319 - val_loss: 2.8305\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6299 - val_loss: 2.8187\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6325 - val_loss: 2.8359\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6346 - val_loss: 2.8239\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6408 - val_loss: 2.8424\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6410 - val_loss: 2.8287\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6459 - val_loss: 2.8429\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6406 - val_loss: 2.8254\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 56.9844 - val_loss: 52.9056\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 55.0974 - val_loss: 49.9340\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 52.1261 - val_loss: 45.9559\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 48.1486 - val_loss: 40.9422\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 43.1343 - val_loss: 35.1331\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 37.3180 - val_loss: 28.9568\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 31.1046 - val_loss: 23.3035\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 25.2117 - val_loss: 19.8305\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 21.0608 - val_loss: 18.0680\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 18.7766 - val_loss: 17.1904\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 17.4949 - val_loss: 16.8432\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.8476 - val_loss: 16.7611\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.5701 - val_loss: 16.7610\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.4597 - val_loss: 16.7422\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 16.3890 - val_loss: 16.6700\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 16.3022 - val_loss: 16.5445\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 16.1850 - val_loss: 16.3716\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 16.0338 - val_loss: 16.1478\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 15.8400 - val_loss: 15.8576\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 15.5860 - val_loss: 15.4725\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 15.2440 - val_loss: 14.9483\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 14.7733 - val_loss: 14.2212\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 14.1173 - val_loss: 13.2287\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 13.2227 - val_loss: 12.0534\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.1580 - val_loss: 10.9243\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 11.0997 - val_loss: 9.8735\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 10.0591 - val_loss: 9.0612\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.2596 - val_loss: 8.6226\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.8482 - val_loss: 8.4766\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.7131 - val_loss: 8.4120\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.6332 - val_loss: 8.3720\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.5746 - val_loss: 8.3345\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.5367 - val_loss: 8.2989\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.5054 - val_loss: 8.2666\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.4669 - val_loss: 8.2268\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4244 - val_loss: 8.1832\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.3816 - val_loss: 8.1425\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.3391 - val_loss: 8.1020\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2960 - val_loss: 8.0578\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.2509 - val_loss: 8.0114\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2030 - val_loss: 7.9623\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.1524 - val_loss: 7.9106\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.0990 - val_loss: 7.8569\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.0427 - val_loss: 7.8000\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.9832 - val_loss: 7.7400\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.9205 - val_loss: 7.6780\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8549 - val_loss: 7.6141\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.7867 - val_loss: 7.5485\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.7162 - val_loss: 7.4821\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.6439 - val_loss: 7.4148\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5701 - val_loss: 7.3469\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.4949 - val_loss: 7.2791\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.4187 - val_loss: 7.2112\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.3418 - val_loss: 7.1432\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.2644 - val_loss: 7.0761\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.1867 - val_loss: 7.0095\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.1091 - val_loss: 6.9431\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.0320 - val_loss: 6.8782\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.9557 - val_loss: 6.8150\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 6.8801 - val_loss: 6.7512\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.8052 - val_loss: 6.6891\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.7309 - val_loss: 6.6275\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.6572 - val_loss: 6.5658\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 6.5839 - val_loss: 6.5054\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.5110 - val_loss: 6.4453\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 6.4384 - val_loss: 6.3850\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.3658 - val_loss: 6.3250\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.2934 - val_loss: 6.2648\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.2209 - val_loss: 6.2042\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.1483 - val_loss: 6.1432\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.0756 - val_loss: 6.0816\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.0026 - val_loss: 6.0193\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.9294 - val_loss: 5.9565\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.8559 - val_loss: 5.8931\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.7820 - val_loss: 5.8285\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.7076 - val_loss: 5.7624\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.6327 - val_loss: 5.6942\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.5571 - val_loss: 5.6237\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.4806 - val_loss: 5.5508\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.4032 - val_loss: 5.4750\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.3246 - val_loss: 5.3962\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.2447 - val_loss: 5.3138\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.1635 - val_loss: 5.2287\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 5.0810 - val_loss: 5.1391\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.9969 - val_loss: 5.0459\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.9114 - val_loss: 4.9489\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.8245 - val_loss: 4.8488\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.7368 - val_loss: 4.7471\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.6489 - val_loss: 4.6458\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.5623 - val_loss: 4.5470\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.4789 - val_loss: 4.4529\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.3999 - val_loss: 4.3676\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.3260 - val_loss: 4.2930\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.2582 - val_loss: 4.2275\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.1964 - val_loss: 4.1702\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.1406 - val_loss: 4.1174\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.0896 - val_loss: 4.0723\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.0426 - val_loss: 4.0201\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.9989 - val_loss: 3.9860\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.9582 - val_loss: 3.9333\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.9203 - val_loss: 3.9268\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.8873 - val_loss: 3.8442\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.8623 - val_loss: 3.9280\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.8552 - val_loss: 3.7856\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.8626 - val_loss: 4.0332\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.9206 - val_loss: 3.8154\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.0079 - val_loss: 4.1941\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.0450 - val_loss: 3.8062\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.0079 - val_loss: 4.0479\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.9092 - val_loss: 3.6887\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.8179 - val_loss: 3.8401\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.7376 - val_loss: 3.5929\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6784 - val_loss: 3.7054\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.6399 - val_loss: 3.5386\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.6140 - val_loss: 3.6549\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.5934 - val_loss: 3.5062\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5818 - val_loss: 3.6324\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.5643 - val_loss: 3.4729\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.5533 - val_loss: 3.5981\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5315 - val_loss: 3.4400\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5217 - val_loss: 3.5633\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4996 - val_loss: 3.4109\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.4872 - val_loss: 3.5282\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.4684 - val_loss: 3.3800\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4510 - val_loss: 3.4884\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4364 - val_loss: 3.3515\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4173 - val_loss: 3.4424\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.4028 - val_loss: 3.3290\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3838 - val_loss: 3.3817\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.3652 - val_loss: 3.3219\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3490 - val_loss: 3.3270\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3367 - val_loss: 3.3091\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.3269 - val_loss: 3.2952\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3179 - val_loss: 3.2960\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.3092 - val_loss: 3.2753\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.3006 - val_loss: 3.2822\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2924 - val_loss: 3.2534\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2843 - val_loss: 3.2655\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.2763 - val_loss: 3.2372\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2686 - val_loss: 3.2544\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2613 - val_loss: 3.2172\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2547 - val_loss: 3.2472\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2480 - val_loss: 3.1995\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2420 - val_loss: 3.2438\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2366 - val_loss: 3.1817\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.2315 - val_loss: 3.2426\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.2268 - val_loss: 3.1647\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2236 - val_loss: 3.2395\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2166 - val_loss: 3.1518\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.2124 - val_loss: 3.2289\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.2057 - val_loss: 3.1388\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2017 - val_loss: 3.2245\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 3.1979 - val_loss: 3.1268\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1945 - val_loss: 3.2175\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1893 - val_loss: 3.1151\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1843 - val_loss: 3.2030\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1788 - val_loss: 3.1027\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1748 - val_loss: 3.1905\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1683 - val_loss: 3.0937\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.1643 - val_loss: 3.1777\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.1587 - val_loss: 3.0827\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.1555 - val_loss: 3.1678\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1508 - val_loss: 3.0729\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1476 - val_loss: 3.1596\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1435 - val_loss: 3.0685\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1371 - val_loss: 3.1453\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1335 - val_loss: 3.0596\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.1277 - val_loss: 3.1334\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1254 - val_loss: 3.0504\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1201 - val_loss: 3.1264\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1190 - val_loss: 3.0421\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1137 - val_loss: 3.1211\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1134 - val_loss: 3.0331\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1078 - val_loss: 3.1148\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1076 - val_loss: 3.0248\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1022 - val_loss: 3.1098\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1023 - val_loss: 3.0173\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0969 - val_loss: 3.1058\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0973 - val_loss: 3.0099\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0921 - val_loss: 3.1040\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0932 - val_loss: 3.0023\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0885 - val_loss: 3.1024\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0894 - val_loss: 2.9952\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0845 - val_loss: 3.0977\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0846 - val_loss: 2.9886\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0794 - val_loss: 3.0931\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0802 - val_loss: 2.9811\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0761 - val_loss: 3.0923\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0774 - val_loss: 2.9734\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0747 - val_loss: 3.0921\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0746 - val_loss: 2.9672\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0722 - val_loss: 3.0927\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0720 - val_loss: 2.9601\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0715 - val_loss: 3.0917\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0691 - val_loss: 2.9533\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0696 - val_loss: 3.0878\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0655 - val_loss: 2.9470\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0665 - val_loss: 3.0875\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0637 - val_loss: 2.9395\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0669 - val_loss: 3.0876\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0619 - val_loss: 2.9339\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0641 - val_loss: 3.0825\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0572 - val_loss: 2.9290\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0588 - val_loss: 3.0748\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0510 - val_loss: 2.9241\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0526 - val_loss: 3.0665\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0446 - val_loss: 2.9194\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0465 - val_loss: 3.0589\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0385 - val_loss: 2.9151\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0404 - val_loss: 3.0512\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0323 - val_loss: 2.9113\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0328 - val_loss: 3.0395\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0243 - val_loss: 2.9090\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0204 - val_loss: 3.0264\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0160 - val_loss: 2.9045\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0112 - val_loss: 3.0152\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0092 - val_loss: 2.8998\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0062 - val_loss: 3.0107\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0047 - val_loss: 2.8951\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0022 - val_loss: 3.0080\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0013 - val_loss: 2.8895\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0021 - val_loss: 3.0146\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0014 - val_loss: 2.8843\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0045 - val_loss: 3.0191\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0012 - val_loss: 2.8799\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0043 - val_loss: 3.0210\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0001 - val_loss: 2.8753\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0063 - val_loss: 3.0274\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0013 - val_loss: 2.8728\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0187 - val_loss: 3.0666\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0191 - val_loss: 2.8813\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0596 - val_loss: 3.1404\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0654 - val_loss: 2.9026\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.1087 - val_loss: 3.2272\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.1261 - val_loss: 2.9444\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1748 - val_loss: 3.2863\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1694 - val_loss: 2.9570\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1995 - val_loss: 3.2382\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1365 - val_loss: 2.8911\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0842 - val_loss: 3.0223\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9958 - val_loss: 2.8676\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9599 - val_loss: 2.8972\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9484 - val_loss: 2.8820\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9445 - val_loss: 2.8662\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9435 - val_loss: 2.8964\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9427 - val_loss: 2.8604\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9404 - val_loss: 2.8963\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9382 - val_loss: 2.8615\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9352 - val_loss: 2.8838\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9328 - val_loss: 2.8643\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9305 - val_loss: 2.8794\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9285 - val_loss: 2.8650\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9266 - val_loss: 2.8786\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9248 - val_loss: 2.8621\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9229 - val_loss: 2.8711\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9210 - val_loss: 2.8626\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9191 - val_loss: 2.8633\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9173 - val_loss: 2.8621\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9156 - val_loss: 2.8597\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9139 - val_loss: 2.8591\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9122 - val_loss: 2.8572\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9105 - val_loss: 2.8560\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9088 - val_loss: 2.8546\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9071 - val_loss: 2.8528\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9055 - val_loss: 2.8525\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9039 - val_loss: 2.8479\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9023 - val_loss: 2.8533\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9007 - val_loss: 2.8433\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8992 - val_loss: 2.8528\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8978 - val_loss: 2.8388\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8963 - val_loss: 2.8527\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8949 - val_loss: 2.8352\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8936 - val_loss: 2.8544\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8927 - val_loss: 2.8299\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8917 - val_loss: 2.8610\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8912 - val_loss: 2.8218\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8912 - val_loss: 2.8810\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8925 - val_loss: 2.8094\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8972 - val_loss: 2.9392\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9102 - val_loss: 2.8078\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9478 - val_loss: 3.0645\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9831 - val_loss: 2.8913\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1219 - val_loss: 3.3845\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2400 - val_loss: 3.0987\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.4574 - val_loss: 3.2261\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1305 - val_loss: 2.7910\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9674 - val_loss: 2.8708\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8943 - val_loss: 2.8582\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8831 - val_loss: 2.8220\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8792 - val_loss: 2.8252\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8769 - val_loss: 2.8128\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8752 - val_loss: 2.8267\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8734 - val_loss: 2.8177\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8719 - val_loss: 2.8317\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8701 - val_loss: 2.8105\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8686 - val_loss: 2.8376\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8676 - val_loss: 2.8055\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8666 - val_loss: 2.8400\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8649 - val_loss: 2.8083\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8631 - val_loss: 2.8358\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8617 - val_loss: 2.8078\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8602 - val_loss: 2.8348\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8591 - val_loss: 2.8044\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8581 - val_loss: 2.8358\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8570 - val_loss: 2.8022\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8559 - val_loss: 2.8347\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8546 - val_loss: 2.8018\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8533 - val_loss: 2.8329\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8521 - val_loss: 2.8011\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8509 - val_loss: 2.8318\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8498 - val_loss: 2.7996\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8487 - val_loss: 2.8311\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8476 - val_loss: 2.7983\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8465 - val_loss: 2.8303\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8454 - val_loss: 2.7975\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8443 - val_loss: 2.8291\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8432 - val_loss: 2.7968\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8421 - val_loss: 2.8279\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8410 - val_loss: 2.7961\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8400 - val_loss: 2.8269\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8389 - val_loss: 2.7951\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8379 - val_loss: 2.8264\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8369 - val_loss: 2.7932\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8361 - val_loss: 2.8278\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8353 - val_loss: 2.7840\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8364 - val_loss: 2.8535\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8407 - val_loss: 2.7693\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8559 - val_loss: 2.9903\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9111 - val_loss: 2.8629\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0904 - val_loss: 3.3236\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1779 - val_loss: 3.0077\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.3513 - val_loss: 3.1452\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0563 - val_loss: 2.7628\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9203 - val_loss: 2.8276\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8400 - val_loss: 2.8405\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8361 - val_loss: 2.7703\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8404 - val_loss: 2.8772\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8487 - val_loss: 2.7595\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8559 - val_loss: 2.9054\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8579 - val_loss: 2.7650\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8632 - val_loss: 2.9253\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8683 - val_loss: 2.7648\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8829 - val_loss: 2.9451\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8798 - val_loss: 2.7660\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8928 - val_loss: 2.9602\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8878 - val_loss: 2.7659\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8957 - val_loss: 2.9495\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8816 - val_loss: 2.7601\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8829 - val_loss: 2.9400\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8753 - val_loss: 2.7579\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8788 - val_loss: 2.9276\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8681 - val_loss: 2.7542\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8631 - val_loss: 2.9134\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8575 - val_loss: 2.7535\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8515 - val_loss: 2.8900\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8441 - val_loss: 2.7516\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8388 - val_loss: 2.8689\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8320 - val_loss: 2.7536\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8289 - val_loss: 2.8405\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8193 - val_loss: 2.7565\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8176 - val_loss: 2.8241\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8129 - val_loss: 2.7642\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8117 - val_loss: 2.8209\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8107 - val_loss: 2.7634\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8105 - val_loss: 2.8236\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8102 - val_loss: 2.7565\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8133 - val_loss: 2.8339\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8117 - val_loss: 2.7521\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8170 - val_loss: 2.8455\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8140 - val_loss: 2.7501\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8192 - val_loss: 2.8573\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8178 - val_loss: 2.7465\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8256 - val_loss: 2.8873\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8314 - val_loss: 2.7445\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8517 - val_loss: 2.9607\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8762 - val_loss: 2.7675\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9315 - val_loss: 2.9299\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8620 - val_loss: 2.7361\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8415 - val_loss: 2.8999\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8370 - val_loss: 2.7403\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8391 - val_loss: 2.9138\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8464 - val_loss: 2.7367\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8546 - val_loss: 2.9210\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8516 - val_loss: 2.7365\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8513 - val_loss: 2.9120\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8446 - val_loss: 2.7343\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8408 - val_loss: 2.8956\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8344 - val_loss: 2.7331\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8298 - val_loss: 2.8774\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8242 - val_loss: 2.7337\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8144 - val_loss: 2.8311\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8021 - val_loss: 2.7380\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7973 - val_loss: 2.7973\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7885 - val_loss: 2.7582\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7864 - val_loss: 2.7852\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7843 - val_loss: 2.7620\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7839 - val_loss: 2.7909\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7840 - val_loss: 2.7494\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7866 - val_loss: 2.8082\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7870 - val_loss: 2.7402\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7907 - val_loss: 2.8161\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7877 - val_loss: 2.7386\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7902 - val_loss: 2.8186\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7871 - val_loss: 2.7364\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7904 - val_loss: 2.8212\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7870 - val_loss: 2.7344\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7905 - val_loss: 2.8240\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7871 - val_loss: 2.7323\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7918 - val_loss: 2.8404\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7933 - val_loss: 2.7268\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8069 - val_loss: 2.8919\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8208 - val_loss: 2.7253\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8440 - val_loss: 2.9145\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8372 - val_loss: 2.7241\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8577 - val_loss: 2.8894\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8224 - val_loss: 2.7185\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8147 - val_loss: 2.8721\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8078 - val_loss: 2.7201\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8097 - val_loss: 2.8678\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8070 - val_loss: 2.7169\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8097 - val_loss: 2.8653\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8052 - val_loss: 2.7172\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8060 - val_loss: 2.8580\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8000 - val_loss: 2.7158\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8003 - val_loss: 2.8384\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7900 - val_loss: 2.7189\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7833 - val_loss: 2.8009\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7727 - val_loss: 2.7309\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7695 - val_loss: 2.7797\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7640 - val_loss: 2.7479\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7622 - val_loss: 2.7726\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7611 - val_loss: 2.7458\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7612 - val_loss: 2.7841\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7623 - val_loss: 2.7257\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7692 - val_loss: 2.8145\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7723 - val_loss: 2.7177\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7785 - val_loss: 2.8332\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7786 - val_loss: 2.7132\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7892 - val_loss: 2.8486\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7870 - val_loss: 2.7091\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8016 - val_loss: 2.8701\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8005 - val_loss: 2.7071\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8134 - val_loss: 2.8762\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8037 - val_loss: 2.7050\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8104 - val_loss: 2.8620\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7942 - val_loss: 2.7041\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7952 - val_loss: 2.8448\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7833 - val_loss: 2.7044\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7818 - val_loss: 2.8160\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7693 - val_loss: 2.7145\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7613 - val_loss: 2.7763\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7526 - val_loss: 2.7298\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7512 - val_loss: 2.7692\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7495 - val_loss: 2.7327\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7493 - val_loss: 2.7743\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7492 - val_loss: 2.7238\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7515 - val_loss: 2.7878\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7520 - val_loss: 2.7180\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7538 - val_loss: 2.7984\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7550 - val_loss: 2.7091\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7595 - val_loss: 2.8094\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7586 - val_loss: 2.7035\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7658 - val_loss: 2.8163\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7616 - val_loss: 2.7007\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7678 - val_loss: 2.8169\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7613 - val_loss: 2.7001\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7648 - val_loss: 2.8095\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7571 - val_loss: 2.7022\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7567 - val_loss: 2.7974\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7509 - val_loss: 2.7099\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7472 - val_loss: 2.7838\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7448 - val_loss: 2.7125\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7434 - val_loss: 2.7838\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7437 - val_loss: 2.7087\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7447 - val_loss: 2.7910\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7457 - val_loss: 2.6985\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7517 - val_loss: 2.8055\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7511 - val_loss: 2.6929\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7620 - val_loss: 2.8242\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7600 - val_loss: 2.6890\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7750 - val_loss: 2.8445\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7718 - val_loss: 2.6867\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7849 - val_loss: 2.8421\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7712 - val_loss: 2.6845\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7752 - val_loss: 2.8227\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7583 - val_loss: 2.6860\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7592 - val_loss: 2.8034\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7479 - val_loss: 2.6964\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7397 - val_loss: 2.7647\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7325 - val_loss: 2.7105\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7294 - val_loss: 2.7513\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7272 - val_loss: 2.7184\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7262 - val_loss: 2.7500\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7254 - val_loss: 2.7177\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7252 - val_loss: 2.7538\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7252 - val_loss: 2.7091\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7269 - val_loss: 2.7698\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7286 - val_loss: 2.6999\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7307 - val_loss: 2.7794\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7312 - val_loss: 2.6885\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7370 - val_loss: 2.7878\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7349 - val_loss: 2.6831\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7422 - val_loss: 2.7920\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7363 - val_loss: 2.6818\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7417 - val_loss: 2.7893\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7343 - val_loss: 2.6828\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7366 - val_loss: 2.7816\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7301 - val_loss: 2.6859\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7307 - val_loss: 2.7720\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7256 - val_loss: 2.6901\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7251 - val_loss: 2.7654\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7224 - val_loss: 2.6943\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7214 - val_loss: 2.7591\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7189 - val_loss: 2.6976\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7185 - val_loss: 2.7556\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7167 - val_loss: 2.6971\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7173 - val_loss: 2.7596\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7170 - val_loss: 2.6909\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7194 - val_loss: 2.7677\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7193 - val_loss: 2.6817\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7249 - val_loss: 2.7737\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7216 - val_loss: 2.6776\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7259 - val_loss: 2.7741\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7211 - val_loss: 2.6767\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7248 - val_loss: 2.7716\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7195 - val_loss: 2.6770\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7221 - val_loss: 2.7666\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7169 - val_loss: 2.6782\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7188 - val_loss: 2.7627\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7147 - val_loss: 2.6787\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7167 - val_loss: 2.7609\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7133 - val_loss: 2.6780\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7155 - val_loss: 2.7598\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7122 - val_loss: 2.6772\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7144 - val_loss: 2.7588\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7111 - val_loss: 2.6763\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7134 - val_loss: 2.7578\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7100 - val_loss: 2.6754\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7122 - val_loss: 2.7567\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7089 - val_loss: 2.6747\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7111 - val_loss: 2.7554\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7078 - val_loss: 2.6741\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7098 - val_loss: 2.7541\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7066 - val_loss: 2.6736\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7086 - val_loss: 2.7527\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7053 - val_loss: 2.6731\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7073 - val_loss: 2.7511\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.7041 - val_loss: 2.6728\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7058 - val_loss: 2.7493\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7027 - val_loss: 2.6730\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7041 - val_loss: 2.7467\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7010 - val_loss: 2.6743\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7018 - val_loss: 2.7423\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6986 - val_loss: 2.6767\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6990 - val_loss: 2.7370\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6957 - val_loss: 2.6803\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6961 - val_loss: 2.7323\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6931 - val_loss: 2.6839\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6934 - val_loss: 2.7307\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.6914 - val_loss: 2.6835\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6924 - val_loss: 2.7322\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6909 - val_loss: 2.6792\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6930 - val_loss: 2.7395\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6929 - val_loss: 2.6663\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6992 - val_loss: 2.7595\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7010 - val_loss: 2.6557\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7116 - val_loss: 2.7784\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7105 - val_loss: 2.6514\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7173 - val_loss: 2.7833\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7129 - val_loss: 2.6507\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7163 - val_loss: 2.7766\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7090 - val_loss: 2.6509\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7074 - val_loss: 2.7527\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6972 - val_loss: 2.6558\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6954 - val_loss: 2.7355\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6890 - val_loss: 2.6686\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6861 - val_loss: 2.7147\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6815 - val_loss: 2.6870\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6799 - val_loss: 2.7081\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6788 - val_loss: 2.6913\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6781 - val_loss: 2.7030\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6772 - val_loss: 2.6951\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6766 - val_loss: 2.6997\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6759 - val_loss: 2.6994\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6753 - val_loss: 2.6987\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6748 - val_loss: 2.6988\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6742 - val_loss: 2.6989\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6737 - val_loss: 2.6971\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6732 - val_loss: 2.6998\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6726 - val_loss: 2.6951\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6722 - val_loss: 2.7007\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6717 - val_loss: 2.6916\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6713 - val_loss: 2.7018\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6708 - val_loss: 2.6878\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6705 - val_loss: 2.7039\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6700 - val_loss: 2.6815\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6704 - val_loss: 2.7163\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6715 - val_loss: 2.6619\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6775 - val_loss: 2.7546\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6861 - val_loss: 2.6431\n",
      "Epoch 1/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 55.6260\n",
      "Epoch 2/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 49.8495\n",
      "Epoch 3/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 39.2953\n",
      "Epoch 4/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 26.6130\n",
      "Epoch 5/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.2044\n",
      "Epoch 6/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.1016\n",
      "Epoch 7/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.5866\n",
      "Epoch 8/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.4307\n",
      "Epoch 9/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 16.2390\n",
      "Epoch 10/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.9366\n",
      "Epoch 11/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.3968\n",
      "Epoch 12/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.3466\n",
      "Epoch 13/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.7188\n",
      "Epoch 14/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.7136\n",
      "Epoch 15/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.1234\n",
      "Epoch 16/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.7013\n",
      "Epoch 17/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5385\n",
      "Epoch 18/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4783\n",
      "Epoch 19/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4278\n",
      "Epoch 20/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.3539\n",
      "Epoch 21/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3195\n",
      "Epoch 22/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.2397\n",
      "Epoch 23/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1835\n",
      "Epoch 24/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1984\n",
      "Epoch 25/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0274\n",
      "Epoch 26/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8921\n",
      "Epoch 27/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7680\n",
      "Epoch 28/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.7116\n",
      "Epoch 29/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5284\n",
      "Epoch 30/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.2888\n",
      "Epoch 31/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1308\n",
      "Epoch 32/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.9185\n",
      "Epoch 33/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.7330\n",
      "Epoch 34/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.5215\n",
      "Epoch 35/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4052\n",
      "Epoch 36/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.0957\n",
      "Epoch 37/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8851\n",
      "Epoch 38/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7208\n",
      "Epoch 39/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4652\n",
      "Epoch 40/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2297\n",
      "Epoch 41/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0508\n",
      "Epoch 42/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8606\n",
      "Epoch 43/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7565\n",
      "Epoch 44/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6374\n",
      "Epoch 45/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 4.7206\n",
      "Epoch 46/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4068\n",
      "Epoch 47/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.455 - 0s 2ms/step - loss: 4.4580\n",
      "Epoch 48/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5029\n",
      "Epoch 49/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2469\n",
      "Epoch 50/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1256\n",
      "Epoch 51/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.2092\n",
      "Epoch 52/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0787\n",
      "Epoch 53/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.8846\n",
      "Epoch 54/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.8278\n",
      "Epoch 55/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.7917\n",
      "Epoch 56/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.7339\n",
      "Epoch 57/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.7771\n",
      "Epoch 58/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6693\n",
      "Epoch 59/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.6587\n",
      "Epoch 60/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.5949\n",
      "Epoch 61/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.6837\n",
      "Epoch 62/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5722\n",
      "Epoch 63/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6005\n",
      "Epoch 64/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.6999\n",
      "Epoch 65/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.5225\n",
      "Epoch 66/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5064\n",
      "Epoch 67/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.4477\n",
      "Epoch 68/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.5280\n",
      "Epoch 69/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.5464\n",
      "Epoch 70/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5183\n",
      "Epoch 71/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5876\n",
      "Epoch 72/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3907\n",
      "Epoch 73/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3763\n",
      "Epoch 74/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.4559\n",
      "Epoch 75/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6318\n",
      "Epoch 76/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3144\n",
      "Epoch 77/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.2502\n",
      "Epoch 78/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4116\n",
      "Epoch 79/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3259\n",
      "Epoch 80/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3237\n",
      "Epoch 81/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2243\n",
      "Epoch 82/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2012\n",
      "Epoch 83/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2433\n",
      "Epoch 84/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2709\n",
      "Epoch 85/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.2430\n",
      "Epoch 86/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2375\n",
      "Epoch 87/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1672\n",
      "Epoch 88/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.209 - 0s 3ms/step - loss: 3.2295\n",
      "Epoch 89/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3576\n",
      "Epoch 90/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.2266\n",
      "Epoch 91/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1762\n",
      "Epoch 92/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1282\n",
      "Epoch 93/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3157\n",
      "Epoch 94/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1596\n",
      "Epoch 95/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1250\n",
      "Epoch 96/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1575\n",
      "Epoch 97/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1199\n",
      "Epoch 98/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.2023\n",
      "Epoch 99/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0984\n",
      "Epoch 100/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1084\n",
      "Epoch 101/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0681\n",
      "Epoch 102/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.0968\n",
      "Epoch 103/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1552\n",
      "Epoch 104/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0678\n",
      "Epoch 105/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0317\n",
      "Epoch 106/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0350\n",
      "Epoch 107/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0916\n",
      "Epoch 108/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.043 - 0s 3ms/step - loss: 3.0829\n",
      "Epoch 109/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1889\n",
      "Epoch 110/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0084\n",
      "Epoch 111/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0403\n",
      "Epoch 112/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0183\n",
      "Epoch 113/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0793\n",
      "Epoch 114/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0539\n",
      "Epoch 115/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0196\n",
      "Epoch 116/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0343\n",
      "Epoch 117/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0826\n",
      "Epoch 118/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.1591\n",
      "Epoch 119/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0110\n",
      "Epoch 120/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9763\n",
      "Epoch 121/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9827\n",
      "Epoch 122/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0337\n",
      "Epoch 123/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0434\n",
      "Epoch 124/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0442\n",
      "Epoch 125/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0341\n",
      "Epoch 126/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0578\n",
      "Epoch 127/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0535\n",
      "Epoch 128/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9675\n",
      "Epoch 129/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9306\n",
      "Epoch 130/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0036\n",
      "Epoch 131/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0893\n",
      "Epoch 132/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9450\n",
      "Epoch 133/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9499\n",
      "Epoch 134/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0346\n",
      "Epoch 135/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1005\n",
      "Epoch 136/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0281\n",
      "Epoch 137/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1403\n",
      "Epoch 138/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9828\n",
      "Epoch 139/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9587\n",
      "Epoch 140/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0260\n",
      "Epoch 141/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9319\n",
      "Epoch 142/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9346\n",
      "Epoch 143/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9131\n",
      "Epoch 144/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9053\n",
      "Epoch 145/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0009\n",
      "Epoch 146/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.933 - 0s 3ms/step - loss: 2.9286\n",
      "Epoch 147/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9188\n",
      "Epoch 148/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8853\n",
      "Epoch 149/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9204\n",
      "Epoch 150/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9321\n",
      "Epoch 151/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9036\n",
      "Epoch 152/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8826\n",
      "Epoch 153/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8827\n",
      "Epoch 154/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9207\n",
      "Epoch 155/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8805\n",
      "Epoch 156/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9487\n",
      "Epoch 157/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9982\n",
      "Epoch 158/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9228\n",
      "Epoch 159/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8529\n",
      "Epoch 160/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8881\n",
      "Epoch 161/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8877\n",
      "Epoch 162/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8604\n",
      "Epoch 163/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9273\n",
      "Epoch 164/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8941\n",
      "Epoch 165/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8868\n",
      "Epoch 166/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9397\n",
      "Epoch 167/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9016\n",
      "Epoch 168/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8789\n",
      "Epoch 169/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8432\n",
      "Epoch 170/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8692\n",
      "Epoch 171/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9007\n",
      "Epoch 172/600\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.8615\n",
      "Epoch 173/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9075\n",
      "Epoch 174/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9073\n",
      "Epoch 175/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8247\n",
      "Epoch 176/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8926\n",
      "Epoch 177/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8427\n",
      "Epoch 178/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9971\n",
      "Epoch 179/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9008\n",
      "Epoch 180/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9205\n",
      "Epoch 181/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8345\n",
      "Epoch 182/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8312\n",
      "Epoch 183/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8343\n",
      "Epoch 184/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8485\n",
      "Epoch 185/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0024\n",
      "Epoch 186/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8390\n",
      "Epoch 187/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9126\n",
      "Epoch 188/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9551\n",
      "Epoch 189/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8105\n",
      "Epoch 190/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8942\n",
      "Epoch 191/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.868 - 0s 2ms/step - loss: 2.8662\n",
      "Epoch 192/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8216\n",
      "Epoch 193/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8954\n",
      "Epoch 194/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8246\n",
      "Epoch 195/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8162\n",
      "Epoch 196/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8634\n",
      "Epoch 197/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9231\n",
      "Epoch 198/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8868\n",
      "Epoch 199/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8647\n",
      "Epoch 200/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7967\n",
      "Epoch 201/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7755\n",
      "Epoch 202/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8023\n",
      "Epoch 203/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7751\n",
      "Epoch 204/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8299\n",
      "Epoch 205/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8744\n",
      "Epoch 206/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8076\n",
      "Epoch 207/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8452\n",
      "Epoch 208/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7834\n",
      "Epoch 209/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9351\n",
      "Epoch 210/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7825\n",
      "Epoch 211/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8263\n",
      "Epoch 212/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7800\n",
      "Epoch 213/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8119\n",
      "Epoch 214/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.9121\n",
      "Epoch 215/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7871\n",
      "Epoch 216/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8205\n",
      "Epoch 217/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7932\n",
      "Epoch 218/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8117\n",
      "Epoch 219/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8046\n",
      "Epoch 220/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7742\n",
      "Epoch 221/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7440\n",
      "Epoch 222/600\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.9010\n",
      "Epoch 223/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7714\n",
      "Epoch 224/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7548\n",
      "Epoch 225/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8083\n",
      "Epoch 226/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7578\n",
      "Epoch 227/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7643\n",
      "Epoch 228/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8385\n",
      "Epoch 229/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7350\n",
      "Epoch 230/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8073\n",
      "Epoch 231/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8297\n",
      "Epoch 232/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7871\n",
      "Epoch 233/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7452\n",
      "Epoch 234/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8288\n",
      "Epoch 235/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7307\n",
      "Epoch 236/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8448\n",
      "Epoch 237/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8134\n",
      "Epoch 238/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7239\n",
      "Epoch 239/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7461\n",
      "Epoch 240/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7730\n",
      "Epoch 241/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7734\n",
      "Epoch 242/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7267\n",
      "Epoch 243/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7630\n",
      "Epoch 244/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7227\n",
      "Epoch 245/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7974\n",
      "Epoch 246/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7113\n",
      "Epoch 247/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7429\n",
      "Epoch 248/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7194\n",
      "Epoch 249/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7498\n",
      "Epoch 250/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7432\n",
      "Epoch 251/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7394\n",
      "Epoch 252/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7070\n",
      "Epoch 253/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7236\n",
      "Epoch 254/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7845\n",
      "Epoch 255/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7278\n",
      "Epoch 256/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7135\n",
      "Epoch 257/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7308\n",
      "Epoch 258/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6967\n",
      "Epoch 259/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6947\n",
      "Epoch 260/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7298\n",
      "Epoch 261/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7687\n",
      "Epoch 262/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7628\n",
      "Epoch 263/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7081\n",
      "Epoch 264/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8061\n",
      "Epoch 265/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7160\n",
      "Epoch 266/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6938\n",
      "Epoch 267/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6990\n",
      "Epoch 268/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6906\n",
      "Epoch 269/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0196\n",
      "Epoch 270/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7162\n",
      "Epoch 271/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.734 - 0s 3ms/step - loss: 2.7447\n",
      "Epoch 272/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6865\n",
      "Epoch 273/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6866\n",
      "Epoch 274/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7075\n",
      "Epoch 275/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6913\n",
      "Epoch 276/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6827\n",
      "Epoch 277/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7814\n",
      "Epoch 278/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.8127\n",
      "Epoch 279/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7211\n",
      "Epoch 280/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7311\n",
      "Epoch 281/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6687\n",
      "Epoch 282/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6959\n",
      "Epoch 283/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7666\n",
      "Epoch 284/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6948\n",
      "Epoch 285/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7558\n",
      "Epoch 286/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7368\n",
      "Epoch 287/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8253\n",
      "Epoch 288/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6945\n",
      "Epoch 289/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6721\n",
      "Epoch 290/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6962\n",
      "Epoch 291/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6662\n",
      "Epoch 292/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6723\n",
      "Epoch 293/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7563\n",
      "Epoch 294/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6732\n",
      "Epoch 295/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6887\n",
      "Epoch 296/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7870\n",
      "Epoch 297/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7050\n",
      "Epoch 298/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6885\n",
      "Epoch 299/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6806\n",
      "Epoch 300/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6543\n",
      "Epoch 301/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7471\n",
      "Epoch 302/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7122\n",
      "Epoch 303/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7359\n",
      "Epoch 304/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7123\n",
      "Epoch 305/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7271\n",
      "Epoch 306/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6895\n",
      "Epoch 307/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7039\n",
      "Epoch 308/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6649\n",
      "Epoch 309/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6853\n",
      "Epoch 310/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6463\n",
      "Epoch 311/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6556\n",
      "Epoch 312/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7153\n",
      "Epoch 313/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6712\n",
      "Epoch 314/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6383\n",
      "Epoch 315/600\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.6803\n",
      "Epoch 316/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6789\n",
      "Epoch 317/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6442\n",
      "Epoch 318/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6729\n",
      "Epoch 319/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6752\n",
      "Epoch 320/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7035\n",
      "Epoch 321/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6517\n",
      "Epoch 322/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6677\n",
      "Epoch 323/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.683 - 0s 2ms/step - loss: 2.6977\n",
      "Epoch 324/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6446\n",
      "Epoch 325/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8529\n",
      "Epoch 326/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6544\n",
      "Epoch 327/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7479\n",
      "Epoch 328/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7464\n",
      "Epoch 329/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6538\n",
      "Epoch 330/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7695\n",
      "Epoch 331/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6693\n",
      "Epoch 332/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7564\n",
      "Epoch 333/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6863\n",
      "Epoch 334/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6391\n",
      "Epoch 335/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6968\n",
      "Epoch 336/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6325\n",
      "Epoch 337/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6616\n",
      "Epoch 338/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6610\n",
      "Epoch 339/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6384\n",
      "Epoch 340/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6694\n",
      "Epoch 341/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6290\n",
      "Epoch 342/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6893\n",
      "Epoch 343/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7609\n",
      "Epoch 344/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6499\n",
      "Epoch 345/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6352\n",
      "Epoch 346/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6423\n",
      "Epoch 347/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6581\n",
      "Epoch 348/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6741\n",
      "Epoch 349/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6547\n",
      "Epoch 350/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6595\n",
      "Epoch 351/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6862\n",
      "Epoch 352/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6340\n",
      "Epoch 353/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6446\n",
      "Epoch 354/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6170\n",
      "Epoch 355/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6539\n",
      "Epoch 356/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6205\n",
      "Epoch 357/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6399\n",
      "Epoch 358/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6221\n",
      "Epoch 359/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6700\n",
      "Epoch 360/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6837\n",
      "Epoch 361/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6709\n",
      "Epoch 362/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6050\n",
      "Epoch 363/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7231\n",
      "Epoch 364/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7958\n",
      "Epoch 365/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6315\n",
      "Epoch 366/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6281\n",
      "Epoch 367/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6727\n",
      "Epoch 368/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6274\n",
      "Epoch 369/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5990\n",
      "Epoch 370/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6410\n",
      "Epoch 371/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6022\n",
      "Epoch 372/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6193\n",
      "Epoch 373/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6102\n",
      "Epoch 374/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6352\n",
      "Epoch 375/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7053\n",
      "Epoch 376/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6560\n",
      "Epoch 377/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5962\n",
      "Epoch 378/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6177\n",
      "Epoch 379/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6271\n",
      "Epoch 380/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6490\n",
      "Epoch 381/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5941\n",
      "Epoch 382/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6153\n",
      "Epoch 383/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5946\n",
      "Epoch 384/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6372\n",
      "Epoch 385/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6096\n",
      "Epoch 386/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5970\n",
      "Epoch 387/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6188\n",
      "Epoch 388/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6178\n",
      "Epoch 389/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5881\n",
      "Epoch 390/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6489\n",
      "Epoch 391/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5969\n",
      "Epoch 392/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5963\n",
      "Epoch 393/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5787\n",
      "Epoch 394/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5871\n",
      "Epoch 395/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5867\n",
      "Epoch 396/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6419\n",
      "Epoch 397/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5851\n",
      "Epoch 398/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5874\n",
      "Epoch 399/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6374\n",
      "Epoch 400/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5952\n",
      "Epoch 401/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5860\n",
      "Epoch 402/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6041\n",
      "Epoch 403/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6421\n",
      "Epoch 404/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5946\n",
      "Epoch 405/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5892\n",
      "Epoch 406/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6339\n",
      "Epoch 407/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5919\n",
      "Epoch 408/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6008\n",
      "Epoch 409/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5836\n",
      "Epoch 410/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6585\n",
      "Epoch 411/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5809\n",
      "Epoch 412/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6707\n",
      "Epoch 413/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6214\n",
      "Epoch 414/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5934\n",
      "Epoch 415/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5815\n",
      "Epoch 416/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6312\n",
      "Epoch 417/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6632\n",
      "Epoch 418/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5997\n",
      "Epoch 419/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5964\n",
      "Epoch 420/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.539 - 0s 2ms/step - loss: 2.6096\n",
      "Epoch 421/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5845\n",
      "Epoch 422/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5955\n",
      "Epoch 423/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5924\n",
      "Epoch 424/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5797\n",
      "Epoch 425/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5851\n",
      "Epoch 426/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6509\n",
      "Epoch 427/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6333\n",
      "Epoch 428/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5844\n",
      "Epoch 429/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5823\n",
      "Epoch 430/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6420\n",
      "Epoch 431/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5882\n",
      "Epoch 432/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5882\n",
      "Epoch 433/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5956\n",
      "Epoch 434/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5819\n",
      "Epoch 435/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7289\n",
      "Epoch 436/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6178\n",
      "Epoch 437/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6272\n",
      "Epoch 438/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.623 - 0s 2ms/step - loss: 2.6038\n",
      "Epoch 439/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5719\n",
      "Epoch 440/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5725\n",
      "Epoch 441/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5784\n",
      "Epoch 442/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5813\n",
      "Epoch 443/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5685\n",
      "Epoch 444/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5649\n",
      "Epoch 445/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6014\n",
      "Epoch 446/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5582\n",
      "Epoch 447/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6136\n",
      "Epoch 448/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6392\n",
      "Epoch 449/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5500\n",
      "Epoch 450/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5567\n",
      "Epoch 451/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5755\n",
      "Epoch 452/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6159\n",
      "Epoch 453/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6102\n",
      "Epoch 454/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6116\n",
      "Epoch 455/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5585\n",
      "Epoch 456/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5545\n",
      "Epoch 457/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5538\n",
      "Epoch 458/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5539\n",
      "Epoch 459/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5566\n",
      "Epoch 460/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5651\n",
      "Epoch 461/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5682\n",
      "Epoch 462/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5457\n",
      "Epoch 463/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5658\n",
      "Epoch 464/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5644\n",
      "Epoch 465/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5574\n",
      "Epoch 466/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5581\n",
      "Epoch 467/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5451\n",
      "Epoch 468/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5820\n",
      "Epoch 469/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5844\n",
      "Epoch 470/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6067\n",
      "Epoch 471/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5577\n",
      "Epoch 472/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5520\n",
      "Epoch 473/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5429\n",
      "Epoch 474/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5420\n",
      "Epoch 475/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5883\n",
      "Epoch 476/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5813\n",
      "Epoch 477/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5802\n",
      "Epoch 478/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5890\n",
      "Epoch 479/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6337\n",
      "Epoch 480/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5908\n",
      "Epoch 481/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6027\n",
      "Epoch 482/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5728\n",
      "Epoch 483/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5588\n",
      "Epoch 484/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5587\n",
      "Epoch 485/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5370\n",
      "Epoch 486/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5807\n",
      "Epoch 487/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5890\n",
      "Epoch 488/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5975\n",
      "Epoch 489/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5541\n",
      "Epoch 490/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5821\n",
      "Epoch 491/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5475\n",
      "Epoch 492/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6553\n",
      "Epoch 493/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6308\n",
      "Epoch 494/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5489\n",
      "Epoch 495/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5397\n",
      "Epoch 496/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5851\n",
      "Epoch 497/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5977\n",
      "Epoch 498/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5365\n",
      "Epoch 499/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5796\n",
      "Epoch 500/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5606\n",
      "Epoch 501/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5657\n",
      "Epoch 502/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5309\n",
      "Epoch 503/600\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.5793\n",
      "Epoch 504/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5728\n",
      "Epoch 505/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5475\n",
      "Epoch 506/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5491\n",
      "Epoch 507/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5354\n",
      "Epoch 508/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6002\n",
      "Epoch 509/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5859\n",
      "Epoch 510/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5936\n",
      "Epoch 511/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6293\n",
      "Epoch 512/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5867\n",
      "Epoch 513/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6037\n",
      "Epoch 514/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.511 - 0s 2ms/step - loss: 2.5363\n",
      "Epoch 515/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5656\n",
      "Epoch 516/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5414\n",
      "Epoch 517/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5358\n",
      "Epoch 518/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5217\n",
      "Epoch 519/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5750\n",
      "Epoch 520/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5699\n",
      "Epoch 521/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5661\n",
      "Epoch 522/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5431\n",
      "Epoch 523/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5201\n",
      "Epoch 524/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5834\n",
      "Epoch 525/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5321\n",
      "Epoch 526/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5217\n",
      "Epoch 527/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5482\n",
      "Epoch 528/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5663\n",
      "Epoch 529/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5386\n",
      "Epoch 530/600\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.5336\n",
      "Epoch 531/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6428\n",
      "Epoch 532/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5329\n",
      "Epoch 533/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5446\n",
      "Epoch 534/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5635\n",
      "Epoch 535/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5690\n",
      "Epoch 536/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5256\n",
      "Epoch 537/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5259\n",
      "Epoch 538/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5477\n",
      "Epoch 539/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6171\n",
      "Epoch 540/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5235\n",
      "Epoch 541/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5151\n",
      "Epoch 542/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5133\n",
      "Epoch 543/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5259\n",
      "Epoch 544/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5440\n",
      "Epoch 545/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5192\n",
      "Epoch 546/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5136\n",
      "Epoch 547/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5183\n",
      "Epoch 548/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5297\n",
      "Epoch 549/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5594\n",
      "Epoch 550/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5160\n",
      "Epoch 551/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5261\n",
      "Epoch 552/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5407\n",
      "Epoch 553/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5163\n",
      "Epoch 554/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5531\n",
      "Epoch 555/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5254\n",
      "Epoch 556/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5042\n",
      "Epoch 557/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4993\n",
      "Epoch 558/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5395\n",
      "Epoch 559/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5404\n",
      "Epoch 560/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5562\n",
      "Epoch 561/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.562 - 0s 2ms/step - loss: 2.5236\n",
      "Epoch 562/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5411\n",
      "Epoch 563/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4968\n",
      "Epoch 564/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5355\n",
      "Epoch 565/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5016\n",
      "Epoch 566/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5269\n",
      "Epoch 567/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5151\n",
      "Epoch 568/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5109\n",
      "Epoch 569/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5062\n",
      "Epoch 570/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5334\n",
      "Epoch 571/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5444\n",
      "Epoch 572/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5522\n",
      "Epoch 573/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5911\n",
      "Epoch 574/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5302\n",
      "Epoch 575/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5175\n",
      "Epoch 576/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5278\n",
      "Epoch 577/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5148\n",
      "Epoch 578/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4935\n",
      "Epoch 579/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5076\n",
      "Epoch 580/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5411\n",
      "Epoch 581/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5129\n",
      "Epoch 582/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5209\n",
      "Epoch 583/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5237\n",
      "Epoch 584/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5321\n",
      "Epoch 585/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4967\n",
      "Epoch 586/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.471 - 0s 2ms/step - loss: 2.5096\n",
      "Epoch 587/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5820\n",
      "Epoch 588/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5544\n",
      "Epoch 589/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5509\n",
      "Epoch 590/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5727\n",
      "Epoch 591/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5472\n",
      "Epoch 592/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5174\n",
      "Epoch 593/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4859\n",
      "Epoch 594/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5060\n",
      "Epoch 595/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5793\n",
      "Epoch 596/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4970\n",
      "Epoch 597/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5502\n",
      "Epoch 598/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5001\n",
      "Epoch 599/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5084\n",
      "Epoch 600/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [55.6259651184082,\n",
       "  49.8494987487793,\n",
       "  39.2952880859375,\n",
       "  26.61297607421875,\n",
       "  19.204370498657227,\n",
       "  17.10157012939453,\n",
       "  16.586631774902344,\n",
       "  16.430715560913086,\n",
       "  16.238954544067383,\n",
       "  15.936568260192871,\n",
       "  15.39681339263916,\n",
       "  14.346609115600586,\n",
       "  12.718762397766113,\n",
       "  10.713644027709961,\n",
       "  9.123400688171387,\n",
       "  8.701292037963867,\n",
       "  8.538537979125977,\n",
       "  8.478301048278809,\n",
       "  8.427812576293945,\n",
       "  8.353949546813965,\n",
       "  8.319498062133789,\n",
       "  8.239707946777344,\n",
       "  8.18350601196289,\n",
       "  8.198384284973145,\n",
       "  8.02735424041748,\n",
       "  7.892054080963135,\n",
       "  7.768004894256592,\n",
       "  7.71157169342041,\n",
       "  7.528373718261719,\n",
       "  7.288827896118164,\n",
       "  7.130802154541016,\n",
       "  6.918467998504639,\n",
       "  6.733037948608398,\n",
       "  6.521510124206543,\n",
       "  6.405158042907715,\n",
       "  6.0956950187683105,\n",
       "  5.885077953338623,\n",
       "  5.720829010009766,\n",
       "  5.4651970863342285,\n",
       "  5.229676246643066,\n",
       "  5.0508222579956055,\n",
       "  4.86061954498291,\n",
       "  4.756466388702393,\n",
       "  4.637425899505615,\n",
       "  4.720574378967285,\n",
       "  4.4067792892456055,\n",
       "  4.4579854011535645,\n",
       "  4.502875328063965,\n",
       "  4.2468647956848145,\n",
       "  4.125566482543945,\n",
       "  4.209178924560547,\n",
       "  4.078658580780029,\n",
       "  3.8846139907836914,\n",
       "  3.8277974128723145,\n",
       "  3.7916882038116455,\n",
       "  3.7339327335357666,\n",
       "  3.777129888534546,\n",
       "  3.669306516647339,\n",
       "  3.658729076385498,\n",
       "  3.594906806945801,\n",
       "  3.683659315109253,\n",
       "  3.5721969604492188,\n",
       "  3.6004700660705566,\n",
       "  3.699939012527466,\n",
       "  3.5225095748901367,\n",
       "  3.506437063217163,\n",
       "  3.4476559162139893,\n",
       "  3.5280344486236572,\n",
       "  3.5464210510253906,\n",
       "  3.518308162689209,\n",
       "  3.5875632762908936,\n",
       "  3.390660524368286,\n",
       "  3.3763129711151123,\n",
       "  3.455937147140503,\n",
       "  3.6318469047546387,\n",
       "  3.3143715858459473,\n",
       "  3.250169277191162,\n",
       "  3.4116125106811523,\n",
       "  3.325863838195801,\n",
       "  3.3236851692199707,\n",
       "  3.2242746353149414,\n",
       "  3.2011771202087402,\n",
       "  3.2432878017425537,\n",
       "  3.270948648452759,\n",
       "  3.2430155277252197,\n",
       "  3.2375147342681885,\n",
       "  3.167163372039795,\n",
       "  3.2295150756835938,\n",
       "  3.3575754165649414,\n",
       "  3.2266054153442383,\n",
       "  3.1761839389801025,\n",
       "  3.1281652450561523,\n",
       "  3.315734624862671,\n",
       "  3.1595616340637207,\n",
       "  3.1249983310699463,\n",
       "  3.1575443744659424,\n",
       "  3.119910955429077,\n",
       "  3.2023301124572754,\n",
       "  3.0984246730804443,\n",
       "  3.1083858013153076,\n",
       "  3.0680923461914062,\n",
       "  3.096816062927246,\n",
       "  3.1551713943481445,\n",
       "  3.0678226947784424,\n",
       "  3.031731128692627,\n",
       "  3.0350284576416016,\n",
       "  3.091609477996826,\n",
       "  3.0829029083251953,\n",
       "  3.188898801803589,\n",
       "  3.0084307193756104,\n",
       "  3.0403060913085938,\n",
       "  3.018347978591919,\n",
       "  3.079301118850708,\n",
       "  3.0539205074310303,\n",
       "  3.0195698738098145,\n",
       "  3.0342721939086914,\n",
       "  3.0826268196105957,\n",
       "  3.159137725830078,\n",
       "  3.0109920501708984,\n",
       "  2.976317882537842,\n",
       "  2.982696294784546,\n",
       "  3.0336766242980957,\n",
       "  3.043428421020508,\n",
       "  3.044220447540283,\n",
       "  3.0340542793273926,\n",
       "  3.057819366455078,\n",
       "  3.0534844398498535,\n",
       "  2.9675357341766357,\n",
       "  2.9306154251098633,\n",
       "  3.003636598587036,\n",
       "  3.0893280506134033,\n",
       "  2.9450130462646484,\n",
       "  2.949932336807251,\n",
       "  3.0346388816833496,\n",
       "  3.1005020141601562,\n",
       "  3.0280609130859375,\n",
       "  3.1402924060821533,\n",
       "  2.9828004837036133,\n",
       "  2.958693265914917,\n",
       "  3.025979518890381,\n",
       "  2.9318654537200928,\n",
       "  2.9345812797546387,\n",
       "  2.913081169128418,\n",
       "  2.9053428173065186,\n",
       "  3.00089430809021,\n",
       "  2.928640842437744,\n",
       "  2.9188449382781982,\n",
       "  2.8853492736816406,\n",
       "  2.920374631881714,\n",
       "  2.932136058807373,\n",
       "  2.9036307334899902,\n",
       "  2.882640838623047,\n",
       "  2.8826560974121094,\n",
       "  2.920694351196289,\n",
       "  2.8804988861083984,\n",
       "  2.948650598526001,\n",
       "  2.9981627464294434,\n",
       "  2.92277193069458,\n",
       "  2.852915048599243,\n",
       "  2.88808274269104,\n",
       "  2.8876633644104004,\n",
       "  2.86041259765625,\n",
       "  2.9272615909576416,\n",
       "  2.89411997795105,\n",
       "  2.8868308067321777,\n",
       "  2.939662218093872,\n",
       "  2.901622772216797,\n",
       "  2.878887414932251,\n",
       "  2.8431508541107178,\n",
       "  2.8691983222961426,\n",
       "  2.900730609893799,\n",
       "  2.8615317344665527,\n",
       "  2.907458782196045,\n",
       "  2.9072511196136475,\n",
       "  2.8246679306030273,\n",
       "  2.8926055431365967,\n",
       "  2.84269118309021,\n",
       "  2.9971282482147217,\n",
       "  2.900773048400879,\n",
       "  2.920508861541748,\n",
       "  2.834540605545044,\n",
       "  2.831216335296631,\n",
       "  2.834258556365967,\n",
       "  2.8485004901885986,\n",
       "  3.002425193786621,\n",
       "  2.838999032974243,\n",
       "  2.912578821182251,\n",
       "  2.955134630203247,\n",
       "  2.81046986579895,\n",
       "  2.8941538333892822,\n",
       "  2.8661935329437256,\n",
       "  2.821626901626587,\n",
       "  2.895404815673828,\n",
       "  2.824561357498169,\n",
       "  2.8161659240722656,\n",
       "  2.8634045124053955,\n",
       "  2.9230659008026123,\n",
       "  2.886845827102661,\n",
       "  2.864689826965332,\n",
       "  2.796729803085327,\n",
       "  2.7755274772644043,\n",
       "  2.802320957183838,\n",
       "  2.775137424468994,\n",
       "  2.829944372177124,\n",
       "  2.87436580657959,\n",
       "  2.807598352432251,\n",
       "  2.8452072143554688,\n",
       "  2.7833971977233887,\n",
       "  2.9350943565368652,\n",
       "  2.7824809551239014,\n",
       "  2.826301097869873,\n",
       "  2.7799999713897705,\n",
       "  2.811908721923828,\n",
       "  2.912109851837158,\n",
       "  2.7870731353759766,\n",
       "  2.820525646209717,\n",
       "  2.793224811553955,\n",
       "  2.8117473125457764,\n",
       "  2.8045873641967773,\n",
       "  2.774174690246582,\n",
       "  2.744011402130127,\n",
       "  2.900972366333008,\n",
       "  2.7713844776153564,\n",
       "  2.7548248767852783,\n",
       "  2.80826997756958,\n",
       "  2.7578413486480713,\n",
       "  2.7642745971679688,\n",
       "  2.838451385498047,\n",
       "  2.735034942626953,\n",
       "  2.8072924613952637,\n",
       "  2.8297231197357178,\n",
       "  2.787079095840454,\n",
       "  2.7452499866485596,\n",
       "  2.8288064002990723,\n",
       "  2.730656862258911,\n",
       "  2.8448233604431152,\n",
       "  2.81335711479187,\n",
       "  2.723891496658325,\n",
       "  2.746119976043701,\n",
       "  2.772970199584961,\n",
       "  2.7734243869781494,\n",
       "  2.7266857624053955,\n",
       "  2.7630488872528076,\n",
       "  2.722703695297241,\n",
       "  2.79736065864563,\n",
       "  2.7112631797790527,\n",
       "  2.7428698539733887,\n",
       "  2.7193665504455566,\n",
       "  2.749821186065674,\n",
       "  2.743215799331665,\n",
       "  2.739360809326172,\n",
       "  2.706972360610962,\n",
       "  2.723612070083618,\n",
       "  2.7844676971435547,\n",
       "  2.727820634841919,\n",
       "  2.713454008102417,\n",
       "  2.730823278427124,\n",
       "  2.696744918823242,\n",
       "  2.6947433948516846,\n",
       "  2.729849100112915,\n",
       "  2.7686684131622314,\n",
       "  2.7627835273742676,\n",
       "  2.708109140396118,\n",
       "  2.806143045425415,\n",
       "  2.7160184383392334,\n",
       "  2.6937859058380127,\n",
       "  2.6989643573760986,\n",
       "  2.6905901432037354,\n",
       "  3.0195751190185547,\n",
       "  2.7162234783172607,\n",
       "  2.7447140216827393,\n",
       "  2.6865005493164062,\n",
       "  2.6865625381469727,\n",
       "  2.7075138092041016,\n",
       "  2.6913492679595947,\n",
       "  2.6827385425567627,\n",
       "  2.7814433574676514,\n",
       "  2.8126938343048096,\n",
       "  2.721095323562622,\n",
       "  2.7310965061187744,\n",
       "  2.668745279312134,\n",
       "  2.6959116458892822,\n",
       "  2.766601800918579,\n",
       "  2.694807767868042,\n",
       "  2.755765914916992,\n",
       "  2.7367706298828125,\n",
       "  2.825331449508667,\n",
       "  2.694532632827759,\n",
       "  2.6721034049987793,\n",
       "  2.6961796283721924,\n",
       "  2.6661934852600098,\n",
       "  2.672297477722168,\n",
       "  2.756254196166992,\n",
       "  2.6732094287872314,\n",
       "  2.688734292984009,\n",
       "  2.786985397338867,\n",
       "  2.7050399780273438,\n",
       "  2.688521385192871,\n",
       "  2.6806070804595947,\n",
       "  2.6542749404907227,\n",
       "  2.747135639190674,\n",
       "  2.7121825218200684,\n",
       "  2.7358758449554443,\n",
       "  2.7122514247894287,\n",
       "  2.7270941734313965,\n",
       "  2.6894516944885254,\n",
       "  2.703947067260742,\n",
       "  2.664937734603882,\n",
       "  2.685344696044922,\n",
       "  2.6463451385498047,\n",
       "  2.6556053161621094,\n",
       "  2.7153358459472656,\n",
       "  2.6711854934692383,\n",
       "  2.638273239135742,\n",
       "  2.6803014278411865,\n",
       "  2.6789379119873047,\n",
       "  2.6441915035247803,\n",
       "  2.6729214191436768,\n",
       "  2.675154209136963,\n",
       "  2.703458547592163,\n",
       "  2.651712656021118,\n",
       "  2.6676697731018066,\n",
       "  2.6977245807647705,\n",
       "  2.6446497440338135,\n",
       "  2.8528690338134766,\n",
       "  2.6543540954589844,\n",
       "  2.747931480407715,\n",
       "  2.746387481689453,\n",
       "  2.6537578105926514,\n",
       "  2.7695038318634033,\n",
       "  2.6692612171173096,\n",
       "  2.7564408779144287,\n",
       "  2.6863231658935547,\n",
       "  2.639113426208496,\n",
       "  2.6968278884887695,\n",
       "  2.6324923038482666,\n",
       "  2.6616404056549072,\n",
       "  2.6609725952148438,\n",
       "  2.638415575027466,\n",
       "  2.669395923614502,\n",
       "  2.6290013790130615,\n",
       "  2.689345598220825,\n",
       "  2.760925531387329,\n",
       "  2.649948835372925,\n",
       "  2.6352224349975586,\n",
       "  2.642253875732422,\n",
       "  2.658108949661255,\n",
       "  2.6740541458129883,\n",
       "  2.6547350883483887,\n",
       "  2.6594715118408203,\n",
       "  2.686234712600708,\n",
       "  2.6339528560638428,\n",
       "  2.6446497440338135,\n",
       "  2.616957664489746,\n",
       "  2.6538524627685547,\n",
       "  2.620490789413452,\n",
       "  2.6398823261260986,\n",
       "  2.622144937515259,\n",
       "  2.669987678527832,\n",
       "  2.6837034225463867,\n",
       "  2.670945882797241,\n",
       "  2.6049582958221436,\n",
       "  2.723053216934204,\n",
       "  2.7958059310913086,\n",
       "  2.6315155029296875,\n",
       "  2.628140687942505,\n",
       "  2.6726529598236084,\n",
       "  2.6273996829986572,\n",
       "  2.599001169204712,\n",
       "  2.6409924030303955,\n",
       "  2.602205991744995,\n",
       "  2.6193392276763916,\n",
       "  2.610222816467285,\n",
       "  2.6351866722106934,\n",
       "  2.705305576324463,\n",
       "  2.656022787094116,\n",
       "  2.596237897872925,\n",
       "  2.6177361011505127,\n",
       "  2.6271121501922607,\n",
       "  2.6490049362182617,\n",
       "  2.594088554382324,\n",
       "  2.61527419090271,\n",
       "  2.594619035720825,\n",
       "  2.6372454166412354,\n",
       "  2.6096081733703613,\n",
       "  2.596980571746826,\n",
       "  2.618807554244995,\n",
       "  2.617811441421509,\n",
       "  2.588080883026123,\n",
       "  2.6488544940948486,\n",
       "  2.596860408782959,\n",
       "  2.596325635910034,\n",
       "  2.578676223754883,\n",
       "  2.5870862007141113,\n",
       "  2.5867488384246826,\n",
       "  2.6418914794921875,\n",
       "  2.5851285457611084,\n",
       "  2.587430238723755,\n",
       "  2.6374378204345703,\n",
       "  2.595156669616699,\n",
       "  2.586012840270996,\n",
       "  2.6040830612182617,\n",
       "  2.6421444416046143,\n",
       "  2.5946168899536133,\n",
       "  2.589175224304199,\n",
       "  2.6338889598846436,\n",
       "  2.59185528755188,\n",
       "  2.6007680892944336,\n",
       "  2.583606481552124,\n",
       "  2.6584765911102295,\n",
       "  2.580939292907715,\n",
       "  2.6707024574279785,\n",
       "  2.6214401721954346,\n",
       "  2.593439817428589,\n",
       "  2.581509590148926,\n",
       "  2.6311960220336914,\n",
       "  2.663198709487915,\n",
       "  2.599656581878662,\n",
       "  2.596393346786499,\n",
       "  2.6095798015594482,\n",
       "  2.5845019817352295,\n",
       "  2.59553861618042,\n",
       "  2.592372179031372,\n",
       "  2.5797371864318848,\n",
       "  2.585073947906494,\n",
       "  2.6508944034576416,\n",
       "  2.633305549621582,\n",
       "  2.584440231323242,\n",
       "  2.582258939743042,\n",
       "  2.6419880390167236,\n",
       "  2.588200807571411,\n",
       "  2.5881712436676025,\n",
       "  2.595641851425171,\n",
       "  2.581918954849243,\n",
       "  2.7288594245910645,\n",
       "  2.6178324222564697,\n",
       "  2.627206325531006,\n",
       "  2.603835105895996,\n",
       "  2.5718860626220703,\n",
       "  2.5724539756774902,\n",
       "  2.5783956050872803,\n",
       "  2.581326961517334,\n",
       "  2.5685226917266846,\n",
       "  2.564911127090454,\n",
       "  2.6013617515563965,\n",
       "  2.5582289695739746,\n",
       "  2.6136419773101807,\n",
       "  2.6391751766204834,\n",
       "  2.549992799758911,\n",
       "  2.556689739227295,\n",
       "  2.5755069255828857,\n",
       "  2.615858316421509,\n",
       "  2.6101906299591064,\n",
       "  2.611588478088379,\n",
       "  2.558450937271118,\n",
       "  2.5545194149017334,\n",
       "  2.5538361072540283,\n",
       "  2.5538971424102783,\n",
       "  2.5566041469573975,\n",
       "  2.5650856494903564,\n",
       "  2.5681629180908203,\n",
       "  2.545668363571167,\n",
       "  2.5657777786254883,\n",
       "  2.5643932819366455,\n",
       "  2.557447671890259,\n",
       "  2.5580899715423584,\n",
       "  2.5451159477233887,\n",
       "  2.5819873809814453,\n",
       "  2.5844366550445557,\n",
       "  2.6067006587982178,\n",
       "  2.557748794555664,\n",
       "  2.551980972290039,\n",
       "  2.5429279804229736,\n",
       "  2.5419931411743164,\n",
       "  2.588251829147339,\n",
       "  2.58127498626709,\n",
       "  2.58020281791687,\n",
       "  2.5889668464660645,\n",
       "  2.6337199211120605,\n",
       "  2.590831995010376,\n",
       "  2.60270619392395,\n",
       "  2.572751998901367,\n",
       "  2.5587637424468994,\n",
       "  2.558732032775879,\n",
       "  2.5370383262634277,\n",
       "  2.580735683441162,\n",
       "  2.5890214443206787,\n",
       "  2.5974864959716797,\n",
       "  2.5540592670440674,\n",
       "  2.582137107849121,\n",
       "  2.547452926635742,\n",
       "  2.6553006172180176,\n",
       "  2.630795478820801,\n",
       "  2.5488765239715576,\n",
       "  2.5396885871887207,\n",
       "  2.5851244926452637,\n",
       "  2.5977392196655273,\n",
       "  2.536532163619995,\n",
       "  2.579608201980591,\n",
       "  2.5605571269989014,\n",
       "  2.5656540393829346,\n",
       "  2.5308995246887207,\n",
       "  2.579298496246338,\n",
       "  2.5728118419647217,\n",
       "  2.5474660396575928,\n",
       "  2.549102783203125,\n",
       "  2.5354228019714355,\n",
       "  2.6002025604248047,\n",
       "  2.5859177112579346,\n",
       "  2.593641519546509,\n",
       "  2.629257917404175,\n",
       "  2.5866618156433105,\n",
       "  2.6037042140960693,\n",
       "  2.5362722873687744,\n",
       "  2.5656235218048096,\n",
       "  2.541408061981201,\n",
       "  2.5357820987701416,\n",
       "  2.5217125415802,\n",
       "  2.574960708618164,\n",
       "  2.569946050643921,\n",
       "  2.566058874130249,\n",
       "  2.5431151390075684,\n",
       "  2.520063877105713,\n",
       "  2.583423614501953,\n",
       "  2.532069683074951,\n",
       "  2.521703004837036,\n",
       "  2.548222303390503,\n",
       "  2.5662784576416016,\n",
       "  2.5385665893554688,\n",
       "  2.5336368083953857,\n",
       "  2.642826795578003,\n",
       "  2.532928228378296,\n",
       "  2.544553518295288,\n",
       "  2.5635428428649902,\n",
       "  2.568993091583252,\n",
       "  2.525592088699341,\n",
       "  2.525912046432495,\n",
       "  2.547663688659668,\n",
       "  2.61711049079895,\n",
       "  2.523470640182495,\n",
       "  2.51513409614563,\n",
       "  2.5133228302001953,\n",
       "  2.5258514881134033,\n",
       "  2.5440151691436768,\n",
       "  2.519193172454834,\n",
       "  2.5135576725006104,\n",
       "  2.5182783603668213,\n",
       "  2.529676675796509,\n",
       "  2.559358835220337,\n",
       "  2.5160226821899414,\n",
       "  2.5261032581329346,\n",
       "  2.5406646728515625,\n",
       "  2.516268491744995,\n",
       "  2.553119421005249,\n",
       "  2.525392770767212,\n",
       "  2.504190444946289,\n",
       "  2.4993464946746826,\n",
       "  2.5394794940948486,\n",
       "  2.540405511856079,\n",
       "  2.5561797618865967,\n",
       "  2.523618221282959,\n",
       "  2.5410714149475098,\n",
       "  2.496753215789795,\n",
       "  2.5354983806610107,\n",
       "  2.5016438961029053,\n",
       "  2.526916265487671,\n",
       "  2.5150983333587646,\n",
       "  2.510910987854004,\n",
       "  2.5062437057495117,\n",
       "  2.5333800315856934,\n",
       "  2.5444018840789795,\n",
       "  2.5521655082702637,\n",
       "  2.591104745864868,\n",
       "  2.5302352905273438,\n",
       "  2.517544984817505,\n",
       "  2.5278406143188477,\n",
       "  2.5147643089294434,\n",
       "  2.4934635162353516,\n",
       "  2.5075531005859375,\n",
       "  2.5410828590393066,\n",
       "  2.5129129886627197,\n",
       "  2.5208542346954346,\n",
       "  2.523681163787842,\n",
       "  2.5321402549743652,\n",
       "  2.496731758117676,\n",
       "  2.5095951557159424,\n",
       "  2.581993341445923,\n",
       "  2.5543665885925293,\n",
       "  2.5509426593780518,\n",
       "  2.5726633071899414,\n",
       "  2.5471999645233154,\n",
       "  2.517364501953125,\n",
       "  2.4858574867248535,\n",
       "  2.5060136318206787,\n",
       "  2.579293966293335,\n",
       "  2.49697208404541,\n",
       "  2.5501744747161865,\n",
       "  2.5001213550567627,\n",
       "  2.5083606243133545,\n",
       "  2.542415142059326]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_NoReg = []\n",
    "historyTr_NoReg = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_NoReg()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal_NoReg.append(history['val_loss'])\n",
    "    historyTr_NoReg.append(history['loss'])\n",
    "model=create_model_NoReg()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_NoReg_mean=np.mean(historyVal_NoReg, axis=0)\n",
    "historyTr_NoReg_mean=np.mean(historyTr_NoReg, axis=0)\n",
    "\n",
    "historyVal_NoReg_sd=np.std(historyVal_NoReg, axis=0)\n",
    "historyTr_NoReg_sd=np.std(historyTr_NoReg, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_NoReg_mean)),historyTr_NoReg_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_NoReg_mean)), historyTr_NoReg_mean - historyTr_NoReg_sd, \n",
    "                 historyTr_NoReg_mean + historyTr_NoReg_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_NoReg_mean)), historyVal_NoReg_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_NoReg_mean)), historyVal_NoReg_mean - historyVal_NoReg_sd, \n",
    "                 historyVal_NoReg_mean + historyVal_NoReg_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +2))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No regularization (no alpha) result:\n",
      "MEE on the validation 2.8283557891845703 with standard deviation 0.12234814579802553\n",
      "MEE on the training 2.6282647132873533 with standard deviation 0.039828152082425426\n"
     ]
    }
   ],
   "source": [
    "print(\"No regularization (no alpha) result:\")\n",
    "print(\"MEE on the validation\",historyVal_NoReg_mean[-1],\"with standard deviation\",historyVal_NoReg_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_NoReg_mean[-1],\"with standard deviation\",historyTr_NoReg_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_Low_LR():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.01, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 56.7120 - val_loss: 56.5695\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 56.6329 - val_loss: 56.4508\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 56.5142 - val_loss: 56.3121\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 56.3754 - val_loss: 56.1631\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 56.2264 - val_loss: 56.0086\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 56.0718 - val_loss: 55.8510\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 55.9141 - val_loss: 55.6914\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 55.7544 - val_loss: 55.5302\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 55.5932 - val_loss: 55.3677\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 55.4307 - val_loss: 55.2040\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 55.2668 - val_loss: 55.0389\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 55.1016 - val_loss: 54.8723\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 54.9350 - val_loss: 54.7043\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 54.7669 - val_loss: 54.5347\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 54.5972 - val_loss: 54.3633\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 54.4257 - val_loss: 54.1901\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 54.2524 - val_loss: 54.0149\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 54.0771 - val_loss: 53.8377\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 53.8998 - val_loss: 53.6583\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 53.7202 - val_loss: 53.4766\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 53.5384 - val_loss: 53.2926\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 53.3542 - val_loss: 53.1060\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 53.1675 - val_loss: 52.9169\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 52.9782 - val_loss: 52.7252\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 52.7862 - val_loss: 52.5307\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 52.5915 - val_loss: 52.3333\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 52.3939 - val_loss: 52.1331\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 52.1934 - val_loss: 51.9298\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 51.9898 - val_loss: 51.7235\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 51.7832 - val_loss: 51.5141\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 51.5735 - val_loss: 51.3016\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 51.3606 - val_loss: 51.0858\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.1444 - val_loss: 50.8667\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 50.9250 - val_loss: 50.6444\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 50.7023 - val_loss: 50.4187\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 50.4762 - val_loss: 50.1897\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 50.2468 - val_loss: 49.9574\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 50.0140 - val_loss: 49.7217\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 49.7778 - val_loss: 49.4826\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 49.5383 - val_loss: 49.2402\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 49.2953 - val_loss: 48.9944\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 49.0491 - val_loss: 48.7453\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 48.7995 - val_loss: 48.4929\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 48.5467 - val_loss: 48.2373\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 48.2905 - val_loss: 47.9784\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 48.0311 - val_loss: 47.7163\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 47.7686 - val_loss: 47.4511\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 47.5029 - val_loss: 47.1827\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 47.2341 - val_loss: 46.9114\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 46.9622 - val_loss: 46.6370\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 46.6874 - val_loss: 46.3597\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 46.4097 - val_loss: 46.0796\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 46.1291 - val_loss: 45.7966\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 45.8458 - val_loss: 45.5109\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 45.5597 - val_loss: 45.2226\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 45.2709 - val_loss: 44.9316\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 44.9796 - val_loss: 44.6381\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 44.6857 - val_loss: 44.3421\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 44.3894 - val_loss: 44.0437\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 44.0907 - val_loss: 43.7429\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 43.7897 - val_loss: 43.4399\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 43.4864 - val_loss: 43.1346\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 43.1809 - val_loss: 42.8272\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 42.8732 - val_loss: 42.5176\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 42.5634 - val_loss: 42.2060\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 42.2516 - val_loss: 41.8924\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 41.9378 - val_loss: 41.5767\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 41.6220 - val_loss: 41.2592\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 41.3043 - val_loss: 40.9398\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 40.9848 - val_loss: 40.6186\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 40.6635 - val_loss: 40.2956\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 40.3403 - val_loss: 39.9709\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 40.0155 - val_loss: 39.6445\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 39.6890 - val_loss: 39.3165\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 39.3609 - val_loss: 38.9869\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 39.0312 - val_loss: 38.6558\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 38.7000 - val_loss: 38.3233\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 38.3674 - val_loss: 37.9894\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 38.0334 - val_loss: 37.6542\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 37.6980 - val_loss: 37.3177\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 37.3614 - val_loss: 36.9801\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 37.0237 - val_loss: 36.6415\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 36.6848 - val_loss: 36.3019\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 36.3451 - val_loss: 35.9615\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 36.0044 - val_loss: 35.6204\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 35.6630 - val_loss: 35.2788\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 35.3211 - val_loss: 34.9367\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 34.9786 - val_loss: 34.5944\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 34.6358 - val_loss: 34.2520\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 34.2929 - val_loss: 33.9097\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 33.9500 - val_loss: 33.5677\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 33.6073 - val_loss: 33.2262\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 33.2650 - val_loss: 32.8854\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 32.9232 - val_loss: 32.5455\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 32.5823 - val_loss: 32.2068\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 32.2424 - val_loss: 31.8694\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 31.9038 - val_loss: 31.5338\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 31.5667 - val_loss: 31.2000\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 31.2313 - val_loss: 30.8683\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 30.8979 - val_loss: 30.5390\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 30.5668 - val_loss: 30.2123\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 30.2382 - val_loss: 29.8885\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 29.9123 - val_loss: 29.5678\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 29.5895 - val_loss: 29.2503\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 29.2699 - val_loss: 28.9364\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 28.9539 - val_loss: 28.6262\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 28.6416 - val_loss: 28.3200\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 28.3333 - val_loss: 28.0178\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 28.0293 - val_loss: 27.7199\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 27.7297 - val_loss: 27.4265\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 27.4348 - val_loss: 27.1377\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 27.1447 - val_loss: 26.8537\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 26.8597 - val_loss: 26.5746\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 26.5800 - val_loss: 26.3006\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 26.3057 - val_loss: 26.0318\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 26.0370 - val_loss: 25.7684\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 25.7742 - val_loss: 25.5106\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 25.5174 - val_loss: 25.2585\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 25.2667 - val_loss: 25.0123\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 25.0222 - val_loss: 24.7721\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 24.7838 - val_loss: 24.5381\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 24.5518 - val_loss: 24.3105\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 24.3259 - val_loss: 24.0894\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 24.1064 - val_loss: 23.8753\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 23.8935 - val_loss: 23.6693\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 23.6872 - val_loss: 23.4722\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 23.4880 - val_loss: 23.2835\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 23.2967 - val_loss: 23.1036\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 23.1139 - val_loss: 22.9308\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 22.9385 - val_loss: 22.7640\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 22.7694 - val_loss: 22.6026\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 22.6061 - val_loss: 22.4463\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 22.4475 - val_loss: 22.2946\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 22.2932 - val_loss: 22.1472\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 22.1431 - val_loss: 22.0039\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 21.9969 - val_loss: 21.8641\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 21.8542 - val_loss: 21.7276\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 21.7149 - val_loss: 21.5940\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 21.5788 - val_loss: 21.4633\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 21.4457 - val_loss: 21.3352\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 21.3155 - val_loss: 21.2097\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 21.1881 - val_loss: 21.0866\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 21.0634 - val_loss: 20.9658\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 20.9412 - val_loss: 20.8474\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 20.8214 - val_loss: 20.7311\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 20.7040 - val_loss: 20.6170\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 20.5889 - val_loss: 20.5051\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 20.4759 - val_loss: 20.3951\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 20.3650 - val_loss: 20.2871\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 20.2562 - val_loss: 20.1810\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 20.1492 - val_loss: 20.0768\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 20.0442 - val_loss: 19.9743\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 19.9409 - val_loss: 19.8736\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 19.8394 - val_loss: 19.7746\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 19.7396 - val_loss: 19.6772\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 19.6415 - val_loss: 19.5814\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 19.5449 - val_loss: 19.4871\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 19.4499 - val_loss: 19.3943\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 19.3564 - val_loss: 19.3030\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 19.2643 - val_loss: 19.2131\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 19.1737 - val_loss: 19.1246\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 19.0845 - val_loss: 19.0374\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 18.9966 - val_loss: 18.9515\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 18.9100 - val_loss: 18.8668\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 18.8247 - val_loss: 18.7834\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18.7407 - val_loss: 18.7012\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18.6579 - val_loss: 18.6201\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.5762 - val_loss: 18.5401\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 18.4957 - val_loss: 18.4612\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 18.4163 - val_loss: 18.3834\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.3380 - val_loss: 18.3065\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 18.2607 - val_loss: 18.2306\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 18.1844 - val_loss: 18.1556\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.1091 - val_loss: 18.0815\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 18.0348 - val_loss: 18.0083\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17.9613 - val_loss: 17.9358\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 17.8887 - val_loss: 17.8642\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 17.8169 - val_loss: 17.7932\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 17.7460 - val_loss: 17.7230\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.6758 - val_loss: 17.6534\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17.6063 - val_loss: 17.5844\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17.5375 - val_loss: 17.5160\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17.4694 - val_loss: 17.4481\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 17.4018 - val_loss: 17.3808\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.3349 - val_loss: 17.3139\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 17.2685 - val_loss: 17.2475\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 17.2026 - val_loss: 17.1814\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 17.1371 - val_loss: 17.1158\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17.0721 - val_loss: 17.0504\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 17.0075 - val_loss: 16.9854\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 16.9432 - val_loss: 16.9206\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 16.8792 - val_loss: 16.8560\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 16.8155 - val_loss: 16.7916\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 16.7521 - val_loss: 16.7274\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 16.6889 - val_loss: 16.6633\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 16.6258 - val_loss: 16.5993\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.5629 - val_loss: 16.5353\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 16.5001 - val_loss: 16.4714\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.4374 - val_loss: 16.4075\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.3747 - val_loss: 16.3435\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 16.3120 - val_loss: 16.2795\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 16.2493 - val_loss: 16.2154\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 16.1865 - val_loss: 16.1512\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 16.1237 - val_loss: 16.0868\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 16.0607 - val_loss: 16.0222\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 15.9975 - val_loss: 15.9574\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.9342 - val_loss: 15.8923\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 15.8706 - val_loss: 15.8269\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.8068 - val_loss: 15.7613\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 15.7427 - val_loss: 15.6952\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 15.6783 - val_loss: 15.6288\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 15.6135 - val_loss: 15.5620\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.5484 - val_loss: 15.4948\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.4828 - val_loss: 15.4270\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 15.4167 - val_loss: 15.3588\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.3502 - val_loss: 15.2899\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.2831 - val_loss: 15.2205\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.2154 - val_loss: 15.1505\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 15.1472 - val_loss: 15.0799\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.0783 - val_loss: 15.0085\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 15.0087 - val_loss: 14.9365\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 14.9385 - val_loss: 14.8637\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 14.8675 - val_loss: 14.7901\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14.7957 - val_loss: 14.7158\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 14.7232 - val_loss: 14.6406\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 14.6499 - val_loss: 14.5646\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 14.5757 - val_loss: 14.4878\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 14.5007 - val_loss: 14.4101\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.4249 - val_loss: 14.3315\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 14.3482 - val_loss: 14.2521\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 14.2707 - val_loss: 14.1718\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 14.1923 - val_loss: 14.0907\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 14.1131 - val_loss: 14.0088\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.0331 - val_loss: 13.9260\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.9523 - val_loss: 13.8425\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 13.8708 - val_loss: 13.7582\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 13.7886 - val_loss: 13.6733\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 13.7058 - val_loss: 13.5877\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 13.6224 - val_loss: 13.5015\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 13.5385 - val_loss: 13.4148\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13.4540 - val_loss: 13.3275\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.3692 - val_loss: 13.2399\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 13.2840 - val_loss: 13.1518\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 13.1984 - val_loss: 13.0634\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 13.1126 - val_loss: 12.9747\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 13.0265 - val_loss: 12.8859\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 12.9403 - val_loss: 12.7968\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.8540 - val_loss: 12.7077\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 12.7677 - val_loss: 12.6185\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 12.6813 - val_loss: 12.5293\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 12.5950 - val_loss: 12.4402\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.5088 - val_loss: 12.3513\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 12.4228 - val_loss: 12.2626\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 12.3370 - val_loss: 12.1742\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 12.2516 - val_loss: 12.0863\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 12.1665 - val_loss: 11.9989\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 12.0819 - val_loss: 11.9120\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 11.9978 - val_loss: 11.8258\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.9143 - val_loss: 11.7405\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.8315 - val_loss: 11.6559\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 11.7493 - val_loss: 11.5722\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 11.6679 - val_loss: 11.4894\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.5873 - val_loss: 11.4076\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 11.5074 - val_loss: 11.3267\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.4284 - val_loss: 11.2470\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 11.3503 - val_loss: 11.1685\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.2732 - val_loss: 11.0913\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.1972 - val_loss: 11.0155\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.1223 - val_loss: 10.9410\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.0485 - val_loss: 10.8679\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.9759 - val_loss: 10.7963\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 10.9045 - val_loss: 10.7261\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.8343 - val_loss: 10.6572\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 10.7654 - val_loss: 10.5897\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 10.6977 - val_loss: 10.5235\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 10.6314 - val_loss: 10.4586\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.5664 - val_loss: 10.3951\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 10.5028 - val_loss: 10.3330\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.4405 - val_loss: 10.2723\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 10.3795 - val_loss: 10.2130\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.3198 - val_loss: 10.1550\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.2615 - val_loss: 10.0984\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.2045 - val_loss: 10.0432\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 10.1488 - val_loss: 9.9893\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.0944 - val_loss: 9.9368\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.0413 - val_loss: 9.8857\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.9894 - val_loss: 9.8358\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.9388 - val_loss: 9.7873\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.8894 - val_loss: 9.7401\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 9.8413 - val_loss: 9.6941\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.7944 - val_loss: 9.6494\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.7487 - val_loss: 9.6060\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 9.7042 - val_loss: 9.5638\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.6608 - val_loss: 9.5228\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.6186 - val_loss: 9.4830\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.5776 - val_loss: 9.4444\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.5377 - val_loss: 9.4070\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 9.4989 - val_loss: 9.3707\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.4612 - val_loss: 9.3356\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 9.4246 - val_loss: 9.3015\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.3890 - val_loss: 9.2685\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.3544 - val_loss: 9.2365\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.3208 - val_loss: 9.2055\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 9.2882 - val_loss: 9.1755\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.2566 - val_loss: 9.1464\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.2259 - val_loss: 9.1183\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 9.1960 - val_loss: 9.0910\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.1671 - val_loss: 9.0646\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 9.1390 - val_loss: 9.0390\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.1118 - val_loss: 9.0143\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.0854 - val_loss: 8.9904\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.0598 - val_loss: 8.9672\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.0349 - val_loss: 8.9448\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 9.0108 - val_loss: 8.9232\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.9875 - val_loss: 8.9022\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.9649 - val_loss: 8.8820\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.9430 - val_loss: 8.8624\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 8.9217 - val_loss: 8.8434\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.9011 - val_loss: 8.8251\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.8812 - val_loss: 8.8074\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.8619 - val_loss: 8.7903\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.8432 - val_loss: 8.7738\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.8251 - val_loss: 8.7578\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.8075 - val_loss: 8.7423\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.7905 - val_loss: 8.7274\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.7741 - val_loss: 8.7129\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.7581 - val_loss: 8.6989\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.7427 - val_loss: 8.6854\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.7278 - val_loss: 8.6724\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.7133 - val_loss: 8.6598\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.6993 - val_loss: 8.6478\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.6857 - val_loss: 8.6362\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.6726 - val_loss: 8.6250\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.6598 - val_loss: 8.6141\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.6475 - val_loss: 8.6036\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.6355 - val_loss: 8.5934\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.6240 - val_loss: 8.5836\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.6127 - val_loss: 8.5741\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.6019 - val_loss: 8.5649\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.5913 - val_loss: 8.5560\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.5811 - val_loss: 8.5473\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.5711 - val_loss: 8.5390\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.5615 - val_loss: 8.5309\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.5521 - val_loss: 8.5231\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.5430 - val_loss: 8.5155\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.5341 - val_loss: 8.5081\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.5255 - val_loss: 8.5010\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.5171 - val_loss: 8.4940\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.5089 - val_loss: 8.4873\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.5009 - val_loss: 8.4807\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4931 - val_loss: 8.4744\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.4855 - val_loss: 8.4681\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.4781 - val_loss: 8.4621\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.4708 - val_loss: 8.4562\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.4637 - val_loss: 8.4504\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.4567 - val_loss: 8.4447\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.4499 - val_loss: 8.4392\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4432 - val_loss: 8.4337\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.4366 - val_loss: 8.4284\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.4302 - val_loss: 8.4231\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.4239 - val_loss: 8.4180\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.4176 - val_loss: 8.4130\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.4115 - val_loss: 8.4082\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.4055 - val_loss: 8.4034\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.3997 - val_loss: 8.3988\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.3939 - val_loss: 8.3942\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.3883 - val_loss: 8.3896\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3827 - val_loss: 8.3852\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.3772 - val_loss: 8.3807\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.3718 - val_loss: 8.3764\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.3665 - val_loss: 8.3720\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3612 - val_loss: 8.3677\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.3560 - val_loss: 8.3635\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3509 - val_loss: 8.3592\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.3458 - val_loss: 8.3551\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3409 - val_loss: 8.3509\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.3360 - val_loss: 8.3468\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3312 - val_loss: 8.3427\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.3264 - val_loss: 8.3386\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.3217 - val_loss: 8.3346\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3170 - val_loss: 8.3306\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.3124 - val_loss: 8.3266\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.3078 - val_loss: 8.3226\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.3032 - val_loss: 8.3186\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.2986 - val_loss: 8.3147\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2941 - val_loss: 8.3108\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.2896 - val_loss: 8.3069\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.2852 - val_loss: 8.3030\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.2808 - val_loss: 8.2991\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.2764 - val_loss: 8.2953\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.2721 - val_loss: 8.2914\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2678 - val_loss: 8.2876\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.2636 - val_loss: 8.2838\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.2594 - val_loss: 8.2800\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.2552 - val_loss: 8.2762\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.2510 - val_loss: 8.2725\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.2469 - val_loss: 8.2687\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.2428 - val_loss: 8.2650\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.2387 - val_loss: 8.2613\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.2346 - val_loss: 8.2575\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.2306 - val_loss: 8.2538\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.2266 - val_loss: 8.2501\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.2226 - val_loss: 8.2465\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.2186 - val_loss: 8.2428\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2146 - val_loss: 8.2391\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2107 - val_loss: 8.2355\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.2067 - val_loss: 8.2318\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.2028 - val_loss: 8.2282\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.1989 - val_loss: 8.2246\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1950 - val_loss: 8.2210\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.1911 - val_loss: 8.2174\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.1872 - val_loss: 8.2138\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.1834 - val_loss: 8.2102\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.1795 - val_loss: 8.2066\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1757 - val_loss: 8.2030\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.1719 - val_loss: 8.1995\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1681 - val_loss: 8.1959\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.1643 - val_loss: 8.1924\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1606 - val_loss: 8.1889\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1568 - val_loss: 8.1853\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.1531 - val_loss: 8.1818\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.1494 - val_loss: 8.1783\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.1457 - val_loss: 8.1748\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.1420 - val_loss: 8.1713\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.1383 - val_loss: 8.1678\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1346 - val_loss: 8.1644\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.1309 - val_loss: 8.1609\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.1273 - val_loss: 8.1574\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.1236 - val_loss: 8.1540\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.1200 - val_loss: 8.1505\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1163 - val_loss: 8.1471\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.1127 - val_loss: 8.1436\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.1091 - val_loss: 8.1402\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.1055 - val_loss: 8.1368\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1018 - val_loss: 8.1333\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.0982 - val_loss: 8.1299\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.0946 - val_loss: 8.1265\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.0911 - val_loss: 8.1231\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0875 - val_loss: 8.1196\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.0839 - val_loss: 8.1162\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.0803 - val_loss: 8.1128\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0768 - val_loss: 8.1094\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.0732 - val_loss: 8.1060\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.0696 - val_loss: 8.1026\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0661 - val_loss: 8.0992\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.0625 - val_loss: 8.0958\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.0590 - val_loss: 8.0924\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.0555 - val_loss: 8.0890\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.0519 - val_loss: 8.0856\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0484 - val_loss: 8.0822\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.0448 - val_loss: 8.0788\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.0413 - val_loss: 8.0754\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.0377 - val_loss: 8.0720\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.0342 - val_loss: 8.0686\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.0307 - val_loss: 8.0651\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0271 - val_loss: 8.0617\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0236 - val_loss: 8.0583\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0200 - val_loss: 8.0549\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.0165 - val_loss: 8.0515\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.0130 - val_loss: 8.0481\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.0094 - val_loss: 8.0446\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.0059 - val_loss: 8.0412\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0023 - val_loss: 8.0378\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9988 - val_loss: 8.0343\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.9952 - val_loss: 8.0309\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.9917 - val_loss: 8.0275\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.9881 - val_loss: 8.0240\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.9846 - val_loss: 8.0206\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.9810 - val_loss: 8.0171\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.9775 - val_loss: 8.0136\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9739 - val_loss: 8.0102\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9703 - val_loss: 8.0067\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.9668 - val_loss: 8.0032\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.9632 - val_loss: 7.9997\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.9596 - val_loss: 7.9963\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.9560 - val_loss: 7.9928\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9525 - val_loss: 7.9893\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.9489 - val_loss: 7.9858\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.9453 - val_loss: 7.9823\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.9417 - val_loss: 7.9787\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9381 - val_loss: 7.9752\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.9345 - val_loss: 7.9717\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.9309 - val_loss: 7.9682\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.9273 - val_loss: 7.9646\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.9237 - val_loss: 7.9611\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.9200 - val_loss: 7.9575\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9164 - val_loss: 7.9540\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.9128 - val_loss: 7.9504\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.9091 - val_loss: 7.9468\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9055 - val_loss: 7.9432\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.9018 - val_loss: 7.9396\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8982 - val_loss: 7.9361\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.8945 - val_loss: 7.9325\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.8909 - val_loss: 7.9288\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.8872 - val_loss: 7.9252\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.8835 - val_loss: 7.9216\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.8799 - val_loss: 7.9180\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8762 - val_loss: 7.9143\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8725 - val_loss: 7.9107\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.8688 - val_loss: 7.9070\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.8651 - val_loss: 7.9034\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.8614 - val_loss: 7.8997\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8576 - val_loss: 7.8960\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.8539 - val_loss: 7.8924\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.8502 - val_loss: 7.8887\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.8465 - val_loss: 7.8850\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8427 - val_loss: 7.8813\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.8390 - val_loss: 7.8775\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.8352 - val_loss: 7.8738\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.8315 - val_loss: 7.8701\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.8277 - val_loss: 7.8664\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.8239 - val_loss: 7.8626\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.8201 - val_loss: 7.8589\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.8164 - val_loss: 7.8551\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.8126 - val_loss: 7.8514\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.8088 - val_loss: 7.8476\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8050 - val_loss: 7.8438\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.8011 - val_loss: 7.8400\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.7973 - val_loss: 7.8362\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.7935 - val_loss: 7.8324\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.7897 - val_loss: 7.8286\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.7858 - val_loss: 7.8248\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.7820 - val_loss: 7.8210\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.7781 - val_loss: 7.8171\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.7743 - val_loss: 7.8133\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7704 - val_loss: 7.8094\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.7665 - val_loss: 7.8056\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.7627 - val_loss: 7.8017\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.7588 - val_loss: 7.7978\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.7549 - val_loss: 7.7940\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.7510 - val_loss: 7.7901\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.7471 - val_loss: 7.7862\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.7432 - val_loss: 7.7823\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.7393 - val_loss: 7.7784\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7353 - val_loss: 7.7745\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.7314 - val_loss: 7.7705\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.7275 - val_loss: 7.7666\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.7235 - val_loss: 7.7627\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.7196 - val_loss: 7.7587\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7156 - val_loss: 7.7548\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.7117 - val_loss: 7.7508\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.7077 - val_loss: 7.7469\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.7037 - val_loss: 7.7429\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.6997 - val_loss: 7.7389\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.6958 - val_loss: 7.7349\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6918 - val_loss: 7.7309\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6878 - val_loss: 7.7269\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.6838 - val_loss: 7.7229\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.6797 - val_loss: 7.7189\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6757 - val_loss: 7.7149\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.6717 - val_loss: 7.7109\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.6677 - val_loss: 7.7068\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.6636 - val_loss: 7.7028\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.6596 - val_loss: 7.6988\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.6555 - val_loss: 7.6947\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.6515 - val_loss: 7.6907\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.6474 - val_loss: 7.6866\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.6434 - val_loss: 7.6825\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.6393 - val_loss: 7.6785\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.6352 - val_loss: 7.6744\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 7.6311 - val_loss: 7.6703\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6270 - val_loss: 7.6662\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.6229 - val_loss: 7.6621\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.6188 - val_loss: 7.6580\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.6147 - val_loss: 7.6539\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.6106 - val_loss: 7.6498\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6065 - val_loss: 7.6457\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6024 - val_loss: 7.6416\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.5982 - val_loss: 7.6374\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.5941 - val_loss: 7.6333\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.5900 - val_loss: 7.6292\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.5858 - val_loss: 7.6250\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5817 - val_loss: 7.6209\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5775 - val_loss: 7.6167\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.5733 - val_loss: 7.6126\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.5692 - val_loss: 7.6084\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.5650 - val_loss: 7.6042\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.5608 - val_loss: 7.6001\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.5566 - val_loss: 7.5959\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.5525 - val_loss: 7.5917\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5483 - val_loss: 7.5875\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.5441 - val_loss: 7.5834\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5399 - val_loss: 7.5792\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.5356 - val_loss: 7.5750\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.5314 - val_loss: 7.5708\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.5272 - val_loss: 7.5666\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.5230 - val_loss: 7.5624\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 7.5188 - val_loss: 7.5582\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.5145 - val_loss: 7.5540\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.5103 - val_loss: 7.5498\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.5061 - val_loss: 7.5455\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.5018 - val_loss: 7.5413\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4976 - val_loss: 7.5371\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.4933 - val_loss: 7.5329\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.4890 - val_loss: 7.5286\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.4848 - val_loss: 7.5244\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4805 - val_loss: 7.5201\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.4762 - val_loss: 7.5159\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4720 - val_loss: 7.5117\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.4677 - val_loss: 7.5074\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 56.0535 - val_loss: 55.5977\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 55.9731 - val_loss: 55.4771\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 55.8525 - val_loss: 55.3362\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 55.7117 - val_loss: 55.1851\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 55.5607 - val_loss: 55.0287\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 55.4045 - val_loss: 54.8694\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 55.2454 - val_loss: 54.7085\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 55.0847 - val_loss: 54.5464\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 54.9228 - val_loss: 54.3834\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 54.7601 - val_loss: 54.2196\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 54.5965 - val_loss: 54.0549\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 54.4321 - val_loss: 53.8893\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 54.2669 - val_loss: 53.7228\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 54.1007 - val_loss: 53.5552\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 53.9335 - val_loss: 53.3865\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 53.7653 - val_loss: 53.2167\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 53.5959 - val_loss: 53.0456\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 53.4252 - val_loss: 52.8732\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 53.2533 - val_loss: 52.6994\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 53.0800 - val_loss: 52.5241\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 52.9052 - val_loss: 52.3474\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 52.7290 - val_loss: 52.1690\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 52.5511 - val_loss: 51.9889\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 52.3717 - val_loss: 51.8072\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 52.1905 - val_loss: 51.6236\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 52.0075 - val_loss: 51.4383\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 51.8228 - val_loss: 51.2510\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 51.6361 - val_loss: 51.0617\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 51.4475 - val_loss: 50.8705\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 51.2569 - val_loss: 50.6772\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 51.0643 - val_loss: 50.4818\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 50.8695 - val_loss: 50.2842\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 50.6726 - val_loss: 50.0845\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 50.4735 - val_loss: 49.8825\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 50.2722 - val_loss: 49.6782\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 50.0686 - val_loss: 49.4715\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 49.8627 - val_loss: 49.2626\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 49.6545 - val_loss: 49.0512\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 49.4438 - val_loss: 48.8374\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 49.2307 - val_loss: 48.6212\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 49.0152 - val_loss: 48.4025\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 48.7972 - val_loss: 48.1813\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 48.5767 - val_loss: 47.9576\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 48.3537 - val_loss: 47.7314\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 48.1282 - val_loss: 47.5026\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 47.9000 - val_loss: 47.2712\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 47.6693 - val_loss: 47.0372\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 47.4360 - val_loss: 46.8006\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 47.2001 - val_loss: 46.5613\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 46.9615 - val_loss: 46.3195\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 46.7203 - val_loss: 46.0749\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 46.4764 - val_loss: 45.8276\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 46.2298 - val_loss: 45.5777\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 45.9804 - val_loss: 45.3249\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 45.7283 - val_loss: 45.0695\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 45.4734 - val_loss: 44.8112\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 45.2157 - val_loss: 44.5501\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 44.9552 - val_loss: 44.2861\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 44.6918 - val_loss: 44.0193\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 44.4255 - val_loss: 43.7496\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 44.1564 - val_loss: 43.4769\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 43.8843 - val_loss: 43.2014\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 43.6092 - val_loss: 42.9230\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 43.3313 - val_loss: 42.6416\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43.0504 - val_loss: 42.3574\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 42.7666 - val_loss: 42.0703\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 42.4799 - val_loss: 41.7803\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 42.1904 - val_loss: 41.4876\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 41.8981 - val_loss: 41.1922\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 41.6031 - val_loss: 40.8941\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 41.3055 - val_loss: 40.5935\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 41.0052 - val_loss: 40.2904\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 40.7025 - val_loss: 39.9850\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 40.3974 - val_loss: 39.6773\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 40.0900 - val_loss: 39.3675\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 39.7805 - val_loss: 39.0556\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 39.4689 - val_loss: 38.7418\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 39.1554 - val_loss: 38.4262\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 38.8402 - val_loss: 38.1090\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 38.5232 - val_loss: 37.7902\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 38.2047 - val_loss: 37.4699\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 37.8847 - val_loss: 37.1484\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 37.5635 - val_loss: 36.8257\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 37.2410 - val_loss: 36.5019\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 36.9175 - val_loss: 36.1772\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36.5930 - val_loss: 35.8517\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 36.2678 - val_loss: 35.5256\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 35.9418 - val_loss: 35.1989\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 35.6154 - val_loss: 34.8718\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 35.2885 - val_loss: 34.5445\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 34.9614 - val_loss: 34.2172\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 34.6342 - val_loss: 33.8899\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 34.3071 - val_loss: 33.5628\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 33.9802 - val_loss: 33.2362\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 33.6538 - val_loss: 32.9102\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 33.3279 - val_loss: 32.5850\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 33.0028 - val_loss: 32.2607\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 32.6786 - val_loss: 31.9377\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 32.3556 - val_loss: 31.6160\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 32.0339 - val_loss: 31.2959\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 31.7137 - val_loss: 30.9775\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 31.3952 - val_loss: 30.6612\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 31.0787 - val_loss: 30.3470\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 30.7642 - val_loss: 30.0352\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 30.4520 - val_loss: 29.7260\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 30.1423 - val_loss: 29.4196\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 29.8352 - val_loss: 29.1162\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 29.5309 - val_loss: 28.8159\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 29.2297 - val_loss: 28.5190\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 28.9317 - val_loss: 28.2256\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 28.6370 - val_loss: 27.9359\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 28.3458 - val_loss: 27.6501\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 28.0583 - val_loss: 27.3683\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 27.7746 - val_loss: 27.0907\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 27.4950 - val_loss: 26.8175\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 27.2194 - val_loss: 26.5487\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 26.9482 - val_loss: 26.2846\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 26.6814 - val_loss: 26.0252\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 26.4193 - val_loss: 25.7707\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 26.1619 - val_loss: 25.5212\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 25.9094 - val_loss: 25.2768\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 25.6619 - val_loss: 25.0375\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 25.4195 - val_loss: 24.8034\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 25.1825 - val_loss: 24.5746\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 24.9509 - val_loss: 24.3510\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 24.7248 - val_loss: 24.1327\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 24.5044 - val_loss: 23.9198\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 24.2898 - val_loss: 23.7123\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 24.0813 - val_loss: 23.5104\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 23.8791 - val_loss: 23.3146\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 23.6835 - val_loss: 23.1252\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 23.4946 - val_loss: 22.9430\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 23.3130 - val_loss: 22.7678\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 23.1388 - val_loss: 22.5993\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 22.9724 - val_loss: 22.4367\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 22.8123 - val_loss: 22.2796\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 22.6583 - val_loss: 22.1283\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 22.5108 - val_loss: 21.9824\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 22.3682 - val_loss: 21.8412\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 22.2299 - val_loss: 21.7044\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 22.0956 - val_loss: 21.5715\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 21.9653 - val_loss: 21.4423\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 21.8384 - val_loss: 21.3165\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 21.7145 - val_loss: 21.1940\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 21.5936 - val_loss: 21.0746\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 21.4753 - val_loss: 20.9582\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 21.3596 - val_loss: 20.8445\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 21.2464 - val_loss: 20.7334\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 21.1356 - val_loss: 20.6247\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 21.0272 - val_loss: 20.5185\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 20.9210 - val_loss: 20.4145\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 20.8171 - val_loss: 20.3127\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 20.7153 - val_loss: 20.2130\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 20.6156 - val_loss: 20.1154\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 20.5178 - val_loss: 20.0196\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 20.4220 - val_loss: 19.9258\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 20.3280 - val_loss: 19.8338\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 20.2358 - val_loss: 19.7435\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 20.1453 - val_loss: 19.6550\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 20.0565 - val_loss: 19.5681\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 19.9693 - val_loss: 19.4829\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 19.8836 - val_loss: 19.3992\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 19.7994 - val_loss: 19.3170\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 19.7167 - val_loss: 19.2363\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 19.6354 - val_loss: 19.1570\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 19.5554 - val_loss: 19.0792\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 19.4768 - val_loss: 19.0026\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 19.3995 - val_loss: 18.9274\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 19.3235 - val_loss: 18.8535\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 19.2487 - val_loss: 18.7808\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 19.1751 - val_loss: 18.7093\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 19.1027 - val_loss: 18.6389\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 19.0314 - val_loss: 18.5696\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 18.9612 - val_loss: 18.5014\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 18.8921 - val_loss: 18.4343\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 18.8240 - val_loss: 18.3681\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 18.7569 - val_loss: 18.3029\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 18.6907 - val_loss: 18.2386\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 18.6255 - val_loss: 18.1752\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 18.5613 - val_loss: 18.1127\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 18.4978 - val_loss: 18.0510\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 18.4352 - val_loss: 17.9901\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 18.3734 - val_loss: 17.9299\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 18.3123 - val_loss: 17.8705\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.2520 - val_loss: 17.8117\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 18.1924 - val_loss: 17.7536\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 18.1334 - val_loss: 17.6962\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 18.0751 - val_loss: 17.6393\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 18.0173 - val_loss: 17.5829\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 17.9601 - val_loss: 17.5271\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17.9034 - val_loss: 17.4718\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 17.8471 - val_loss: 17.4169\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17.7914 - val_loss: 17.3624\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17.7360 - val_loss: 17.3083\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 17.6810 - val_loss: 17.2545\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 17.6263 - val_loss: 17.2010\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 17.5718 - val_loss: 17.1477\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17.5177 - val_loss: 17.0947\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 17.4637 - val_loss: 17.0418\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 17.4099 - val_loss: 16.9890\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 17.3561 - val_loss: 16.9364\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 17.3025 - val_loss: 16.8837\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17.2488 - val_loss: 16.8311\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17.1952 - val_loss: 16.7784\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 17.1414 - val_loss: 16.7256\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 17.0876 - val_loss: 16.6726\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.0336 - val_loss: 16.6195\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 16.9793 - val_loss: 16.5662\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.9249 - val_loss: 16.5126\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 16.8701 - val_loss: 16.4587\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.8150 - val_loss: 16.4045\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 16.7596 - val_loss: 16.3499\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.7037 - val_loss: 16.2949\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.6475 - val_loss: 16.2395\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.5907 - val_loss: 16.1836\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.5335 - val_loss: 16.1272\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.4757 - val_loss: 16.0704\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.4174 - val_loss: 16.0130\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 16.3585 - val_loss: 15.9551\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 16.2990 - val_loss: 15.8966\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 16.2390 - val_loss: 15.8376\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.1783 - val_loss: 15.7781\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.1170 - val_loss: 15.7179\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.0551 - val_loss: 15.6572\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15.9926 - val_loss: 15.5959\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 15.9294 - val_loss: 15.5341\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 15.8656 - val_loss: 15.4716\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 15.8012 - val_loss: 15.4087\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 15.7362 - val_loss: 15.3451\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 15.6705 - val_loss: 15.2810\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 15.6043 - val_loss: 15.2164\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 15.5374 - val_loss: 15.1512\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 15.4699 - val_loss: 15.0855\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 15.4019 - val_loss: 15.0193\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.3333 - val_loss: 14.9526\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.2642 - val_loss: 14.8854\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.1945 - val_loss: 14.8178\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15.1243 - val_loss: 14.7496\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.0536 - val_loss: 14.6810\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 14.9824 - val_loss: 14.6120\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.9108 - val_loss: 14.5425\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 14.8386 - val_loss: 14.4725\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 14.7659 - val_loss: 14.4020\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 14.6927 - val_loss: 14.3310\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 14.6190 - val_loss: 14.2595\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 14.5448 - val_loss: 14.1875\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 14.4700 - val_loss: 14.1149\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 14.3946 - val_loss: 14.0416\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 14.3186 - val_loss: 13.9678\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 14.2419 - val_loss: 13.8932\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 14.1646 - val_loss: 13.8180\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 14.0865 - val_loss: 13.7420\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 14.0077 - val_loss: 13.6652\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 13.9281 - val_loss: 13.5876\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 13.8477 - val_loss: 13.5092\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.7664 - val_loss: 13.4299\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.6843 - val_loss: 13.3498\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.6014 - val_loss: 13.2689\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 13.5175 - val_loss: 13.1872\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 13.4329 - val_loss: 13.1047\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 13.3474 - val_loss: 13.0214\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.2611 - val_loss: 12.9374\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.1740 - val_loss: 12.8528\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 13.0862 - val_loss: 12.7677\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 12.9978 - val_loss: 12.6821\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 12.9088 - val_loss: 12.5962\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 12.8193 - val_loss: 12.5103\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 12.7294 - val_loss: 12.4244\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 12.6391 - val_loss: 12.3385\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 12.5487 - val_loss: 12.2527\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.4581 - val_loss: 12.1673\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 12.3676 - val_loss: 12.0826\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.2771 - val_loss: 11.9982\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 12.1869 - val_loss: 11.9142\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.0970 - val_loss: 11.8307\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 12.0077 - val_loss: 11.7477\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.9189 - val_loss: 11.6652\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.8308 - val_loss: 11.5834\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.7434 - val_loss: 11.5023\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.6567 - val_loss: 11.4220\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.5708 - val_loss: 11.3425\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.4859 - val_loss: 11.2640\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.4022 - val_loss: 11.1866\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 11.3198 - val_loss: 11.1103\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 11.2386 - val_loss: 11.0353\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 11.1587 - val_loss: 10.9616\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 11.0802 - val_loss: 10.8892\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.0030 - val_loss: 10.8178\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.9273 - val_loss: 10.7476\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.8531 - val_loss: 10.6783\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.7804 - val_loss: 10.6102\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.7093 - val_loss: 10.5430\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.6397 - val_loss: 10.4769\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.5718 - val_loss: 10.4119\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 10.5054 - val_loss: 10.3480\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.4407 - val_loss: 10.2851\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 10.3775 - val_loss: 10.2234\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 10.3158 - val_loss: 10.1629\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 10.2555 - val_loss: 10.1036\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10.1968 - val_loss: 10.0454\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 10.1395 - val_loss: 9.9885\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 10.0837 - val_loss: 9.9328\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 10.0292 - val_loss: 9.8784\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.9762 - val_loss: 9.8252\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.9246 - val_loss: 9.7732\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.8744 - val_loss: 9.7225\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.8256 - val_loss: 9.6730\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 9.7781 - val_loss: 9.6248\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.7320 - val_loss: 9.5778\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.6871 - val_loss: 9.5320\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.6436 - val_loss: 9.4875\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.6013 - val_loss: 9.4441\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 9.5603 - val_loss: 9.4019\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.5205 - val_loss: 9.3608\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.4819 - val_loss: 9.3208\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 9.4445 - val_loss: 9.2820\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.4083 - val_loss: 9.2442\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 9.3731 - val_loss: 9.2075\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.3391 - val_loss: 9.1719\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 9.3061 - val_loss: 9.1372\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.2741 - val_loss: 9.1036\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.2430 - val_loss: 9.0709\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.2130 - val_loss: 9.0392\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.1838 - val_loss: 9.0084\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 9.1556 - val_loss: 8.9786\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.1283 - val_loss: 8.9496\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 9.1018 - val_loss: 8.9214\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.0761 - val_loss: 8.8942\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.0513 - val_loss: 8.8677\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.0272 - val_loss: 8.8421\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.0039 - val_loss: 8.8172\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.9812 - val_loss: 8.7931\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.9593 - val_loss: 8.7697\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.9381 - val_loss: 8.7471\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.9175 - val_loss: 8.7251\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.8976 - val_loss: 8.7038\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.8783 - val_loss: 8.6832\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.8596 - val_loss: 8.6632\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.8414 - val_loss: 8.6438\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.8239 - val_loss: 8.6250\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.8068 - val_loss: 8.6068\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.7903 - val_loss: 8.5891\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.7743 - val_loss: 8.5720\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.7588 - val_loss: 8.5554\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.7437 - val_loss: 8.5393\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.7292 - val_loss: 8.5237\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.7151 - val_loss: 8.5085\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.7014 - val_loss: 8.4938\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.6882 - val_loss: 8.4795\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.6753 - val_loss: 8.4657\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.6628 - val_loss: 8.4523\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.6507 - val_loss: 8.4393\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.6389 - val_loss: 8.4266\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.6274 - val_loss: 8.4144\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.6162 - val_loss: 8.4025\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.6054 - val_loss: 8.3909\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.5948 - val_loss: 8.3797\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.5845 - val_loss: 8.3688\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.5744 - val_loss: 8.3582\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.5646 - val_loss: 8.3479\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.5551 - val_loss: 8.3378\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.5458 - val_loss: 8.3281\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5367 - val_loss: 8.3186\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.5278 - val_loss: 8.3094\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.5191 - val_loss: 8.3004\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 8.5106 - val_loss: 8.2916\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.5023 - val_loss: 8.2831\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4942 - val_loss: 8.2748\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 8.4863 - val_loss: 8.2667\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.4785 - val_loss: 8.2589\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.4710 - val_loss: 8.2512\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.4635 - val_loss: 8.2437\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.4562 - val_loss: 8.2364\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.4491 - val_loss: 8.2293\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.4420 - val_loss: 8.2223\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.4351 - val_loss: 8.2155\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.4284 - val_loss: 8.2089\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.4217 - val_loss: 8.2024\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.4151 - val_loss: 8.1961\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.4087 - val_loss: 8.1899\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4024 - val_loss: 8.1838\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.3962 - val_loss: 8.1778\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.3900 - val_loss: 8.1719\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3840 - val_loss: 8.1661\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.3781 - val_loss: 8.1603\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.3722 - val_loss: 8.1547\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.3664 - val_loss: 8.1491\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.3606 - val_loss: 8.1435\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.3550 - val_loss: 8.1380\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.3493 - val_loss: 8.1326\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.3438 - val_loss: 8.1272\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.3383 - val_loss: 8.1219\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.3328 - val_loss: 8.1166\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.3274 - val_loss: 8.1113\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 8.3220 - val_loss: 8.1061\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3166 - val_loss: 8.1009\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.3112 - val_loss: 8.0957\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.3059 - val_loss: 8.0906\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3007 - val_loss: 8.0855\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.2954 - val_loss: 8.0804\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.2902 - val_loss: 8.0753\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.2850 - val_loss: 8.0703\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.2797 - val_loss: 8.0652\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.2745 - val_loss: 8.0602\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.2693 - val_loss: 8.0552\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 8.2642 - val_loss: 8.0502\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 8.2590 - val_loss: 8.0452\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.2538 - val_loss: 8.0402\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.2487 - val_loss: 8.0352\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.2435 - val_loss: 8.0302\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2384 - val_loss: 8.0252\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.2333 - val_loss: 8.0203\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.2282 - val_loss: 8.0153\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.2231 - val_loss: 8.0103\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2180 - val_loss: 8.0054\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2129 - val_loss: 8.0005\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.2078 - val_loss: 7.9955\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.2027 - val_loss: 7.9906\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.1977 - val_loss: 7.9857\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1926 - val_loss: 7.9808\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.1875 - val_loss: 7.9759\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.1824 - val_loss: 7.9711\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 8.1773 - val_loss: 7.9662\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.1722 - val_loss: 7.9614\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1671 - val_loss: 7.9567\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.1620 - val_loss: 7.9519\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 8.1569 - val_loss: 7.9471\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.1518 - val_loss: 7.9424\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.1466 - val_loss: 7.9376\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.1415 - val_loss: 7.9329\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1364 - val_loss: 7.9282\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1312 - val_loss: 7.9234\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.1260 - val_loss: 7.9187\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.1209 - val_loss: 7.9140\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.1157 - val_loss: 7.9093\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.1105 - val_loss: 7.9046\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.1053 - val_loss: 7.8999\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1001 - val_loss: 7.8952\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.0949 - val_loss: 7.8904\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0897 - val_loss: 7.8857\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.0845 - val_loss: 7.8810\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0792 - val_loss: 7.8763\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.0740 - val_loss: 7.8716\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0687 - val_loss: 7.8668\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0635 - val_loss: 7.8621\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.0582 - val_loss: 7.8573\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.0529 - val_loss: 7.8526\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.0476 - val_loss: 7.8479\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.0424 - val_loss: 7.8431\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.0371 - val_loss: 7.8384\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.0319 - val_loss: 7.8336\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.0266 - val_loss: 7.8288\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0213 - val_loss: 7.8241\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.0160 - val_loss: 7.8193\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.0108 - val_loss: 7.8145\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0055 - val_loss: 7.8097\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.0002 - val_loss: 7.8049\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.9949 - val_loss: 7.8001\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.9895 - val_loss: 7.7953\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9842 - val_loss: 7.7905\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.9789 - val_loss: 7.7856\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.9736 - val_loss: 7.7808\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 7.9682 - val_loss: 7.7760\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9629 - val_loss: 7.7711\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.9576 - val_loss: 7.7663\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9522 - val_loss: 7.7614\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.9469 - val_loss: 7.7566\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.9415 - val_loss: 7.7517\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.9362 - val_loss: 7.7468\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.9308 - val_loss: 7.7420\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.9254 - val_loss: 7.7371\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.9201 - val_loss: 7.7322\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.9147 - val_loss: 7.7273\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.9094 - val_loss: 7.7224\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.9040 - val_loss: 7.7175\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.8986 - val_loss: 7.7127\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.8933 - val_loss: 7.7078\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.8879 - val_loss: 7.7029\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.8825 - val_loss: 7.6980\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.8772 - val_loss: 7.6931\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.8718 - val_loss: 7.6882\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.8664 - val_loss: 7.6833\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.8611 - val_loss: 7.6784\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.8557 - val_loss: 7.6735\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8504 - val_loss: 7.6686\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.8450 - val_loss: 7.6637\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.8397 - val_loss: 7.6588\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8343 - val_loss: 7.6539\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.8290 - val_loss: 7.6490\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.8236 - val_loss: 7.6441\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.8183 - val_loss: 7.6392\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.8129 - val_loss: 7.6343\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.8076 - val_loss: 7.6294\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8023 - val_loss: 7.6245\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7970 - val_loss: 7.6197\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7916 - val_loss: 7.6148\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.7863 - val_loss: 7.6099\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.7810 - val_loss: 7.6050\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.7757 - val_loss: 7.6001\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.7704 - val_loss: 7.5953\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7651 - val_loss: 7.5904\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7598 - val_loss: 7.5855\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.7545 - val_loss: 7.5807\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.7492 - val_loss: 7.5758\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7439 - val_loss: 7.5709\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7387 - val_loss: 7.5661\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.7334 - val_loss: 7.5612\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7281 - val_loss: 7.5564\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.7228 - val_loss: 7.5515\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.7176 - val_loss: 7.5467\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.7123 - val_loss: 7.5418\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7071 - val_loss: 7.5370\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.7018 - val_loss: 7.5321\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.6966 - val_loss: 7.5273\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.6913 - val_loss: 7.5225\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6861 - val_loss: 7.5176\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.6808 - val_loss: 7.5128\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.6756 - val_loss: 7.5080\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.6704 - val_loss: 7.5031\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 7.6651 - val_loss: 7.4983\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.6599 - val_loss: 7.4935\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.6547 - val_loss: 7.4886\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6495 - val_loss: 7.4838\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.6443 - val_loss: 7.4790\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.6390 - val_loss: 7.4742\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6338 - val_loss: 7.4694\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.6286 - val_loss: 7.4645\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.6234 - val_loss: 7.4597\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.6182 - val_loss: 7.4549\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.6130 - val_loss: 7.4501\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6078 - val_loss: 7.4453\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.6026 - val_loss: 7.4404\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.5974 - val_loss: 7.4356\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5922 - val_loss: 7.4308\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.5870 - val_loss: 7.4260\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.5818 - val_loss: 7.4212\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.5766 - val_loss: 7.4164\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.5714 - val_loss: 7.4116\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.5662 - val_loss: 7.4067\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.5610 - val_loss: 7.4019\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.5558 - val_loss: 7.3971\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.5506 - val_loss: 7.3923\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5454 - val_loss: 7.3875\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.5402 - val_loss: 7.3827\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.5350 - val_loss: 7.3778\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.5298 - val_loss: 7.3730\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5246 - val_loss: 7.3682\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5194 - val_loss: 7.3634\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.5142 - val_loss: 7.3586\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.5090 - val_loss: 7.3537\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5038 - val_loss: 7.3489\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.4986 - val_loss: 7.3441\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.4934 - val_loss: 7.3393\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.4881 - val_loss: 7.3344\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.4829 - val_loss: 7.3296\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.4777 - val_loss: 7.3248\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.4725 - val_loss: 7.3199\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 7.4673 - val_loss: 7.3151\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.4621 - val_loss: 7.3103\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.4569 - val_loss: 7.3054\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.4517 - val_loss: 7.3006\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.4465 - val_loss: 7.2957\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.4412 - val_loss: 7.2909\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.4360 - val_loss: 7.2860\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.4308 - val_loss: 7.2812\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.4256 - val_loss: 7.2763\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4203 - val_loss: 7.2715\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.4151 - val_loss: 7.2666\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.4099 - val_loss: 7.2618\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.4046 - val_loss: 7.2569\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.3994 - val_loss: 7.2520\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.3942 - val_loss: 7.2471\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.3889 - val_loss: 7.2423\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.3837 - val_loss: 7.2374\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.3784 - val_loss: 7.2325\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.3732 - val_loss: 7.2276\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.3679 - val_loss: 7.2227\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.3627 - val_loss: 7.2178\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.3574 - val_loss: 7.2129\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.3522 - val_loss: 7.2080\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.3469 - val_loss: 7.2031\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.3416 - val_loss: 7.1982\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.3364 - val_loss: 7.1933\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.3311 - val_loss: 7.1884\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.3258 - val_loss: 7.1835\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.3205 - val_loss: 7.1786\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 7.3153 - val_loss: 7.1736\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.3100 - val_loss: 7.1687\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.3047 - val_loss: 7.1638\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.2994 - val_loss: 7.1588\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.2941 - val_loss: 7.1539\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.2888 - val_loss: 7.1489\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.2835 - val_loss: 7.1440\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.2782 - val_loss: 7.1390\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.2729 - val_loss: 7.1340\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 7.2676 - val_loss: 7.1291\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 7.2623 - val_loss: 7.1241\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.2570 - val_loss: 7.1191\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.2517 - val_loss: 7.1141\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.2464 - val_loss: 7.1091\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 57.4169 - val_loss: 56.5092\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 57.3427 - val_loss: 56.3981\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 57.2314 - val_loss: 56.2684\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 57.1015 - val_loss: 56.1292\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 56.9621 - val_loss: 55.9851\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 56.8179 - val_loss: 55.8384\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 56.6709 - val_loss: 55.6900\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 56.5223 - val_loss: 55.5404\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 56.3725 - val_loss: 55.3898\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 56.2217 - val_loss: 55.2382\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 56.0700 - val_loss: 55.0856\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 55.9172 - val_loss: 54.9319\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 55.7634 - val_loss: 54.7770\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 55.6084 - val_loss: 54.6209\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 55.4521 - val_loss: 54.4633\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 55.2945 - val_loss: 54.3044\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 55.1354 - val_loss: 54.1438\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 54.9748 - val_loss: 53.9817\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 54.8126 - val_loss: 53.8178\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 54.6487 - val_loss: 53.6521\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 54.4829 - val_loss: 53.4845\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 54.3153 - val_loss: 53.3150\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 54.1458 - val_loss: 53.1435\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 53.9742 - val_loss: 52.9698\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 53.8006 - val_loss: 52.7941\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 53.6249 - val_loss: 52.6161\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 53.4469 - val_loss: 52.4359\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 53.2667 - val_loss: 52.2534\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 53.0842 - val_loss: 52.0686\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 52.8995 - val_loss: 51.8814\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 52.7123 - val_loss: 51.6918\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 52.5227 - val_loss: 51.4998\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 52.3307 - val_loss: 51.3053\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 52.1363 - val_loss: 51.1084\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 51.9393 - val_loss: 50.9090\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 51.7399 - val_loss: 50.7070\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 51.5380 - val_loss: 50.5026\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 51.3336 - val_loss: 50.2957\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 51.1267 - val_loss: 50.0863\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 50.9173 - val_loss: 49.8744\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 50.7054 - val_loss: 49.6599\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 50.4909 - val_loss: 49.4430\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 50.2740 - val_loss: 49.2236\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 50.0546 - val_loss: 49.0017\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 49.8326 - val_loss: 48.7773\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 49.6082 - val_loss: 48.5504\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 49.3813 - val_loss: 48.3210\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 49.1520 - val_loss: 48.0892\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 48.9202 - val_loss: 47.8549\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 48.6858 - val_loss: 47.6181\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 48.4491 - val_loss: 47.3788\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 48.2098 - val_loss: 47.1370\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 47.9681 - val_loss: 46.8927\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 47.7238 - val_loss: 46.6459\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 47.4771 - val_loss: 46.3966\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 47.2279 - val_loss: 46.1447\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 46.9762 - val_loss: 45.8903\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 46.7219 - val_loss: 45.6333\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 46.4650 - val_loss: 45.3737\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 46.2056 - val_loss: 45.1114\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 45.9436 - val_loss: 44.8466\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 45.6790 - val_loss: 44.5791\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 45.4118 - val_loss: 44.3090\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 45.1420 - val_loss: 44.0361\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 44.8695 - val_loss: 43.7606\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 44.5944 - val_loss: 43.4824\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 44.3166 - val_loss: 43.2014\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 44.0361 - val_loss: 42.9178\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 43.7530 - val_loss: 42.6314\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 43.4672 - val_loss: 42.3424\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 43.1787 - val_loss: 42.0507\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 42.8876 - val_loss: 41.7563\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 42.5940 - val_loss: 41.4592\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 42.2977 - val_loss: 41.1596\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 41.9989 - val_loss: 40.8575\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 41.6976 - val_loss: 40.5528\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 41.3938 - val_loss: 40.2457\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 41.0877 - val_loss: 39.9362\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 40.7793 - val_loss: 39.6244\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 40.4686 - val_loss: 39.3104\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 40.1558 - val_loss: 38.9942\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 39.8409 - val_loss: 38.6761\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 39.5240 - val_loss: 38.3560\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 39.2053 - val_loss: 38.0340\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 38.8848 - val_loss: 37.7104\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 38.5627 - val_loss: 37.3853\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 38.2391 - val_loss: 37.0586\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 37.9141 - val_loss: 36.7307\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 37.5879 - val_loss: 36.4017\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 37.2607 - val_loss: 36.0716\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 36.9325 - val_loss: 35.7408\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 36.6035 - val_loss: 35.4092\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 36.2740 - val_loss: 35.0772\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 35.9440 - val_loss: 34.7449\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 35.6138 - val_loss: 34.4124\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 35.2836 - val_loss: 34.0800\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 34.9534 - val_loss: 33.7479\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 34.6236 - val_loss: 33.4163\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 34.2944 - val_loss: 33.0853\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 33.9658 - val_loss: 32.7552\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 33.6382 - val_loss: 32.4262\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 33.3117 - val_loss: 32.0985\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 32.9865 - val_loss: 31.7723\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 32.6629 - val_loss: 31.4478\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 32.3410 - val_loss: 31.1252\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 32.0210 - val_loss: 30.8048\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 31.7031 - val_loss: 30.4867\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 31.3875 - val_loss: 30.1712\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 31.0743 - val_loss: 29.8584\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 30.7639 - val_loss: 29.5485\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 30.4562 - val_loss: 29.2417\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 30.1515 - val_loss: 28.9383\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 29.8500 - val_loss: 28.6383\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 29.5517 - val_loss: 28.3420\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 29.2568 - val_loss: 28.0494\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 28.9655 - val_loss: 27.7609\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 28.6780 - val_loss: 27.4764\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 28.3942 - val_loss: 27.1963\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 28.1143 - val_loss: 26.9204\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 27.8385 - val_loss: 26.6491\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 27.5668 - val_loss: 26.3824\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 27.2995 - val_loss: 26.1204\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 27.0365 - val_loss: 25.8631\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 26.7780 - val_loss: 25.6108\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 26.5240 - val_loss: 25.3634\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 26.2748 - val_loss: 25.1210\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 26.0303 - val_loss: 24.8837\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 25.7907 - val_loss: 24.6514\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 25.5559 - val_loss: 24.4243\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 25.3262 - val_loss: 24.2023\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 25.1016 - val_loss: 23.9855\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 24.8820 - val_loss: 23.7739\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 24.6676 - val_loss: 23.5678\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 24.4584 - val_loss: 23.3672\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 24.2544 - val_loss: 23.1728\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 24.0557 - val_loss: 22.9847\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 23.8623 - val_loss: 22.8032\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 23.6743 - val_loss: 22.6282\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 23.4919 - val_loss: 22.4609\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 23.3153 - val_loss: 22.2998\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 23.1455 - val_loss: 22.1455\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 22.9820 - val_loss: 21.9966\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 22.8245 - val_loss: 21.8531\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.6733 - val_loss: 21.7149\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 22.5277 - val_loss: 21.5816\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 22.3869 - val_loss: 21.4532\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 22.2501 - val_loss: 21.3290\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 22.1171 - val_loss: 21.2083\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 21.9874 - val_loss: 21.0906\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 21.8608 - val_loss: 20.9759\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 21.7371 - val_loss: 20.8641\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 21.6162 - val_loss: 20.7549\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 21.4979 - val_loss: 20.6484\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 21.3822 - val_loss: 20.5443\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 21.2688 - val_loss: 20.4426\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 21.1578 - val_loss: 20.3432\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 21.0490 - val_loss: 20.2460\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 20.9424 - val_loss: 20.1509\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 20.8379 - val_loss: 20.0578\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 20.7354 - val_loss: 19.9667\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 20.6349 - val_loss: 19.8774\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 20.5363 - val_loss: 19.7900\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 20.4395 - val_loss: 19.7044\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 20.3444 - val_loss: 19.6204\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 20.2511 - val_loss: 19.5381\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20.1595 - val_loss: 19.4574\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 20.0694 - val_loss: 19.3782\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 19.9810 - val_loss: 19.3005\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 19.8942 - val_loss: 19.2242\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 19.8088 - val_loss: 19.1494\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 19.7250 - val_loss: 19.0760\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 19.6426 - val_loss: 19.0039\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 19.5616 - val_loss: 18.9332\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.4820 - val_loss: 18.8637\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 19.4038 - val_loss: 18.7955\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.3270 - val_loss: 18.7286\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 19.2514 - val_loss: 18.6629\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 19.1772 - val_loss: 18.5984\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 19.1041 - val_loss: 18.5350\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 19.0324 - val_loss: 18.4727\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 18.9618 - val_loss: 18.4116\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 18.8924 - val_loss: 18.3515\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 18.8241 - val_loss: 18.2924\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 18.7570 - val_loss: 18.2344\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 18.6909 - val_loss: 18.1774\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.6259 - val_loss: 18.1213\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 18.5619 - val_loss: 18.0662\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 18.4990 - val_loss: 18.0120\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 18.4370 - val_loss: 17.9588\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 18.3759 - val_loss: 17.9064\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 18.3158 - val_loss: 17.8548\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 18.2566 - val_loss: 17.8041\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.1983 - val_loss: 17.7543\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 18.1409 - val_loss: 17.7052\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 18.0843 - val_loss: 17.6569\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 18.0284 - val_loss: 17.6094\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17.9734 - val_loss: 17.5626\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17.9191 - val_loss: 17.5165\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17.8656 - val_loss: 17.4711\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 17.8128 - val_loss: 17.4263\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.7606 - val_loss: 17.3822\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17.7091 - val_loss: 17.3386\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17.6583 - val_loss: 17.2956\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17.6080 - val_loss: 17.2532\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17.5583 - val_loss: 17.2112\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 17.5092 - val_loss: 17.1697\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 17.4605 - val_loss: 17.1287\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 17.4124 - val_loss: 17.0880\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 17.3647 - val_loss: 17.0477\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 17.3174 - val_loss: 17.0077\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17.2705 - val_loss: 16.9680\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 17.2240 - val_loss: 16.9286\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17.1778 - val_loss: 16.8894\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17.1318 - val_loss: 16.8503\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 17.0861 - val_loss: 16.8113\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 17.0406 - val_loss: 16.7724\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.9953 - val_loss: 16.7335\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 16.9500 - val_loss: 16.6946\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 16.9049 - val_loss: 16.6556\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 16.8598 - val_loss: 16.6165\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 16.8146 - val_loss: 16.5772\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.7694 - val_loss: 16.5376\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 16.7240 - val_loss: 16.4977\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.6784 - val_loss: 16.4575\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 16.6326 - val_loss: 16.4168\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.5865 - val_loss: 16.3756\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.5401 - val_loss: 16.3338\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 16.4931 - val_loss: 16.2914\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 16.4457 - val_loss: 16.2482\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 16.3977 - val_loss: 16.2042\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 16.3490 - val_loss: 16.1593\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 16.2996 - val_loss: 16.1134\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.2494 - val_loss: 16.0665\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.1982 - val_loss: 16.0183\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 16.1461 - val_loss: 15.9689\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 16.0928 - val_loss: 15.9182\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 16.0384 - val_loss: 15.8660\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 15.9828 - val_loss: 15.8122\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 15.9257 - val_loss: 15.7568\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 15.8673 - val_loss: 15.6997\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.8073 - val_loss: 15.6407\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 15.7457 - val_loss: 15.5799\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15.6825 - val_loss: 15.5171\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15.6174 - val_loss: 15.4523\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.5506 - val_loss: 15.3855\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.4820 - val_loss: 15.3165\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 15.4114 - val_loss: 15.2454\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 15.3389 - val_loss: 15.1722\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 15.2645 - val_loss: 15.0969\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.1881 - val_loss: 15.0195\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 15.1098 - val_loss: 14.9401\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 15.0296 - val_loss: 14.8586\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 14.9475 - val_loss: 14.7752\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14.8636 - val_loss: 14.6899\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 14.7779 - val_loss: 14.6028\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 14.6906 - val_loss: 14.5141\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 14.6016 - val_loss: 14.4238\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 14.5110 - val_loss: 14.3320\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 14.4191 - val_loss: 14.2389\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 14.3258 - val_loss: 14.1445\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 14.2312 - val_loss: 14.0491\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 14.1355 - val_loss: 13.9527\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 14.0388 - val_loss: 13.8555\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 13.9412 - val_loss: 13.7575\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 13.8427 - val_loss: 13.6590\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.7436 - val_loss: 13.5599\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.6440 - val_loss: 13.4606\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 13.5439 - val_loss: 13.3609\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 13.4437 - val_loss: 13.2613\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 13.3437 - val_loss: 13.1618\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 13.2438 - val_loss: 13.0624\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13.1442 - val_loss: 12.9632\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.0448 - val_loss: 12.8644\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 12.9462 - val_loss: 12.7663\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 12.8484 - val_loss: 12.6689\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 12.7516 - val_loss: 12.5725\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 12.6558 - val_loss: 12.4771\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 12.5612 - val_loss: 12.3828\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 12.4678 - val_loss: 12.2897\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 12.3757 - val_loss: 12.1978\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 12.2848 - val_loss: 12.1071\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 12.1953 - val_loss: 12.0178\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.1070 - val_loss: 11.9298\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 12.0202 - val_loss: 11.8432\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.9348 - val_loss: 11.7582\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.8509 - val_loss: 11.6748\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 11.7685 - val_loss: 11.5929\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 11.6876 - val_loss: 11.5124\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.6080 - val_loss: 11.4336\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.5299 - val_loss: 11.3562\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 11.4532 - val_loss: 11.2804\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.3780 - val_loss: 11.2060\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.3041 - val_loss: 11.1331\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 11.2316 - val_loss: 11.0617\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.1604 - val_loss: 10.9917\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 11.0905 - val_loss: 10.9230\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 11.0220 - val_loss: 10.8557\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 10.9547 - val_loss: 10.7897\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.8887 - val_loss: 10.7250\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.8240 - val_loss: 10.6616\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.7605 - val_loss: 10.5995\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 10.6983 - val_loss: 10.5387\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 10.6373 - val_loss: 10.4791\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.5776 - val_loss: 10.4209\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 10.5191 - val_loss: 10.3640\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 10.4618 - val_loss: 10.3084\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.4057 - val_loss: 10.2540\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 10.3508 - val_loss: 10.2007\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 10.2971 - val_loss: 10.1486\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 10.2446 - val_loss: 10.0977\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 10.1932 - val_loss: 10.0478\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 10.1430 - val_loss: 9.9990\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 10.0939 - val_loss: 9.9513\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.0460 - val_loss: 9.9047\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.9992 - val_loss: 9.8590\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.9535 - val_loss: 9.8144\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.9088 - val_loss: 9.7708\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 9.8653 - val_loss: 9.7283\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.8229 - val_loss: 9.6867\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.7815 - val_loss: 9.6460\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.7411 - val_loss: 9.6063\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.7018 - val_loss: 9.5675\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.6635 - val_loss: 9.5296\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.6261 - val_loss: 9.4926\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.5897 - val_loss: 9.4564\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.5543 - val_loss: 9.4210\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.5198 - val_loss: 9.3865\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.4862 - val_loss: 9.3527\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.4535 - val_loss: 9.3198\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 9.4216 - val_loss: 9.2876\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.3907 - val_loss: 9.2562\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.3606 - val_loss: 9.2255\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.3313 - val_loss: 9.1956\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.3028 - val_loss: 9.1663\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.2751 - val_loss: 9.1378\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.2481 - val_loss: 9.1099\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 9.2219 - val_loss: 9.0826\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.1964 - val_loss: 9.0560\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.1716 - val_loss: 9.0300\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.1475 - val_loss: 9.0047\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.1241 - val_loss: 8.9799\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.1014 - val_loss: 8.9557\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.0792 - val_loss: 8.9321\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.0577 - val_loss: 8.9090\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.0369 - val_loss: 8.8865\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.0166 - val_loss: 8.8646\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.9968 - val_loss: 8.8431\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.9777 - val_loss: 8.8222\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.9590 - val_loss: 8.8018\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.9409 - val_loss: 8.7818\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.9233 - val_loss: 8.7623\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.9062 - val_loss: 8.7433\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.8895 - val_loss: 8.7248\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.8733 - val_loss: 8.7066\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.857 - 0s 87ms/step - loss: 8.8576 - val_loss: 8.6889\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.8423 - val_loss: 8.6716\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.8274 - val_loss: 8.6548\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.8129 - val_loss: 8.6383\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.7988 - val_loss: 8.6222\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.7851 - val_loss: 8.6065\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.7717 - val_loss: 8.5912\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.7587 - val_loss: 8.5762\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.7460 - val_loss: 8.5616\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.7337 - val_loss: 8.5473\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 8.7217 - val_loss: 8.5333\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.7100 - val_loss: 8.5197\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.6985 - val_loss: 8.5063\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.6874 - val_loss: 8.4933\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.6765 - val_loss: 8.4805\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.6659 - val_loss: 8.4681\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.6556 - val_loss: 8.4559\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.6455 - val_loss: 8.4439\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.6356 - val_loss: 8.4322\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.6260 - val_loss: 8.4208\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.6165 - val_loss: 8.4096\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.6073 - val_loss: 8.3986\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.5983 - val_loss: 8.3878\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.5894 - val_loss: 8.3773\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.5808 - val_loss: 8.3670\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.5723 - val_loss: 8.3569\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.5640 - val_loss: 8.3469\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.5559 - val_loss: 8.3372\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.5479 - val_loss: 8.3276\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.5401 - val_loss: 8.3181\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.5324 - val_loss: 8.3088\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.5249 - val_loss: 8.2997\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.5174 - val_loss: 8.2907\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.5102 - val_loss: 8.2818\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.5030 - val_loss: 8.2731\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.4959 - val_loss: 8.2645\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.4890 - val_loss: 8.2560\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.4821 - val_loss: 8.2477\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4754 - val_loss: 8.2394\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.4687 - val_loss: 8.2313\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.4621 - val_loss: 8.2233\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4557 - val_loss: 8.2153\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.4493 - val_loss: 8.2075\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.4429 - val_loss: 8.1998\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.4367 - val_loss: 8.1921\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.4305 - val_loss: 8.1845\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.4243 - val_loss: 8.1770\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 8.4182 - val_loss: 8.1696\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 8.4122 - val_loss: 8.1622\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 8.4061 - val_loss: 8.1549\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.4001 - val_loss: 8.1476\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.3941 - val_loss: 8.1404\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.3882 - val_loss: 8.1332\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.3822 - val_loss: 8.1260\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.3763 - val_loss: 8.1189\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.3704 - val_loss: 8.1119\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.3645 - val_loss: 8.1050\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.3587 - val_loss: 8.0980\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3529 - val_loss: 8.0912\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.3470 - val_loss: 8.0843\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.3412 - val_loss: 8.0775\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.3354 - val_loss: 8.0707\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.3296 - val_loss: 8.0639\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.3237 - val_loss: 8.0572\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.3178 - val_loss: 8.0504\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.3120 - val_loss: 8.0437\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.3061 - val_loss: 8.0370\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.3002 - val_loss: 8.0303\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.2942 - val_loss: 8.0236\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.2883 - val_loss: 8.0169\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.2823 - val_loss: 8.0102\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.2763 - val_loss: 8.0035\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2702 - val_loss: 7.9968\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2642 - val_loss: 7.9902\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.2581 - val_loss: 7.9835\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.2520 - val_loss: 7.9769\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2458 - val_loss: 7.9702\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.2397 - val_loss: 7.9636\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2335 - val_loss: 7.9570\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2273 - val_loss: 7.9503\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.2211 - val_loss: 7.9437\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.2148 - val_loss: 7.9372\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.2086 - val_loss: 7.9306\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2023 - val_loss: 7.9240\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.1960 - val_loss: 7.9175\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.1897 - val_loss: 7.9110\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.1834 - val_loss: 7.9045\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.1770 - val_loss: 7.8980\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1707 - val_loss: 7.8915\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.1644 - val_loss: 7.8851\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1580 - val_loss: 7.8786\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.1517 - val_loss: 7.8722\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1453 - val_loss: 7.8659\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.1390 - val_loss: 7.8595\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.1326 - val_loss: 7.8532\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.1263 - val_loss: 7.8469\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.1200 - val_loss: 7.8406\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.1136 - val_loss: 7.8343\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1073 - val_loss: 7.8281\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1010 - val_loss: 7.8219\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0947 - val_loss: 7.8157\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0884 - val_loss: 7.8095\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.0821 - val_loss: 7.8034\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.0758 - val_loss: 7.7973\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.0696 - val_loss: 7.7912\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 8.0633 - val_loss: 7.7851\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 8.0571 - val_loss: 7.7791\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 8.0509 - val_loss: 7.7731\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.0447 - val_loss: 7.7672\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.0385 - val_loss: 7.7612\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.0324 - val_loss: 7.7553\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.0262 - val_loss: 7.7494\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.0201 - val_loss: 7.7436\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0140 - val_loss: 7.7377\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0080 - val_loss: 7.7319\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.0019 - val_loss: 7.7261\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.9959 - val_loss: 7.7204\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.9899 - val_loss: 7.7147\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.9839 - val_loss: 7.7090\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.9780 - val_loss: 7.7033\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.9720 - val_loss: 7.6976\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9661 - val_loss: 7.6920\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.9603 - val_loss: 7.6864\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.9544 - val_loss: 7.6808\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.9486 - val_loss: 7.6752\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 7.9428 - val_loss: 7.6697\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.9370 - val_loss: 7.6642\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.9312 - val_loss: 7.6587\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9255 - val_loss: 7.6532\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9198 - val_loss: 7.6478\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.9141 - val_loss: 7.6423\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.9085 - val_loss: 7.6369\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9028 - val_loss: 7.6316\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.8972 - val_loss: 7.6262\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.8916 - val_loss: 7.6209\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 7.8860 - val_loss: 7.6155\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.8805 - val_loss: 7.6102\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.8750 - val_loss: 7.6050\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.8695 - val_loss: 7.5997\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.8640 - val_loss: 7.5945\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.8585 - val_loss: 7.5893\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.8531 - val_loss: 7.5841\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.8477 - val_loss: 7.5789\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.8423 - val_loss: 7.5737\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.8369 - val_loss: 7.5686\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8316 - val_loss: 7.5635\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.8262 - val_loss: 7.5584\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.8209 - val_loss: 7.5533\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.8157 - val_loss: 7.5483\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.8104 - val_loss: 7.5433\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8051 - val_loss: 7.5383\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.7999 - val_loss: 7.5333\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.7947 - val_loss: 7.5283\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.7895 - val_loss: 7.5234\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.7843 - val_loss: 7.5184\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7792 - val_loss: 7.5135\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.7741 - val_loss: 7.5086\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.7689 - val_loss: 7.5038\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.7638 - val_loss: 7.4989\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7588 - val_loss: 7.4941\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7537 - val_loss: 7.4893\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.7487 - val_loss: 7.4846\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.7436 - val_loss: 7.4799\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.7386 - val_loss: 7.4752\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.7337 - val_loss: 7.4705\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.7287 - val_loss: 7.4658\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.7237 - val_loss: 7.4612\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.7188 - val_loss: 7.4565\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7139 - val_loss: 7.4519\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7.7090 - val_loss: 7.4473\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.7041 - val_loss: 7.4427\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.6992 - val_loss: 7.4381\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.6943 - val_loss: 7.4335\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.6894 - val_loss: 7.4290\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6846 - val_loss: 7.4244\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6797 - val_loss: 7.4199\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.6749 - val_loss: 7.4153\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.6701 - val_loss: 7.4108\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.6653 - val_loss: 7.4063\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.6605 - val_loss: 7.4018\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.6557 - val_loss: 7.3973\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6509 - val_loss: 7.3928\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.6462 - val_loss: 7.3883\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.6414 - val_loss: 7.3838\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.6367 - val_loss: 7.3794\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.6319 - val_loss: 7.3749\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.6272 - val_loss: 7.3705\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.6225 - val_loss: 7.3661\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 7.6178 - val_loss: 7.3616\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 7.6131 - val_loss: 7.3572\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.6084 - val_loss: 7.3528\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.6037 - val_loss: 7.3484\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.5991 - val_loss: 7.3440\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.5944 - val_loss: 7.3397\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5898 - val_loss: 7.3354\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.5851 - val_loss: 7.3310\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.5805 - val_loss: 7.3267\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.5759 - val_loss: 7.3224\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5713 - val_loss: 7.3181\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.5667 - val_loss: 7.3138\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5622 - val_loss: 7.3096\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 7.5576 - val_loss: 7.3053\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.5530 - val_loss: 7.3010\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.5484 - val_loss: 7.2967\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.5439 - val_loss: 7.2924\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.5393 - val_loss: 7.2882\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5348 - val_loss: 7.2839\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5302 - val_loss: 7.2797\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.5257 - val_loss: 7.2754\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.5212 - val_loss: 7.2712\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.5166 - val_loss: 7.2669\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.5121 - val_loss: 7.2627\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5076 - val_loss: 7.2585\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5031 - val_loss: 7.2542\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.4985 - val_loss: 7.2500\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.4940 - val_loss: 7.2458\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.4895 - val_loss: 7.2416\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.4850 - val_loss: 7.2374\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.4805 - val_loss: 7.2332\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4760 - val_loss: 7.2290\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.4715 - val_loss: 7.2248\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.4671 - val_loss: 7.2206\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.4626 - val_loss: 7.2165\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.4581 - val_loss: 7.2123\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.4536 - val_loss: 7.2081\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.4491 - val_loss: 7.2040\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.4447 - val_loss: 7.1998\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.4402 - val_loss: 7.1957\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.4357 - val_loss: 7.1915\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.4313 - val_loss: 7.1874\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4268 - val_loss: 7.1832\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.4223 - val_loss: 7.1791\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4179 - val_loss: 7.1750\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.4134 - val_loss: 7.1708\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.4090 - val_loss: 7.1667\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.4045 - val_loss: 7.1626\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.4001 - val_loss: 7.1584\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.3956 - val_loss: 7.1543\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.3912 - val_loss: 7.1502\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.3867 - val_loss: 7.1461\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.3823 - val_loss: 7.1420\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.3778 - val_loss: 7.1379\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.3734 - val_loss: 7.1338\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.3689 - val_loss: 7.1297\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.3645 - val_loss: 7.1256\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.3601 - val_loss: 7.1215\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 58.3487 - val_loss: 57.1258\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 58.2695 - val_loss: 57.0072\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 58.1510 - val_loss: 56.8693\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 58.0133 - val_loss: 56.7221\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 57.8662 - val_loss: 56.5706\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 57.7149 - val_loss: 56.4171\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 57.5616 - val_loss: 56.2628\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57.4076 - val_loss: 56.1083\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 57.2533 - val_loss: 55.9538\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 57.0990 - val_loss: 55.7993\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 56.9447 - val_loss: 55.6447\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 56.7904 - val_loss: 55.4900\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 56.6360 - val_loss: 55.3352\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 56.4815 - val_loss: 55.1801\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 56.3267 - val_loss: 55.0246\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 56.1715 - val_loss: 54.8686\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 56.0158 - val_loss: 54.7120\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 55.8595 - val_loss: 54.5547\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 55.7026 - val_loss: 54.3965\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 55.5448 - val_loss: 54.2375\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 55.3862 - val_loss: 54.0775\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 55.2265 - val_loss: 53.9164\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 55.0658 - val_loss: 53.7541\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 54.9039 - val_loss: 53.5905\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 54.7407 - val_loss: 53.4255\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 54.5762 - val_loss: 53.2591\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 54.4103 - val_loss: 53.0912\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 54.2429 - val_loss: 52.9217\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 54.0738 - val_loss: 52.7505\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 53.9031 - val_loss: 52.5775\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 53.7307 - val_loss: 52.4027\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 53.5564 - val_loss: 52.2261\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 53.3803 - val_loss: 52.0475\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 53.2023 - val_loss: 51.8669\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 53.0223 - val_loss: 51.6843\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 52.8402 - val_loss: 51.4996\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 52.6561 - val_loss: 51.3128\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 52.4699 - val_loss: 51.1237\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 52.2815 - val_loss: 50.9325\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 52.0908 - val_loss: 50.7391\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 51.8980 - val_loss: 50.5434\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.7029 - val_loss: 50.3454\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.5056 - val_loss: 50.1452\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 51.3059 - val_loss: 49.9426\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 51.1040 - val_loss: 49.7378\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 50.8999 - val_loss: 49.5308\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 50.6933 - val_loss: 49.3214\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 50.4846 - val_loss: 49.1099\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 50.2736 - val_loss: 48.8961\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 50.0604 - val_loss: 48.6802\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 49.8450 - val_loss: 48.4621\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 49.6275 - val_loss: 48.2419\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 49.4078 - val_loss: 48.0197\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 49.1861 - val_loss: 47.7955\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 48.9624 - val_loss: 47.5694\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 48.7367 - val_loss: 47.3414\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 48.5091 - val_loss: 47.1115\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 48.2796 - val_loss: 46.8800\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 48.0484 - val_loss: 46.6467\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 47.8155 - val_loss: 46.4118\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 47.5809 - val_loss: 46.1753\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 47.3447 - val_loss: 45.9374\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 47.1070 - val_loss: 45.6980\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 46.8678 - val_loss: 45.4571\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 46.6272 - val_loss: 45.2150\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 46.3853 - val_loss: 44.9715\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 46.1420 - val_loss: 44.7268\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 45.8975 - val_loss: 44.4810\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 45.6517 - val_loss: 44.2339\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 45.4048 - val_loss: 43.9857\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 45.1567 - val_loss: 43.7364\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 44.9076 - val_loss: 43.4860\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 44.6573 - val_loss: 43.2346\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 44.4060 - val_loss: 42.9821\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 44.1536 - val_loss: 42.7286\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 43.9002 - val_loss: 42.4741\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 43.6457 - val_loss: 42.2186\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 43.3903 - val_loss: 41.9621\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 43.1339 - val_loss: 41.7046\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 42.8765 - val_loss: 41.4461\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 42.6180 - val_loss: 41.1866\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 42.3586 - val_loss: 40.9261\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 42.0982 - val_loss: 40.6646\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 41.8368 - val_loss: 40.4021\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 41.5744 - val_loss: 40.1386\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 41.3109 - val_loss: 39.8740\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 41.0465 - val_loss: 39.6084\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 40.7810 - val_loss: 39.3418\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 40.5145 - val_loss: 39.0741\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 40.2469 - val_loss: 38.8054\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 39.9783 - val_loss: 38.5356\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 39.7086 - val_loss: 38.2647\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 39.4379 - val_loss: 37.9927\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 39.1661 - val_loss: 37.7197\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 38.8932 - val_loss: 37.4457\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 38.6193 - val_loss: 37.1705\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 38.3444 - val_loss: 36.8944\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 38.0684 - val_loss: 36.6172\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 37.7915 - val_loss: 36.3389\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 37.5135 - val_loss: 36.0598\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 37.2346 - val_loss: 35.7796\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 36.9547 - val_loss: 35.4985\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36.6739 - val_loss: 35.2166\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 36.3923 - val_loss: 34.9338\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36.1099 - val_loss: 34.6503\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 35.8267 - val_loss: 34.3661\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 35.5429 - val_loss: 34.0812\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 35.2585 - val_loss: 33.7958\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 34.9736 - val_loss: 33.5099\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 34.6882 - val_loss: 33.2236\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 34.4025 - val_loss: 32.9371\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 34.1165 - val_loss: 32.6503\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 33.8304 - val_loss: 32.3635\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 33.5443 - val_loss: 32.0768\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 33.2583 - val_loss: 31.7902\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 32.9726 - val_loss: 31.5039\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 32.6871 - val_loss: 31.2181\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 32.4022 - val_loss: 30.9328\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 32.1179 - val_loss: 30.6482\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 31.8343 - val_loss: 30.3644\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 31.5516 - val_loss: 30.0816\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 31.2700 - val_loss: 29.7999\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 30.9895 - val_loss: 29.5195\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 30.7104 - val_loss: 29.2404\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 30.4327 - val_loss: 28.9629\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 30.1566 - val_loss: 28.6870\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 29.8822 - val_loss: 28.4129\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 29.6098 - val_loss: 28.1408\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 29.3394 - val_loss: 27.8707\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 29.0711 - val_loss: 27.6028\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 28.8052 - val_loss: 27.3373\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 28.5416 - val_loss: 27.0742\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 28.2807 - val_loss: 26.8137\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 28.0225 - val_loss: 26.5559\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 27.7671 - val_loss: 26.3010\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 27.5147 - val_loss: 26.0490\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 27.2653 - val_loss: 25.8001\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 27.0192 - val_loss: 25.5545\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 26.7764 - val_loss: 25.3122\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 26.5370 - val_loss: 25.0733\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 26.3012 - val_loss: 24.8381\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 26.0691 - val_loss: 24.6066\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 25.8407 - val_loss: 24.3790\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 25.6161 - val_loss: 24.1554\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 25.3954 - val_loss: 23.9359\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 25.1786 - val_loss: 23.7208\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 24.9659 - val_loss: 23.5101\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 24.7573 - val_loss: 23.3041\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 24.5528 - val_loss: 23.1028\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 24.3525 - val_loss: 22.9065\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 24.1566 - val_loss: 22.7152\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 23.9651 - val_loss: 22.5290\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 23.7781 - val_loss: 22.3482\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 23.5959 - val_loss: 22.1730\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 23.4186 - val_loss: 22.0038\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 23.2465 - val_loss: 21.8409\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 23.0797 - val_loss: 21.6845\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 22.9179 - val_loss: 21.5358\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 22.7611 - val_loss: 21.3935\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 22.6102 - val_loss: 21.2582\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 22.4652 - val_loss: 21.1284\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 22.3254 - val_loss: 21.0036\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 22.1904 - val_loss: 20.8832\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 22.0596 - val_loss: 20.7667\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 21.9326 - val_loss: 20.6538\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 21.8089 - val_loss: 20.5441\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 21.6882 - val_loss: 20.4371\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 21.5705 - val_loss: 20.3328\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 21.4554 - val_loss: 20.2308\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 21.3429 - val_loss: 20.1310\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 21.2328 - val_loss: 20.0335\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 21.1250 - val_loss: 19.9380\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 21.0194 - val_loss: 19.8445\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 20.9159 - val_loss: 19.7529\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 20.8144 - val_loss: 19.6631\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 20.7147 - val_loss: 19.5750\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 20.6168 - val_loss: 19.4886\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 20.5206 - val_loss: 19.4037\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 20.4260 - val_loss: 19.3204\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 20.3329 - val_loss: 19.2385\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 20.2414 - val_loss: 19.1579\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20.1513 - val_loss: 19.0786\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 20.0626 - val_loss: 19.0007\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 19.9753 - val_loss: 18.9239\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 19.8892 - val_loss: 18.8483\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 19.8044 - val_loss: 18.7738\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.7209 - val_loss: 18.7004\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 19.6386 - val_loss: 18.6281\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 19.5574 - val_loss: 18.5567\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 19.4774 - val_loss: 18.4864\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 19.3985 - val_loss: 18.4170\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 19.3207 - val_loss: 18.3486\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 19.2440 - val_loss: 18.2810\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 19.1682 - val_loss: 18.2143\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 19.0935 - val_loss: 18.1485\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 19.0198 - val_loss: 18.0834\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.9470 - val_loss: 18.0192\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 18.8751 - val_loss: 17.9557\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 18.8041 - val_loss: 17.8930\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 18.7339 - val_loss: 17.8310\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 18.6646 - val_loss: 17.7696\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 18.5962 - val_loss: 17.7090\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 18.5284 - val_loss: 17.6489\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 18.4615 - val_loss: 17.5895\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 18.3953 - val_loss: 17.5307\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 18.3298 - val_loss: 17.4724\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 18.26 - 0s 67ms/step - loss: 18.2649 - val_loss: 17.4146\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 18.2007 - val_loss: 17.3574\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 18.1372 - val_loss: 17.3006\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 18.0742 - val_loss: 17.2443\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 18.0119 - val_loss: 17.1884\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.9501 - val_loss: 17.1329\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 17.8888 - val_loss: 17.0778\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.8280 - val_loss: 17.0231\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.7676 - val_loss: 16.9687\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.7077 - val_loss: 16.9145\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 17.6483 - val_loss: 16.8607\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 17.5891 - val_loss: 16.8070\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.5304 - val_loss: 16.7535\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 17.4719 - val_loss: 16.7002\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17.4137 - val_loss: 16.6471\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 17.3557 - val_loss: 16.5940\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.2980 - val_loss: 16.5409\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.2403 - val_loss: 16.4878\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 17.1828 - val_loss: 16.4348\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 17.1254 - val_loss: 16.3816\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.0680 - val_loss: 16.3283\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 17.0106 - val_loss: 16.2748\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.9531 - val_loss: 16.2211\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 16.8955 - val_loss: 16.1671\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.8378 - val_loss: 16.1128\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 16.7799 - val_loss: 16.0581\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 16.7217 - val_loss: 16.0030\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 16.6633 - val_loss: 15.9474\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.6045 - val_loss: 15.8913\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 16.5453 - val_loss: 15.8346\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 16.4857 - val_loss: 15.7773\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.4256 - val_loss: 15.7193\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 16.3650 - val_loss: 15.6607\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 16.3039 - val_loss: 15.6012\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.2421 - val_loss: 15.5410\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 16.1797 - val_loss: 15.4799\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 16.1166 - val_loss: 15.4180\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 16.0529 - val_loss: 15.3552\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 15.9884 - val_loss: 15.2915\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 15.9231 - val_loss: 15.2269\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 15.8571 - val_loss: 15.1614\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.7903 - val_loss: 15.0950\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 15.7228 - val_loss: 15.0277\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 15.6545 - val_loss: 14.9594\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 15.5854 - val_loss: 14.8903\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 15.5155 - val_loss: 14.8203\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.4449 - val_loss: 14.7494\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 15.3736 - val_loss: 14.6777\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 15.3015 - val_loss: 14.6052\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 15.2287 - val_loss: 14.5318\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 15.1552 - val_loss: 14.4577\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 15.0810 - val_loss: 14.3827\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 15.0060 - val_loss: 14.3070\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 14.9304 - val_loss: 14.2305\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 14.8541 - val_loss: 14.1532\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 14.7771 - val_loss: 14.0751\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 14.6994 - val_loss: 13.9962\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 14.6209 - val_loss: 13.9165\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.5418 - val_loss: 13.8361\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 14.4620 - val_loss: 13.7549\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 14.3815 - val_loss: 13.6728\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.3002 - val_loss: 13.5901\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 14.2184 - val_loss: 13.5065\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 14.1358 - val_loss: 13.4223\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 14.0526 - val_loss: 13.3373\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.9688 - val_loss: 13.2517\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 13.8844 - val_loss: 13.1654\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 13.7995 - val_loss: 13.0785\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 13.7140 - val_loss: 12.9911\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.6281 - val_loss: 12.9030\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13.5416 - val_loss: 12.8145\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.4548 - val_loss: 12.7254\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 13.3675 - val_loss: 12.6359\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 13.2798 - val_loss: 12.5459\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 13.1917 - val_loss: 12.4555\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 13.1033 - val_loss: 12.3646\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 13.0145 - val_loss: 12.2733\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.9254 - val_loss: 12.1817\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 12.8361 - val_loss: 12.0897\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 12.7464 - val_loss: 11.9974\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 12.6566 - val_loss: 11.9049\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.5666 - val_loss: 11.8123\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 12.4765 - val_loss: 11.7195\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.3865 - val_loss: 11.6269\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.2967 - val_loss: 11.5345\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.2072 - val_loss: 11.4425\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 12.1183 - val_loss: 11.3511\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 12.0300 - val_loss: 11.2604\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 11.9425 - val_loss: 11.1706\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 11.8558 - val_loss: 11.0817\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.7699 - val_loss: 10.9939\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.6850 - val_loss: 10.9072\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 11.6011 - val_loss: 10.8219\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 11.5185 - val_loss: 10.7380\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 11.4370 - val_loss: 10.6556\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 11.3568 - val_loss: 10.5747\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.2777 - val_loss: 10.4954\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.2000 - val_loss: 10.4177\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 11.1235 - val_loss: 10.3417\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 11.0483 - val_loss: 10.2674\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.9745 - val_loss: 10.1947\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 10.9020 - val_loss: 10.1236\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 10.8307 - val_loss: 10.0540\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.7608 - val_loss: 9.9859\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 10.6922 - val_loss: 9.9192\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10.6248 - val_loss: 9.8540\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 10.5588 - val_loss: 9.7903\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 10.4940 - val_loss: 9.7280\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 10.4306 - val_loss: 9.6670\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 10.3684 - val_loss: 9.6075\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.3075 - val_loss: 9.5492\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.2479 - val_loss: 9.4923\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 10.1896 - val_loss: 9.4368\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 10.1326 - val_loss: 9.3826\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.0768 - val_loss: 9.3297\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.0225 - val_loss: 9.2781\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 9.9694 - val_loss: 9.2279\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.9176 - val_loss: 9.1790\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.8671 - val_loss: 9.1314\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.8178 - val_loss: 9.0850\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.7698 - val_loss: 9.0398\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 9.7230 - val_loss: 8.9959\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.6773 - val_loss: 8.9531\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 9.6329 - val_loss: 8.9115\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.5896 - val_loss: 8.8710\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.5474 - val_loss: 8.8317\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.5064 - val_loss: 8.7936\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.4666 - val_loss: 8.7569\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.4279 - val_loss: 8.7215\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.3903 - val_loss: 8.6872\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.3539 - val_loss: 8.6541\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.3186 - val_loss: 8.6222\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.2845 - val_loss: 8.5915\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.2516 - val_loss: 8.5619\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 9.2197 - val_loss: 8.5333\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.1888 - val_loss: 8.5057\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 9.1589 - val_loss: 8.4791\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.1301 - val_loss: 8.4534\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.1022 - val_loss: 8.4287\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.0752 - val_loss: 8.4048\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 9.0492 - val_loss: 8.3818\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.0241 - val_loss: 8.3597\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.9998 - val_loss: 8.3383\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.9764 - val_loss: 8.3178\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.9538 - val_loss: 8.2981\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.9321 - val_loss: 8.2792\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.9111 - val_loss: 8.2610\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.8909 - val_loss: 8.2435\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.8714 - val_loss: 8.2267\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.8526 - val_loss: 8.2105\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.8344 - val_loss: 8.1949\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.8169 - val_loss: 8.1799\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.8000 - val_loss: 8.1655\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.7837 - val_loss: 8.1516\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.7679 - val_loss: 8.1383\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.7526 - val_loss: 8.1254\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.7379 - val_loss: 8.1131\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.7236 - val_loss: 8.1013\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.7098 - val_loss: 8.0899\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.6965 - val_loss: 8.0790\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.6836 - val_loss: 8.0685\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.6710 - val_loss: 8.0584\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.6589 - val_loss: 8.0488\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.6472 - val_loss: 8.0395\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.6358 - val_loss: 8.0307\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.6247 - val_loss: 8.0222\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.6140 - val_loss: 8.0141\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.6035 - val_loss: 8.0063\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.5934 - val_loss: 7.9989\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.5836 - val_loss: 7.9918\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.5740 - val_loss: 7.9850\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 8.5647 - val_loss: 7.9785\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.5557 - val_loss: 7.9723\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.5469 - val_loss: 7.9662\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.5383 - val_loss: 7.9604\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 8.5299 - val_loss: 7.9548\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.5218 - val_loss: 7.9493\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.5139 - val_loss: 7.9440\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.5063 - val_loss: 7.9388\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.4988 - val_loss: 7.9337\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.4915 - val_loss: 7.9288\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.4844 - val_loss: 7.9239\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.4775 - val_loss: 7.9192\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.4707 - val_loss: 7.9145\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.4641 - val_loss: 7.9099\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.4576 - val_loss: 7.9053\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.4512 - val_loss: 7.9008\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4450 - val_loss: 7.8963\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.4388 - val_loss: 7.8919\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.4327 - val_loss: 7.8876\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.4266 - val_loss: 7.8833\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.4207 - val_loss: 7.8792\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.4148 - val_loss: 7.8751\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.4089 - val_loss: 7.8711\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.4031 - val_loss: 7.8671\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.3974 - val_loss: 7.8631\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3917 - val_loss: 7.8592\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3861 - val_loss: 7.8553\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3805 - val_loss: 7.8514\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.3749 - val_loss: 7.8475\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.3694 - val_loss: 7.8436\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.3639 - val_loss: 7.8397\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.3585 - val_loss: 7.8358\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.3530 - val_loss: 7.8318\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.3476 - val_loss: 7.8279\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.3423 - val_loss: 7.8240\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.3369 - val_loss: 7.8200\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3316 - val_loss: 7.8161\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.3263 - val_loss: 7.8121\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.3211 - val_loss: 7.8081\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.3159 - val_loss: 7.8041\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.3107 - val_loss: 7.8001\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.3055 - val_loss: 7.7961\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.3004 - val_loss: 7.7920\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.2953 - val_loss: 7.7880\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.2901 - val_loss: 7.7839\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2850 - val_loss: 7.7799\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.2799 - val_loss: 7.7758\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.2749 - val_loss: 7.7717\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2698 - val_loss: 7.7675\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.2648 - val_loss: 7.7634\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.2598 - val_loss: 7.7593\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.2548 - val_loss: 7.7551\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 8.2498 - val_loss: 7.7509\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2449 - val_loss: 7.7468\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2399 - val_loss: 7.7426\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 8.2349 - val_loss: 7.7384\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.2299 - val_loss: 7.7341\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.2250 - val_loss: 7.7299\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.2200 - val_loss: 7.7257\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.2150 - val_loss: 7.7214\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2101 - val_loss: 7.7172\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.2051 - val_loss: 7.7129\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2002 - val_loss: 7.7086\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.1952 - val_loss: 7.7043\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.1902 - val_loss: 7.7001\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.1853 - val_loss: 7.6958\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.1803 - val_loss: 7.6915\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.1753 - val_loss: 7.6872\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.1704 - val_loss: 7.6829\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.1654 - val_loss: 7.6786\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.1604 - val_loss: 7.6743\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.1554 - val_loss: 7.6700\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1505 - val_loss: 7.6657\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.1455 - val_loss: 7.6613\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1405 - val_loss: 7.6570\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.1355 - val_loss: 7.6527\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.1305 - val_loss: 7.6483\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1256 - val_loss: 7.6440\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1206 - val_loss: 7.6396\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.1156 - val_loss: 7.6352\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1106 - val_loss: 7.6308\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.1056 - val_loss: 7.6264\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.1006 - val_loss: 7.6220\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0956 - val_loss: 7.6176\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0906 - val_loss: 7.6131\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.0856 - val_loss: 7.6087\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0806 - val_loss: 7.6042\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.0756 - val_loss: 7.5998\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0706 - val_loss: 7.5953\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0655 - val_loss: 7.5908\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0605 - val_loss: 7.5863\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.0555 - val_loss: 7.5818\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0505 - val_loss: 7.5773\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0454 - val_loss: 7.5728\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.0404 - val_loss: 7.5682\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.0354 - val_loss: 7.5637\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.0303 - val_loss: 7.5592\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.0253 - val_loss: 7.5546\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0202 - val_loss: 7.5500\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.0152 - val_loss: 7.5455\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.0101 - val_loss: 7.5409\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0050 - val_loss: 7.5363\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.0000 - val_loss: 7.5317\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.9949 - val_loss: 7.5271\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.9898 - val_loss: 7.5225\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.9847 - val_loss: 7.5178\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9796 - val_loss: 7.5132\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.9745 - val_loss: 7.5086\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.9694 - val_loss: 7.5039\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9643 - val_loss: 7.4993\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9592 - val_loss: 7.4946\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.9541 - val_loss: 7.4899\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.9490 - val_loss: 7.4852\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.9439 - val_loss: 7.4806\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.9388 - val_loss: 7.4759\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.9336 - val_loss: 7.4711\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.9285 - val_loss: 7.4664\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.9233 - val_loss: 7.4617\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.9182 - val_loss: 7.4570\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9131 - val_loss: 7.4522\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9079 - val_loss: 7.4475\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.9027 - val_loss: 7.4427\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.8976 - val_loss: 7.4380\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8924 - val_loss: 7.4332\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.8872 - val_loss: 7.4284\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.8821 - val_loss: 7.4236\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8769 - val_loss: 7.4189\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.8717 - val_loss: 7.4141\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.8665 - val_loss: 7.4093\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.8613 - val_loss: 7.4044\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8561 - val_loss: 7.3996\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.8509 - val_loss: 7.3948\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.8457 - val_loss: 7.3900\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8405 - val_loss: 7.3851\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8352 - val_loss: 7.3803\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.8300 - val_loss: 7.3754\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.8248 - val_loss: 7.3706\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.8195 - val_loss: 7.3657\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8143 - val_loss: 7.3608\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.8091 - val_loss: 7.3559\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.8038 - val_loss: 7.3511\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.7985 - val_loss: 7.3462\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.7933 - val_loss: 7.3413\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7880 - val_loss: 7.3364\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7828 - val_loss: 7.3315\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7775 - val_loss: 7.3265\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7722 - val_loss: 7.3216\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.7669 - val_loss: 7.3167\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 7.7616 - val_loss: 7.3118\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.7563 - val_loss: 7.3068\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.7510 - val_loss: 7.3019\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.7457 - val_loss: 7.2969\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.7404 - val_loss: 7.2920\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.7351 - val_loss: 7.2870\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.7298 - val_loss: 7.2820\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.7245 - val_loss: 7.2771\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.7192 - val_loss: 7.2721\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.7138 - val_loss: 7.2671\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.7085 - val_loss: 7.2621\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.7032 - val_loss: 7.2571\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.6978 - val_loss: 7.2521\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.6925 - val_loss: 7.2471\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.6871 - val_loss: 7.2421\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.6818 - val_loss: 7.2371\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.6764 - val_loss: 7.2321\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.6711 - val_loss: 7.2270\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6657 - val_loss: 7.2220\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.6603 - val_loss: 7.2170\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.6550 - val_loss: 7.2119\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.6496 - val_loss: 7.2069\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.6442 - val_loss: 7.2018\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.6388 - val_loss: 7.1968\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.6334 - val_loss: 7.1917\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6281 - val_loss: 7.1867\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.6227 - val_loss: 7.1816\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.6173 - val_loss: 7.1765\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.6119 - val_loss: 7.1715\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.6065 - val_loss: 7.1664\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.6011 - val_loss: 7.1613\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.5957 - val_loss: 7.1562\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.5902 - val_loss: 7.1511\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5848 - val_loss: 7.1460\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.5794 - val_loss: 7.1409\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.5740 - val_loss: 7.1358\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.5686 - val_loss: 7.1307\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.5631 - val_loss: 7.1256\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.5577 - val_loss: 7.1205\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.5523 - val_loss: 7.1154\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.5468 - val_loss: 7.1103\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.5414 - val_loss: 7.1052\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.5360 - val_loss: 7.1000\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.5305 - val_loss: 7.0949\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5251 - val_loss: 7.0898\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.5196 - val_loss: 7.0847\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5142 - val_loss: 7.0795\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.5087 - val_loss: 7.0744\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.5033 - val_loss: 7.0693\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.4978 - val_loss: 7.0641\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4924 - val_loss: 7.0590\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.4869 - val_loss: 7.0538\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.4814 - val_loss: 7.0487\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.4760 - val_loss: 7.0435\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.4705 - val_loss: 7.0384\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.4650 - val_loss: 7.0332\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.4596 - val_loss: 7.0281\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.4541 - val_loss: 7.0229\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 7.4486 - val_loss: 7.0177\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.4431 - val_loss: 7.0126\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.4377 - val_loss: 7.0074\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.4322 - val_loss: 7.0022\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.4267 - val_loss: 6.9970\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.4213 - val_loss: 6.9918\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.4158 - val_loss: 6.9866\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.4104 - val_loss: 6.9814\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.4049 - val_loss: 6.9762\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.3994 - val_loss: 6.9710\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.3940 - val_loss: 6.9658\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.3885 - val_loss: 6.9607\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.3830 - val_loss: 6.9555\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.3776 - val_loss: 6.9503\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.3721 - val_loss: 6.9451\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.3666 - val_loss: 6.9399\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.3611 - val_loss: 6.9347\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 56.8218 - val_loss: 59.1457\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 56.7418 - val_loss: 59.0259\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 56.6218 - val_loss: 58.8863\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 56.4819 - val_loss: 58.7366\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 56.3320 - val_loss: 58.5820\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 56.1771 - val_loss: 58.4247\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 56.0196 - val_loss: 58.2659\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 55.8605 - val_loss: 58.1060\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 55.7004 - val_loss: 57.9454\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 55.5395 - val_loss: 57.7840\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 55.3778 - val_loss: 57.6217\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 55.2152 - val_loss: 57.4585\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 55.0517 - val_loss: 57.2942\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 54.8872 - val_loss: 57.1288\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 54.7215 - val_loss: 56.9622\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 54.5546 - val_loss: 56.7941\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 54.3862 - val_loss: 56.6245\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 54.2163 - val_loss: 56.4533\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 54.0448 - val_loss: 56.2804\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 53.8715 - val_loss: 56.1055\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 53.6963 - val_loss: 55.9287\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 53.5191 - val_loss: 55.7498\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 53.3398 - val_loss: 55.5686\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 53.1583 - val_loss: 55.3851\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 52.9744 - val_loss: 55.1992\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 52.7880 - val_loss: 55.0107\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 52.5991 - val_loss: 54.8195\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 52.4075 - val_loss: 54.6257\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 52.2132 - val_loss: 54.4289\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 52.0160 - val_loss: 54.2293\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 51.8158 - val_loss: 54.0266\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 51.6126 - val_loss: 53.8208\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 51.4063 - val_loss: 53.6119\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 51.1968 - val_loss: 53.3997\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 50.9840 - val_loss: 53.1842\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 50.7679 - val_loss: 52.9653\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 50.5484 - val_loss: 52.7431\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 50.3255 - val_loss: 52.5173\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 50.0992 - val_loss: 52.2881\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 49.8693 - val_loss: 52.0554\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 49.6358 - val_loss: 51.8191\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 49.3988 - val_loss: 51.5793\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 49.1582 - val_loss: 51.3359\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 48.9140 - val_loss: 51.0889\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 48.6662 - val_loss: 50.8384\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 48.4148 - val_loss: 50.5843\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 48.1599 - val_loss: 50.3267\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 47.9014 - val_loss: 50.0657\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 47.6394 - val_loss: 49.8011\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 47.3739 - val_loss: 49.5333\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 47.1051 - val_loss: 49.2621\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 46.8328 - val_loss: 48.9876\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 46.5573 - val_loss: 48.7100\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 46.2786 - val_loss: 48.4293\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 45.9968 - val_loss: 48.1456\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 45.7120 - val_loss: 47.8591\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 45.4243 - val_loss: 47.5697\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 45.1338 - val_loss: 47.2777\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 44.8405 - val_loss: 46.9831\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 44.5448 - val_loss: 46.6862\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 44.2465 - val_loss: 46.3869\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 43.9460 - val_loss: 46.0854\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 43.6433 - val_loss: 45.7819\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 43.3385 - val_loss: 45.4764\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 43.0318 - val_loss: 45.1692\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 42.7233 - val_loss: 44.8603\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 42.4131 - val_loss: 44.5498\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 42.1014 - val_loss: 44.2379\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 41.7882 - val_loss: 43.9247\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 41.4737 - val_loss: 43.6102\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 41.1580 - val_loss: 43.2946\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 40.8412 - val_loss: 42.9781\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 40.5234 - val_loss: 42.6606\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 40.2047 - val_loss: 42.3423\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 39.8852 - val_loss: 42.0233\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 39.5650 - val_loss: 41.7037\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 39.2441 - val_loss: 41.3835\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 38.9227 - val_loss: 41.0628\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 38.6008 - val_loss: 40.7418\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 38.2786 - val_loss: 40.4204\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 37.9560 - val_loss: 40.0988\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 37.6331 - val_loss: 39.7770\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 37.3102 - val_loss: 39.4552\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 36.9871 - val_loss: 39.1333\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 36.6640 - val_loss: 38.8115\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 36.3409 - val_loss: 38.4898\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36.0180 - val_loss: 38.1684\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 35.6952 - val_loss: 37.8472\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 35.3728 - val_loss: 37.5264\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 35.0507 - val_loss: 37.2060\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 34.7290 - val_loss: 36.8862\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 34.4079 - val_loss: 36.5670\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 34.0874 - val_loss: 36.2486\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 33.7677 - val_loss: 35.9309\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 33.4487 - val_loss: 35.6142\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 33.1307 - val_loss: 35.2984\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 32.8136 - val_loss: 34.9838\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 32.4978 - val_loss: 34.6704\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 32.1831 - val_loss: 34.3583\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 31.8699 - val_loss: 34.0477\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 31.5581 - val_loss: 33.7386\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 31.2479 - val_loss: 33.4312\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 30.9394 - val_loss: 33.1256\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 30.6328 - val_loss: 32.8218\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 30.3281 - val_loss: 32.5201\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 30.0256 - val_loss: 32.2206\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 29.7253 - val_loss: 31.9233\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 29.4274 - val_loss: 31.6283\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 29.1319 - val_loss: 31.3359\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 28.8391 - val_loss: 31.0461\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 28.5491 - val_loss: 30.7591\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 28.2619 - val_loss: 30.4749\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 27.9777 - val_loss: 30.1937\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 27.6966 - val_loss: 29.9156\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 27.4188 - val_loss: 29.6407\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 27.1443 - val_loss: 29.3693\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 26.8733 - val_loss: 29.1013\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 26.6059 - val_loss: 28.8369\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 26.3422 - val_loss: 28.5763\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 26.0823 - val_loss: 28.3196\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 25.8263 - val_loss: 28.0668\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 25.5743 - val_loss: 27.8182\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 25.3265 - val_loss: 27.5737\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 25.0830 - val_loss: 27.3336\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 24.8437 - val_loss: 27.0977\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 24.6090 - val_loss: 26.8663\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 24.3788 - val_loss: 26.6393\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 24.1534 - val_loss: 26.4168\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 23.9328 - val_loss: 26.1986\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 23.7171 - val_loss: 25.9850\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 23.5067 - val_loss: 25.7758\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 23.3015 - val_loss: 25.5710\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 23.1019 - val_loss: 25.3708\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 22.9080 - val_loss: 25.1750\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 22.7203 - val_loss: 24.9840\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 22.5389 - val_loss: 24.7979\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 22.3643 - val_loss: 24.6180\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 22.1969 - val_loss: 24.4447\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 22.0359 - val_loss: 24.2772\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 21.8812 - val_loss: 24.1146\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 21.7331 - val_loss: 23.9569\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 21.5917 - val_loss: 23.8038\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 21.4559 - val_loss: 23.6550\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 21.3249 - val_loss: 23.5103\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 21.1984 - val_loss: 23.3693\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 21.0757 - val_loss: 23.2315\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 20.9565 - val_loss: 23.0968\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 20.8404 - val_loss: 22.9650\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 20.7273 - val_loss: 22.8358\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 20.6169 - val_loss: 22.7093\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20.5092 - val_loss: 22.5853\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 20.4040 - val_loss: 22.4637\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 20.3011 - val_loss: 22.3444\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 20.2005 - val_loss: 22.2274\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 20.1021 - val_loss: 22.1125\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 20.0058 - val_loss: 21.9996\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 19.9116 - val_loss: 21.8888\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 19.8192 - val_loss: 21.7799\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 19.7287 - val_loss: 21.6728\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 19.6400 - val_loss: 21.5675\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 19.5531 - val_loss: 21.4639\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 19.4678 - val_loss: 21.3620\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 19.3841 - val_loss: 21.2616\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 19.3019 - val_loss: 21.1629\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 19.2213 - val_loss: 21.0657\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 19.1420 - val_loss: 20.9699\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 19.0642 - val_loss: 20.8756\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 18.9877 - val_loss: 20.7828\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 18.9124 - val_loss: 20.6913\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18.8385 - val_loss: 20.6011\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 18.7657 - val_loss: 20.5123\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 18.6941 - val_loss: 20.4248\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 18.6236 - val_loss: 20.3385\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 18.5541 - val_loss: 20.2535\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 18.4858 - val_loss: 20.1697\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.4184 - val_loss: 20.0870\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 18.3520 - val_loss: 20.0055\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 18.2865 - val_loss: 19.9251\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 18.2219 - val_loss: 19.8458\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 18.1582 - val_loss: 19.7675\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 18.0953 - val_loss: 19.6903\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 18.0333 - val_loss: 19.6141\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 17.9719 - val_loss: 19.5389\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17.9113 - val_loss: 19.4646\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17.8515 - val_loss: 19.3912\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17.7922 - val_loss: 19.3188\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 17.7337 - val_loss: 19.2471\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 17.6757 - val_loss: 19.1764\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17.6184 - val_loss: 19.1064\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 17.5616 - val_loss: 19.0372\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.5053 - val_loss: 18.9688\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 17.4496 - val_loss: 18.9011\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.3944 - val_loss: 18.8342\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.3396 - val_loss: 18.7679\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17.2853 - val_loss: 18.7022\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17.2315 - val_loss: 18.6372\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 17.1781 - val_loss: 18.5729\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17.1250 - val_loss: 18.5091\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 17.0724 - val_loss: 18.4459\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17.0201 - val_loss: 18.3832\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 16.9682 - val_loss: 18.3210\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 16.9165 - val_loss: 18.2594\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 16.8652 - val_loss: 18.1982\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 16.8142 - val_loss: 18.1375\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 16.7635 - val_loss: 18.0772\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 16.7130 - val_loss: 18.0173\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 16.6628 - val_loss: 17.9578\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 16.6127 - val_loss: 17.8987\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.5629 - val_loss: 17.8399\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.5133 - val_loss: 17.7815\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.4638 - val_loss: 17.7233\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 16.4145 - val_loss: 17.6655\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.3652 - val_loss: 17.6079\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.3161 - val_loss: 17.5505\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 16.2670 - val_loss: 17.4933\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 16.2180 - val_loss: 17.4363\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 16.1691 - val_loss: 17.3794\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.1201 - val_loss: 17.3227\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.0711 - val_loss: 17.2661\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.0220 - val_loss: 17.2095\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 15.9728 - val_loss: 17.1530\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 15.9235 - val_loss: 17.0964\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 15.8740 - val_loss: 17.0398\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.8243 - val_loss: 16.9831\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 15.7743 - val_loss: 16.9263\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 15.7241 - val_loss: 16.8693\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.6735 - val_loss: 16.8121\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 15.6224 - val_loss: 16.7546\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 15.5710 - val_loss: 16.6968\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 15.5190 - val_loss: 16.6387\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 15.4665 - val_loss: 16.5801\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 15.4133 - val_loss: 16.5211\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 15.3595 - val_loss: 16.4615\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.3049 - val_loss: 16.4013\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 15.2495 - val_loss: 16.3405\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 15.1932 - val_loss: 16.2791\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 15.1361 - val_loss: 16.2169\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.0779 - val_loss: 16.1540\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 15.0189 - val_loss: 16.0903\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 14.9588 - val_loss: 16.0259\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 14.8977 - val_loss: 15.9607\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.8356 - val_loss: 15.8948\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 14.7726 - val_loss: 15.8283\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 14.7087 - val_loss: 15.7611\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.6440 - val_loss: 15.6933\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 14.5786 - val_loss: 15.6251\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 14.5126 - val_loss: 15.5566\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 14.4461 - val_loss: 15.4879\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 14.3793 - val_loss: 15.4191\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 14.3124 - val_loss: 15.3504\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 14.2455 - val_loss: 15.2818\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 14.1787 - val_loss: 15.2135\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 14.1123 - val_loss: 15.1456\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 14.0462 - val_loss: 15.0781\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 13.9807 - val_loss: 15.0111\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 13.9157 - val_loss: 14.9448\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.8514 - val_loss: 14.8789\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 13.7878 - val_loss: 14.8137\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 13.7249 - val_loss: 14.7491\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 13.6627 - val_loss: 14.6850\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 13.6012 - val_loss: 14.6214\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 13.5402 - val_loss: 14.5584\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 13.4799 - val_loss: 14.4957\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 13.4200 - val_loss: 14.4334\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.3605 - val_loss: 14.3714\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.3014 - val_loss: 14.3097\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 13.2426 - val_loss: 14.2481\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 13.1840 - val_loss: 14.1867\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 13.1254 - val_loss: 14.1253\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 13.0669 - val_loss: 14.0640\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 13.0084 - val_loss: 14.0025\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 12.9497 - val_loss: 13.9410\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 12.8907 - val_loss: 13.8793\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 12.8315 - val_loss: 13.8174\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 12.7719 - val_loss: 13.7552\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 12.7118 - val_loss: 13.6927\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 12.6513 - val_loss: 13.6298\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 12.5901 - val_loss: 13.5665\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 12.5282 - val_loss: 13.5027\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 12.4657 - val_loss: 13.4385\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 12.4024 - val_loss: 13.3737\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 12.3382 - val_loss: 13.3084\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 12.2732 - val_loss: 13.2426\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 12.2072 - val_loss: 13.1762\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 12.1404 - val_loss: 13.1093\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 12.0726 - val_loss: 13.0419\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 12.0038 - val_loss: 12.9740\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 11.9342 - val_loss: 12.9056\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 11.8636 - val_loss: 12.8368\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 11.7923 - val_loss: 12.7677\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.7201 - val_loss: 12.6982\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 11.6473 - val_loss: 12.6287\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 11.5740 - val_loss: 12.5590\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 11.5002 - val_loss: 12.4893\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 11.4262 - val_loss: 12.4197\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 11.3522 - val_loss: 12.3504\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.2783 - val_loss: 12.2815\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 11.2046 - val_loss: 12.2129\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 11.1313 - val_loss: 12.1449\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 11.0584 - val_loss: 12.0775\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 10.9861 - val_loss: 12.0107\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 10.9148 - val_loss: 11.9446\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 10.8445 - val_loss: 11.8792\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 10.7751 - val_loss: 11.8147\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 10.7067 - val_loss: 11.7509\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 10.6394 - val_loss: 11.6881\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.5733 - val_loss: 11.6261\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 10.5084 - val_loss: 11.5651\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 10.4448 - val_loss: 11.5052\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.3826 - val_loss: 11.4463\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.3217 - val_loss: 11.3886\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 10.2621 - val_loss: 11.3322\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 10.2039 - val_loss: 11.2769\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.1470 - val_loss: 11.2229\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.0914 - val_loss: 11.1700\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.0373 - val_loss: 11.1182\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 9.9845 - val_loss: 11.0676\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 9.9331 - val_loss: 11.0181\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.8828 - val_loss: 10.9697\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.8338 - val_loss: 10.9224\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 9.7860 - val_loss: 10.8762\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 9.7394 - val_loss: 10.8311\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 9.6940 - val_loss: 10.7870\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.6497 - val_loss: 10.7440\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.6065 - val_loss: 10.7020\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.5644 - val_loss: 10.6610\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 9.5233 - val_loss: 10.6210\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.4832 - val_loss: 10.5821\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 9.4442 - val_loss: 10.5441\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.4061 - val_loss: 10.5072\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 9.3689 - val_loss: 10.4712\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.3326 - val_loss: 10.4362\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 9.2972 - val_loss: 10.4021\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.2627 - val_loss: 10.3689\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 9.2290 - val_loss: 10.3366\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 9.1962 - val_loss: 10.3051\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 9.1642 - val_loss: 10.2745\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 9.1330 - val_loss: 10.2446\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 9.1026 - val_loss: 10.2155\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 9.0730 - val_loss: 10.1872\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 9.0443 - val_loss: 10.1595\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.0163 - val_loss: 10.1326\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.9890 - val_loss: 10.1065\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.9625 - val_loss: 10.0810\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.9366 - val_loss: 10.0562\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.9114 - val_loss: 10.0321\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.8869 - val_loss: 10.0086\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.8630 - val_loss: 9.9857\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.8398 - val_loss: 9.9635\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 8.8172 - val_loss: 9.9419\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 8.7952 - val_loss: 9.9209\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.7738 - val_loss: 9.9004\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.7530 - val_loss: 9.8805\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.7327 - val_loss: 9.8611\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.7131 - val_loss: 9.8422\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.6940 - val_loss: 9.8239\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.6754 - val_loss: 9.8060\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.6574 - val_loss: 9.7886\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.6399 - val_loss: 9.7717\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.6228 - val_loss: 9.7553\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.6063 - val_loss: 9.7393\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5902 - val_loss: 9.7237\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5746 - val_loss: 9.7085\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.5594 - val_loss: 9.6938\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5446 - val_loss: 9.6794\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.5303 - val_loss: 9.6654\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.5164 - val_loss: 9.6517\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.5028 - val_loss: 9.6384\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.4897 - val_loss: 9.6254\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 8.4769 - val_loss: 9.6128\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.4645 - val_loss: 9.6005\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.4524 - val_loss: 9.5884\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.4407 - val_loss: 9.5767\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.4293 - val_loss: 9.5652\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4182 - val_loss: 9.5541\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4075 - val_loss: 9.5431\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.3970 - val_loss: 9.5325\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.3868 - val_loss: 9.5220\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3769 - val_loss: 9.5118\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.3673 - val_loss: 9.5019\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3580 - val_loss: 9.4921\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.3489 - val_loss: 9.4826\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 8.3400 - val_loss: 9.4732\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.3314 - val_loss: 9.4641\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.3230 - val_loss: 9.4551\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.3149 - val_loss: 9.4463\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.3069 - val_loss: 9.4377\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2992 - val_loss: 9.4293\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.2916 - val_loss: 9.4210\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2843 - val_loss: 9.4129\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2771 - val_loss: 9.4049\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2701 - val_loss: 9.3971\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2633 - val_loss: 9.3894\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.2567 - val_loss: 9.3818\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.2502 - val_loss: 9.3744\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.2439 - val_loss: 9.3671\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.2377 - val_loss: 9.3600\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.2316 - val_loss: 9.3529\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.2257 - val_loss: 9.3460\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.2199 - val_loss: 9.3391\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.2142 - val_loss: 9.3324\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.2086 - val_loss: 9.3257\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.2031 - val_loss: 9.3192\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.1978 - val_loss: 9.3127\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.1925 - val_loss: 9.3063\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.1872 - val_loss: 9.3000\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.1821 - val_loss: 9.2937\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.1770 - val_loss: 9.2875\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.1720 - val_loss: 9.2814\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.1671 - val_loss: 9.2754\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1622 - val_loss: 9.2694\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.1574 - val_loss: 9.2634\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 8.1527 - val_loss: 9.2575\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.1480 - val_loss: 9.2517\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.1433 - val_loss: 9.2458\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.1387 - val_loss: 9.2400\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.1341 - val_loss: 9.2343\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.1295 - val_loss: 9.2285\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.1250 - val_loss: 9.2228\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.1205 - val_loss: 9.2171\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.1160 - val_loss: 9.2115\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.1116 - val_loss: 9.2058\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.1072 - val_loss: 9.2002\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.1028 - val_loss: 9.1946\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.0984 - val_loss: 9.1890\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.0940 - val_loss: 9.1834\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0896 - val_loss: 9.1779\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 8.0853 - val_loss: 9.1723\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.0809 - val_loss: 9.1668\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.0766 - val_loss: 9.1613\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.0723 - val_loss: 9.1558\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.0680 - val_loss: 9.1503\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0638 - val_loss: 9.1449\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 8.0595 - val_loss: 9.1394\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.0553 - val_loss: 9.1340\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.0510 - val_loss: 9.1285\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.0468 - val_loss: 9.1231\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0425 - val_loss: 9.1177\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.0383 - val_loss: 9.1123\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0340 - val_loss: 9.1068\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0298 - val_loss: 9.1014\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0255 - val_loss: 9.0960\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.0212 - val_loss: 9.0906\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0170 - val_loss: 9.0851\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.0127 - val_loss: 9.0797\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.0084 - val_loss: 9.0743\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.0042 - val_loss: 9.0688\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.9999 - val_loss: 9.0634\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9956 - val_loss: 9.0580\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.9913 - val_loss: 9.0526\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.9870 - val_loss: 9.0471\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9827 - val_loss: 9.0417\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.9784 - val_loss: 9.0363\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9741 - val_loss: 9.0309\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.9698 - val_loss: 9.0255\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.9655 - val_loss: 9.0200\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.9611 - val_loss: 9.0147\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9568 - val_loss: 9.0093\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.9525 - val_loss: 9.0039\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9481 - val_loss: 8.9986\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9438 - val_loss: 8.9932\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.9394 - val_loss: 8.9878\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.9351 - val_loss: 8.9825\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.9307 - val_loss: 8.9771\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.9263 - val_loss: 8.9717\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.9219 - val_loss: 8.9663\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 7.9176 - val_loss: 8.9610\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.9132 - val_loss: 8.9556\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.9088 - val_loss: 8.9502\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.9044 - val_loss: 8.9448\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9000 - val_loss: 8.9394\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8955 - val_loss: 8.9341\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.8911 - val_loss: 8.9287\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8867 - val_loss: 8.9233\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.8823 - val_loss: 8.9179\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.8778 - val_loss: 8.9125\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.8734 - val_loss: 8.9071\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8690 - val_loss: 8.9017\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.8645 - val_loss: 8.8964\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.8601 - val_loss: 8.8910\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.8556 - val_loss: 8.8856\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8512 - val_loss: 8.8803\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.8468 - val_loss: 8.8749\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.8423 - val_loss: 8.8696\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8379 - val_loss: 8.8643\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.8334 - val_loss: 8.8589\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.8290 - val_loss: 8.8536\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.8245 - val_loss: 8.8482\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.8201 - val_loss: 8.8429\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.8156 - val_loss: 8.8375\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.8111 - val_loss: 8.8322\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.8067 - val_loss: 8.8268\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.8022 - val_loss: 8.8215\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.7977 - val_loss: 8.8161\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.7932 - val_loss: 8.8107\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7888 - val_loss: 8.8054\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.7843 - val_loss: 8.8000\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.7798 - val_loss: 8.7947\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.7753 - val_loss: 8.7893\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.7708 - val_loss: 8.7839\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.7663 - val_loss: 8.7786\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.7618 - val_loss: 8.7732\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7573 - val_loss: 8.7679\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.7528 - val_loss: 8.7625\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7483 - val_loss: 8.7571\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.7438 - val_loss: 8.7517\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.7393 - val_loss: 8.7464\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.7348 - val_loss: 8.7410\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.7302 - val_loss: 8.7356\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.7257 - val_loss: 8.7302\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.7212 - val_loss: 8.7248\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.7167 - val_loss: 8.7195\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.7121 - val_loss: 8.7141\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.7076 - val_loss: 8.7087\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.7031 - val_loss: 8.7033\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.6986 - val_loss: 8.6979\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.6940 - val_loss: 8.6925\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.6895 - val_loss: 8.6871\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.6850 - val_loss: 8.6817\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.6804 - val_loss: 8.6763\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.6759 - val_loss: 8.6709\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.6713 - val_loss: 8.6655\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6668 - val_loss: 8.6601\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.6623 - val_loss: 8.6548\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.6577 - val_loss: 8.6494\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.6532 - val_loss: 8.6440\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.6487 - val_loss: 8.6387\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.6442 - val_loss: 8.6333\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.6397 - val_loss: 8.6279\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.6351 - val_loss: 8.6226\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6306 - val_loss: 8.6172\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.6261 - val_loss: 8.6118\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.6216 - val_loss: 8.6065\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 7.6171 - val_loss: 8.6011\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.6125 - val_loss: 8.5957\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.6080 - val_loss: 8.5903\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.6035 - val_loss: 8.5849\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7.5990 - val_loss: 8.5796\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.5944 - val_loss: 8.5742\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.5899 - val_loss: 8.5688\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5854 - val_loss: 8.5634\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5808 - val_loss: 8.5580\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.5763 - val_loss: 8.5526\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.5718 - val_loss: 8.5472\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.5672 - val_loss: 8.5418\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.5627 - val_loss: 8.5364\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5582 - val_loss: 8.5311\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.5536 - val_loss: 8.5257\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.5491 - val_loss: 8.5203\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.5445 - val_loss: 8.5149\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5400 - val_loss: 8.5094\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5355 - val_loss: 8.5040\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.5309 - val_loss: 8.4986\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.5264 - val_loss: 8.4932\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.5218 - val_loss: 8.4878\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.5173 - val_loss: 8.4824\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.5127 - val_loss: 8.4770\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.5082 - val_loss: 8.4716\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.5036 - val_loss: 8.4661\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.4991 - val_loss: 8.4607\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.4945 - val_loss: 8.4553\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.4900 - val_loss: 8.4499\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.4854 - val_loss: 8.4444\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.4809 - val_loss: 8.4390\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.4763 - val_loss: 8.4336\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.4717 - val_loss: 8.4281\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4672 - val_loss: 8.4227\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.4626 - val_loss: 8.4172\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.4581 - val_loss: 8.4118\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.4535 - val_loss: 8.4064\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.4489 - val_loss: 8.4009\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.4444 - val_loss: 8.3955\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.4398 - val_loss: 8.3900\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4352 - val_loss: 8.3846\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.4307 - val_loss: 8.3791\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.4261 - val_loss: 8.3737\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.4215 - val_loss: 8.3682\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.4170 - val_loss: 8.3627\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.4124 - val_loss: 8.3573\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.4078 - val_loss: 8.3518\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.4032 - val_loss: 8.3464\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.3987 - val_loss: 8.3409\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.3941 - val_loss: 8.3354\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 7.3895 - val_loss: 8.3300\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.3850 - val_loss: 8.3245\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.3804 - val_loss: 8.3190\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.3758 - val_loss: 8.3136\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.3712 - val_loss: 8.3081\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.3667 - val_loss: 8.3026\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.3621 - val_loss: 8.2972\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.3575 - val_loss: 8.2917\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.3530 - val_loss: 8.2862\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.3484 - val_loss: 8.2808\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.3438 - val_loss: 8.2753\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.3392 - val_loss: 8.2698\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.3347 - val_loss: 8.2644\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.3301 - val_loss: 8.2589\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.3255 - val_loss: 8.2534\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 7.3210 - val_loss: 8.2479\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.3164 - val_loss: 8.2425\n",
      "Epoch 1/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 56.0565\n",
      "Epoch 2/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 55.8568\n",
      "Epoch 3/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 55.5777\n",
      "Epoch 4/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 55.2743\n",
      "Epoch 5/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 54.9597\n",
      "Epoch 6/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 54.6365\n",
      "Epoch 7/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 54.3047\n",
      "Epoch 8/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 53.9635\n",
      "Epoch 9/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 53.6118\n",
      "Epoch 10/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 53.2489\n",
      "Epoch 11/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 52.8741\n",
      "Epoch 12/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 52.4865\n",
      "Epoch 13/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 52.0854\n",
      "Epoch 14/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 51.6704\n",
      "Epoch 15/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 51.48 - 0s 3ms/step - loss: 51.2410\n",
      "Epoch 16/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 50.7969\n",
      "Epoch 17/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 50.3378\n",
      "Epoch 18/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 49.8635\n",
      "Epoch 19/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 49.3735\n",
      "Epoch 20/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.8682\n",
      "Epoch 21/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 48.3475\n",
      "Epoch 22/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 47.8117\n",
      "Epoch 23/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 47.2612\n",
      "Epoch 24/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 46.6963\n",
      "Epoch 25/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 46.1177\n",
      "Epoch 26/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 45.5260\n",
      "Epoch 27/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 44.9219\n",
      "Epoch 28/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 44.3061\n",
      "Epoch 29/600\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 43.6792\n",
      "Epoch 30/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 43.0421\n",
      "Epoch 31/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 42.3959\n",
      "Epoch 32/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 41.7412\n",
      "Epoch 33/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 41.0791\n",
      "Epoch 34/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 40.4096\n",
      "Epoch 35/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 39.7338\n",
      "Epoch 36/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 39.0526\n",
      "Epoch 37/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 38.3672\n",
      "Epoch 38/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 37.6786\n",
      "Epoch 39/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 36.9872\n",
      "Epoch 40/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 36.2931\n",
      "Epoch 41/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 35.5966\n",
      "Epoch 42/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 34.8996\n",
      "Epoch 43/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 34.2045\n",
      "Epoch 44/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.5138\n",
      "Epoch 45/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 32.8288\n",
      "Epoch 46/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 32.1491\n",
      "Epoch 47/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 31.4751\n",
      "Epoch 48/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.8084\n",
      "Epoch 49/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 30.1502\n",
      "Epoch 50/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 29.5053\n",
      "Epoch 51/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 28.8720\n",
      "Epoch 52/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 28.2560\n",
      "Epoch 53/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 27.6548\n",
      "Epoch 54/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 27.0703\n",
      "Epoch 55/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 26.5048\n",
      "Epoch 56/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.9612\n",
      "Epoch 57/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.4406\n",
      "Epoch 58/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.9462\n",
      "Epoch 59/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.4701\n",
      "Epoch 60/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 24.0182\n",
      "Epoch 61/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.5875\n",
      "Epoch 62/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.1850\n",
      "Epoch 63/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.8080\n",
      "Epoch 64/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.4625\n",
      "Epoch 65/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1424\n",
      "Epoch 66/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.8503\n",
      "Epoch 67/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5766\n",
      "Epoch 68/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3210\n",
      "Epoch 69/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.0754\n",
      "Epoch 70/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.8410\n",
      "Epoch 71/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 20.6135\n",
      "Epoch 72/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.3992\n",
      "Epoch 73/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.1988\n",
      "Epoch 74/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.0074\n",
      "Epoch 75/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.8215\n",
      "Epoch 76/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.6378\n",
      "Epoch 77/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.4588\n",
      "Epoch 78/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.2884\n",
      "Epoch 79/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.1237\n",
      "Epoch 80/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.9692\n",
      "Epoch 81/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.8151\n",
      "Epoch 82/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.6713\n",
      "Epoch 83/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.5288\n",
      "Epoch 84/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 18.3929\n",
      "Epoch 85/600\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 18.2583\n",
      "Epoch 86/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 18.1267\n",
      "Epoch 87/600\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 17.9982\n",
      "Epoch 88/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.8718\n",
      "Epoch 89/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.7477\n",
      "Epoch 90/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.6285\n",
      "Epoch 91/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.5119\n",
      "Epoch 92/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.3994\n",
      "Epoch 93/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.2866\n",
      "Epoch 94/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.1752\n",
      "Epoch 95/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.0628\n",
      "Epoch 96/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.9502\n",
      "Epoch 97/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.8373\n",
      "Epoch 98/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.7270\n",
      "Epoch 99/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.6177\n",
      "Epoch 100/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.5099\n",
      "Epoch 101/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.4003\n",
      "Epoch 102/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.2899\n",
      "Epoch 103/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.1804\n",
      "Epoch 104/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.0707\n",
      "Epoch 105/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.9601\n",
      "Epoch 106/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.8483\n",
      "Epoch 107/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.7355\n",
      "Epoch 108/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.6222\n",
      "Epoch 109/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.5088\n",
      "Epoch 110/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 15.3938\n",
      "Epoch 111/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.2774\n",
      "Epoch 112/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.1572\n",
      "Epoch 113/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 15.0362\n",
      "Epoch 114/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.9107\n",
      "Epoch 115/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.7815\n",
      "Epoch 116/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.6473\n",
      "Epoch 117/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.5091\n",
      "Epoch 118/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.3657\n",
      "Epoch 119/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 14.2163\n",
      "Epoch 120/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 14.0619\n",
      "Epoch 121/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.9010\n",
      "Epoch 122/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.7343\n",
      "Epoch 123/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.5613\n",
      "Epoch 124/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.3836\n",
      "Epoch 125/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.2020\n",
      "Epoch 126/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 13.0169\n",
      "Epoch 127/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.8282\n",
      "Epoch 128/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.6361\n",
      "Epoch 129/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.4417\n",
      "Epoch 130/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.2458\n",
      "Epoch 131/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 12.0535\n",
      "Epoch 132/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.8640\n",
      "Epoch 133/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 11.6788\n",
      "Epoch 134/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 11.4959\n",
      "Epoch 135/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 11.3180\n",
      "Epoch 136/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 11.15 - 0s 2ms/step - loss: 11.1456\n",
      "Epoch 137/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.9814\n",
      "Epoch 138/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.8243\n",
      "Epoch 139/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.6742\n",
      "Epoch 140/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.5291\n",
      "Epoch 141/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 10.3901\n",
      "Epoch 142/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.2572\n",
      "Epoch 143/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.1322\n",
      "Epoch 144/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 10.0134\n",
      "Epoch 145/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.9009\n",
      "Epoch 146/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.7939\n",
      "Epoch 147/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.6928\n",
      "Epoch 148/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 9.5971\n",
      "Epoch 149/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.5076\n",
      "Epoch 150/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.4236\n",
      "Epoch 151/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.3461\n",
      "Epoch 152/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.2733\n",
      "Epoch 153/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.2046\n",
      "Epoch 154/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 9.1398\n",
      "Epoch 155/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.0785\n",
      "Epoch 156/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.0214\n",
      "Epoch 157/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.9677\n",
      "Epoch 158/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.9169\n",
      "Epoch 159/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.8683\n",
      "Epoch 160/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.8226\n",
      "Epoch 161/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.7807\n",
      "Epoch 162/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.7413\n",
      "Epoch 163/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.7054\n",
      "Epoch 164/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.6729\n",
      "Epoch 165/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.6422\n",
      "Epoch 166/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.6130\n",
      "Epoch 167/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5844\n",
      "Epoch 168/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.5566\n",
      "Epoch 169/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.5307\n",
      "Epoch 170/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.5069\n",
      "Epoch 171/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.4842\n",
      "Epoch 172/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4626\n",
      "Epoch 173/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.4420\n",
      "Epoch 174/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4227\n",
      "Epoch 175/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4047\n",
      "Epoch 176/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.3875\n",
      "Epoch 177/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.3708\n",
      "Epoch 178/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.3541\n",
      "Epoch 179/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.3381\n",
      "Epoch 180/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.3229\n",
      "Epoch 181/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.307 - 0s 4ms/step - loss: 8.3079\n",
      "Epoch 182/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.2932\n",
      "Epoch 183/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.2784\n",
      "Epoch 184/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2644\n",
      "Epoch 185/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 8.102 - 0s 3ms/step - loss: 8.2504\n",
      "Epoch 186/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2365\n",
      "Epoch 187/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.2228\n",
      "Epoch 188/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.2097\n",
      "Epoch 189/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.1967\n",
      "Epoch 190/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.1839\n",
      "Epoch 191/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1708\n",
      "Epoch 192/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.1582\n",
      "Epoch 193/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.1455\n",
      "Epoch 194/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1329\n",
      "Epoch 195/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1200\n",
      "Epoch 196/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.1067\n",
      "Epoch 197/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0936\n",
      "Epoch 198/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0806\n",
      "Epoch 199/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0677\n",
      "Epoch 200/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0550\n",
      "Epoch 201/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0416\n",
      "Epoch 202/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0278\n",
      "Epoch 203/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 8.0141\n",
      "Epoch 204/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.0003\n",
      "Epoch 205/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9865\n",
      "Epoch 206/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9728\n",
      "Epoch 207/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9590\n",
      "Epoch 208/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9453\n",
      "Epoch 209/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9316\n",
      "Epoch 210/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9181\n",
      "Epoch 211/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.9044\n",
      "Epoch 212/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8905\n",
      "Epoch 213/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8764\n",
      "Epoch 214/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8624\n",
      "Epoch 215/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8484\n",
      "Epoch 216/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8347\n",
      "Epoch 217/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8210\n",
      "Epoch 218/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.8073\n",
      "Epoch 219/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.7926\n",
      "Epoch 220/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7783\n",
      "Epoch 221/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7641\n",
      "Epoch 222/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 7.691 - 0s 3ms/step - loss: 7.7497\n",
      "Epoch 223/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.7358\n",
      "Epoch 224/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7217\n",
      "Epoch 225/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.7079\n",
      "Epoch 226/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6939\n",
      "Epoch 227/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.6800\n",
      "Epoch 228/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6661\n",
      "Epoch 229/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6529\n",
      "Epoch 230/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6384\n",
      "Epoch 231/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.6244\n",
      "Epoch 232/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.6102\n",
      "Epoch 233/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5969\n",
      "Epoch 234/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5838\n",
      "Epoch 235/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.5690\n",
      "Epoch 236/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5550\n",
      "Epoch 237/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5411\n",
      "Epoch 238/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5271\n",
      "Epoch 239/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5134\n",
      "Epoch 240/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.4994\n",
      "Epoch 241/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.4856\n",
      "Epoch 242/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4721\n",
      "Epoch 243/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4586\n",
      "Epoch 244/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 7.4451\n",
      "Epoch 245/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.4318\n",
      "Epoch 246/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.4190\n",
      "Epoch 247/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4051\n",
      "Epoch 248/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3909\n",
      "Epoch 249/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3767\n",
      "Epoch 250/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3627\n",
      "Epoch 251/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3493\n",
      "Epoch 252/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3359\n",
      "Epoch 253/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3226\n",
      "Epoch 254/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3093\n",
      "Epoch 255/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2959\n",
      "Epoch 256/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.2823\n",
      "Epoch 257/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2689\n",
      "Epoch 258/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2555\n",
      "Epoch 259/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2418\n",
      "Epoch 260/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2285\n",
      "Epoch 261/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2148\n",
      "Epoch 262/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.2004\n",
      "Epoch 263/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1864\n",
      "Epoch 264/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1725\n",
      "Epoch 265/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1585\n",
      "Epoch 266/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.1452\n",
      "Epoch 267/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.1310\n",
      "Epoch 268/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.1176\n",
      "Epoch 269/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1041\n",
      "Epoch 270/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0905\n",
      "Epoch 271/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0764\n",
      "Epoch 272/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0624\n",
      "Epoch 273/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 7.0486\n",
      "Epoch 274/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0352\n",
      "Epoch 275/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0217\n",
      "Epoch 276/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.0079\n",
      "Epoch 277/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.9941\n",
      "Epoch 278/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.9803\n",
      "Epoch 279/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.9666\n",
      "Epoch 280/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.9530\n",
      "Epoch 281/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.9389\n",
      "Epoch 282/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.9252\n",
      "Epoch 283/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 6.845 - 0s 4ms/step - loss: 6.9105\n",
      "Epoch 284/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 6.813 - 0s 4ms/step - loss: 6.8962\n",
      "Epoch 285/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.8821\n",
      "Epoch 286/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.8682\n",
      "Epoch 287/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.8543\n",
      "Epoch 288/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.8406\n",
      "Epoch 289/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.8266\n",
      "Epoch 290/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.8131\n",
      "Epoch 291/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.7992\n",
      "Epoch 292/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.7853\n",
      "Epoch 293/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7715\n",
      "Epoch 294/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7571\n",
      "Epoch 295/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7432\n",
      "Epoch 296/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7295\n",
      "Epoch 297/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7158\n",
      "Epoch 298/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.7021\n",
      "Epoch 299/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6882\n",
      "Epoch 300/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.6742\n",
      "Epoch 301/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6603\n",
      "Epoch 302/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6466\n",
      "Epoch 303/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6329\n",
      "Epoch 304/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6197\n",
      "Epoch 305/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6058\n",
      "Epoch 306/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.5922\n",
      "Epoch 307/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.5779\n",
      "Epoch 308/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.5637\n",
      "Epoch 309/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.5501\n",
      "Epoch 310/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.5364\n",
      "Epoch 311/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.5227\n",
      "Epoch 312/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.5092\n",
      "Epoch 313/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.4955\n",
      "Epoch 314/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.4824\n",
      "Epoch 315/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 6.4680\n",
      "Epoch 316/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4547\n",
      "Epoch 317/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4410\n",
      "Epoch 318/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4275\n",
      "Epoch 319/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4138\n",
      "Epoch 320/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4005\n",
      "Epoch 321/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.3868\n",
      "Epoch 322/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.3734\n",
      "Epoch 323/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.3601\n",
      "Epoch 324/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.3469\n",
      "Epoch 325/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.3335\n",
      "Epoch 326/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.3207\n",
      "Epoch 327/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.3078\n",
      "Epoch 328/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2938\n",
      "Epoch 329/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2806\n",
      "Epoch 330/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2674\n",
      "Epoch 331/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2544\n",
      "Epoch 332/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2414\n",
      "Epoch 333/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2281\n",
      "Epoch 334/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2154\n",
      "Epoch 335/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.2026\n",
      "Epoch 336/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1896\n",
      "Epoch 337/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1761\n",
      "Epoch 338/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1631\n",
      "Epoch 339/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1501\n",
      "Epoch 340/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 6.135 - 0s 3ms/step - loss: 6.1378\n",
      "Epoch 341/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1244\n",
      "Epoch 342/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1119\n",
      "Epoch 343/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0983\n",
      "Epoch 344/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0847\n",
      "Epoch 345/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0716\n",
      "Epoch 346/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0590\n",
      "Epoch 347/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0461\n",
      "Epoch 348/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 6.102 - 0s 3ms/step - loss: 6.0333\n",
      "Epoch 349/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.0215\n",
      "Epoch 350/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.0083\n",
      "Epoch 351/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9958\n",
      "Epoch 352/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9831\n",
      "Epoch 353/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9714\n",
      "Epoch 354/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9583\n",
      "Epoch 355/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.9458\n",
      "Epoch 356/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9331\n",
      "Epoch 357/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9205\n",
      "Epoch 358/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9082\n",
      "Epoch 359/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8955\n",
      "Epoch 360/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8833\n",
      "Epoch 361/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.8710\n",
      "Epoch 362/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8591\n",
      "Epoch 363/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8468\n",
      "Epoch 364/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8351\n",
      "Epoch 365/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 5.884 - 0s 3ms/step - loss: 5.8231\n",
      "Epoch 366/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.8110\n",
      "Epoch 367/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7994\n",
      "Epoch 368/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.7883\n",
      "Epoch 369/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7768\n",
      "Epoch 370/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7646\n",
      "Epoch 371/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.7525\n",
      "Epoch 372/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7405\n",
      "Epoch 373/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.7285\n",
      "Epoch 374/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.7164\n",
      "Epoch 375/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.7047\n",
      "Epoch 376/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 5.6928\n",
      "Epoch 377/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6810\n",
      "Epoch 378/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6693\n",
      "Epoch 379/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6583\n",
      "Epoch 380/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.6472\n",
      "Epoch 381/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6358\n",
      "Epoch 382/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.6243\n",
      "Epoch 383/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.6130\n",
      "Epoch 384/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.6017\n",
      "Epoch 385/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.5904\n",
      "Epoch 386/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5795\n",
      "Epoch 387/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5681\n",
      "Epoch 388/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5574\n",
      "Epoch 389/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5464\n",
      "Epoch 390/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.5353\n",
      "Epoch 391/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5242\n",
      "Epoch 392/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.5133\n",
      "Epoch 393/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.5022\n",
      "Epoch 394/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.4919\n",
      "Epoch 395/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4808\n",
      "Epoch 396/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4704\n",
      "Epoch 397/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4601\n",
      "Epoch 398/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.4494\n",
      "Epoch 399/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.4387\n",
      "Epoch 400/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.4282\n",
      "Epoch 401/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4175\n",
      "Epoch 402/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4070\n",
      "Epoch 403/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3964\n",
      "Epoch 404/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3865\n",
      "Epoch 405/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.3751\n",
      "Epoch 406/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3646\n",
      "Epoch 407/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3543\n",
      "Epoch 408/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3447\n",
      "Epoch 409/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3350\n",
      "Epoch 410/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3249\n",
      "Epoch 411/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3148\n",
      "Epoch 412/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 5.297 - 0s 3ms/step - loss: 5.3041\n",
      "Epoch 413/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2940\n",
      "Epoch 414/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.2842\n",
      "Epoch 415/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.2742\n",
      "Epoch 416/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.2643\n",
      "Epoch 417/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2543\n",
      "Epoch 418/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.2444\n",
      "Epoch 419/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2349\n",
      "Epoch 420/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2252\n",
      "Epoch 421/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2158\n",
      "Epoch 422/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2066\n",
      "Epoch 423/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1974\n",
      "Epoch 424/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1878\n",
      "Epoch 425/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1782\n",
      "Epoch 426/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1687\n",
      "Epoch 427/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1589\n",
      "Epoch 428/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1491\n",
      "Epoch 429/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1397\n",
      "Epoch 430/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 5.1302\n",
      "Epoch 431/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.1206\n",
      "Epoch 432/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1112\n",
      "Epoch 433/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1017\n",
      "Epoch 434/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0924\n",
      "Epoch 435/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0827\n",
      "Epoch 436/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0736\n",
      "Epoch 437/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0651\n",
      "Epoch 438/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0569\n",
      "Epoch 439/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0475\n",
      "Epoch 440/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0381\n",
      "Epoch 441/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0288\n",
      "Epoch 442/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 5.0199\n",
      "Epoch 443/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.0112\n",
      "Epoch 444/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0026\n",
      "Epoch 445/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9943\n",
      "Epoch 446/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9860\n",
      "Epoch 447/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.9774\n",
      "Epoch 448/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9688\n",
      "Epoch 449/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9602\n",
      "Epoch 450/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9515\n",
      "Epoch 451/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9428\n",
      "Epoch 452/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9342\n",
      "Epoch 453/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9256\n",
      "Epoch 454/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9172\n",
      "Epoch 455/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9089\n",
      "Epoch 456/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.9006\n",
      "Epoch 457/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8923\n",
      "Epoch 458/600\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 4.8845\n",
      "Epoch 459/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8763\n",
      "Epoch 460/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8677\n",
      "Epoch 461/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.8601\n",
      "Epoch 462/600\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.793 - 0s 2ms/step - loss: 4.8517\n",
      "Epoch 463/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.8438\n",
      "Epoch 464/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8356\n",
      "Epoch 465/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8279\n",
      "Epoch 466/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.8199\n",
      "Epoch 467/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.8126\n",
      "Epoch 468/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8047\n",
      "Epoch 469/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.7970\n",
      "Epoch 470/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7891\n",
      "Epoch 471/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7814\n",
      "Epoch 472/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7734\n",
      "Epoch 473/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7659\n",
      "Epoch 474/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7583\n",
      "Epoch 475/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7510\n",
      "Epoch 476/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7438\n",
      "Epoch 477/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7365\n",
      "Epoch 478/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7291\n",
      "Epoch 479/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7215\n",
      "Epoch 480/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.7143\n",
      "Epoch 481/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.7072\n",
      "Epoch 482/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.7000\n",
      "Epoch 483/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6927\n",
      "Epoch 484/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6853\n",
      "Epoch 485/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6783\n",
      "Epoch 486/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6715\n",
      "Epoch 487/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6646\n",
      "Epoch 488/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6577\n",
      "Epoch 489/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6509\n",
      "Epoch 490/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6439\n",
      "Epoch 491/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6372\n",
      "Epoch 492/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6305\n",
      "Epoch 493/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6241\n",
      "Epoch 494/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6178\n",
      "Epoch 495/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6110\n",
      "Epoch 496/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.6040\n",
      "Epoch 497/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5972\n",
      "Epoch 498/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5903\n",
      "Epoch 499/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5839\n",
      "Epoch 500/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5773\n",
      "Epoch 501/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5712\n",
      "Epoch 502/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5649\n",
      "Epoch 503/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5587\n",
      "Epoch 504/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5526\n",
      "Epoch 505/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5462\n",
      "Epoch 506/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5395\n",
      "Epoch 507/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5331\n",
      "Epoch 508/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.5273\n",
      "Epoch 509/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5212\n",
      "Epoch 510/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5149\n",
      "Epoch 511/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.5090\n",
      "Epoch 512/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.5030\n",
      "Epoch 513/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4970\n",
      "Epoch 514/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.4910\n",
      "Epoch 515/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4846\n",
      "Epoch 516/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.4785\n",
      "Epoch 517/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4727\n",
      "Epoch 518/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.4671\n",
      "Epoch 519/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.4608\n",
      "Epoch 520/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.4554\n",
      "Epoch 521/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4503\n",
      "Epoch 522/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4442\n",
      "Epoch 523/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4382\n",
      "Epoch 524/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4318\n",
      "Epoch 525/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4263\n",
      "Epoch 526/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4202\n",
      "Epoch 527/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4143\n",
      "Epoch 528/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4085\n",
      "Epoch 529/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.4030\n",
      "Epoch 530/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3972\n",
      "Epoch 531/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3921\n",
      "Epoch 532/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3865\n",
      "Epoch 533/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3806\n",
      "Epoch 534/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3747\n",
      "Epoch 535/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3689\n",
      "Epoch 536/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3634\n",
      "Epoch 537/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3583\n",
      "Epoch 538/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3530\n",
      "Epoch 539/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3477\n",
      "Epoch 540/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3421\n",
      "Epoch 541/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3366\n",
      "Epoch 542/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3309\n",
      "Epoch 543/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3255\n",
      "Epoch 544/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3201\n",
      "Epoch 545/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3146\n",
      "Epoch 546/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3091\n",
      "Epoch 547/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.3036\n",
      "Epoch 548/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2981\n",
      "Epoch 549/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2932\n",
      "Epoch 550/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2876\n",
      "Epoch 551/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2823\n",
      "Epoch 552/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2775\n",
      "Epoch 553/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2717\n",
      "Epoch 554/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2660\n",
      "Epoch 555/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2608\n",
      "Epoch 556/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2558\n",
      "Epoch 557/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2504\n",
      "Epoch 558/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2453\n",
      "Epoch 559/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2398\n",
      "Epoch 560/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2346\n",
      "Epoch 561/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2297\n",
      "Epoch 562/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.2242\n",
      "Epoch 563/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.2189\n",
      "Epoch 564/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.2136\n",
      "Epoch 565/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2082\n",
      "Epoch 566/600\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 4.2032\n",
      "Epoch 567/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1981\n",
      "Epoch 568/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1935\n",
      "Epoch 569/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1888\n",
      "Epoch 570/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1840\n",
      "Epoch 571/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1792\n",
      "Epoch 572/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1740\n",
      "Epoch 573/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1691\n",
      "Epoch 574/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1638\n",
      "Epoch 575/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1587\n",
      "Epoch 576/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1538\n",
      "Epoch 577/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1489\n",
      "Epoch 578/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1442\n",
      "Epoch 579/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1392\n",
      "Epoch 580/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1345\n",
      "Epoch 581/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1300\n",
      "Epoch 582/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1252\n",
      "Epoch 583/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1205\n",
      "Epoch 584/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1159\n",
      "Epoch 585/600\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 4.1110\n",
      "Epoch 586/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1061\n",
      "Epoch 587/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.1015\n",
      "Epoch 588/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0967\n",
      "Epoch 589/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0923\n",
      "Epoch 590/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0877\n",
      "Epoch 591/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0835\n",
      "Epoch 592/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0789\n",
      "Epoch 593/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0745\n",
      "Epoch 594/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0708\n",
      "Epoch 595/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0663\n",
      "Epoch 596/600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.0622\n",
      "Epoch 597/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0580\n",
      "Epoch 598/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0533\n",
      "Epoch 599/600\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0490\n",
      "Epoch 600/600\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.0447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [56.056495666503906,\n",
       "  55.85679244995117,\n",
       "  55.57767868041992,\n",
       "  55.27431106567383,\n",
       "  54.959659576416016,\n",
       "  54.63650894165039,\n",
       "  54.30466842651367,\n",
       "  53.96346664428711,\n",
       "  53.61177062988281,\n",
       "  53.248931884765625,\n",
       "  52.87406921386719,\n",
       "  52.48649978637695,\n",
       "  52.08543014526367,\n",
       "  51.670413970947266,\n",
       "  51.24101257324219,\n",
       "  50.79688262939453,\n",
       "  50.337833404541016,\n",
       "  49.86350631713867,\n",
       "  49.37353515625,\n",
       "  48.868247985839844,\n",
       "  48.34745407104492,\n",
       "  47.811710357666016,\n",
       "  47.261173248291016,\n",
       "  46.69626235961914,\n",
       "  46.117698669433594,\n",
       "  45.52598190307617,\n",
       "  44.92191696166992,\n",
       "  44.30610656738281,\n",
       "  43.67923355102539,\n",
       "  43.04209518432617,\n",
       "  42.395904541015625,\n",
       "  41.74115753173828,\n",
       "  41.07905578613281,\n",
       "  40.40959167480469,\n",
       "  39.73381042480469,\n",
       "  39.05263900756836,\n",
       "  38.36717987060547,\n",
       "  37.678550720214844,\n",
       "  36.98716354370117,\n",
       "  36.293113708496094,\n",
       "  35.59661102294922,\n",
       "  34.899559020996094,\n",
       "  34.2044563293457,\n",
       "  33.51380920410156,\n",
       "  32.82879638671875,\n",
       "  32.14908981323242,\n",
       "  31.475069046020508,\n",
       "  30.808406829833984,\n",
       "  30.15022087097168,\n",
       "  29.50532341003418,\n",
       "  28.872013092041016,\n",
       "  28.255990982055664,\n",
       "  27.654783248901367,\n",
       "  27.07028579711914,\n",
       "  26.504777908325195,\n",
       "  25.961166381835938,\n",
       "  25.440643310546875,\n",
       "  24.94621467590332,\n",
       "  24.47008514404297,\n",
       "  24.01824188232422,\n",
       "  23.587539672851562,\n",
       "  23.184993743896484,\n",
       "  22.807952880859375,\n",
       "  22.46251678466797,\n",
       "  22.14238929748535,\n",
       "  21.850343704223633,\n",
       "  21.57662582397461,\n",
       "  21.321044921875,\n",
       "  21.075403213500977,\n",
       "  20.840988159179688,\n",
       "  20.613540649414062,\n",
       "  20.39920425415039,\n",
       "  20.19880485534668,\n",
       "  20.007434844970703,\n",
       "  19.821500778198242,\n",
       "  19.63779640197754,\n",
       "  19.458765029907227,\n",
       "  19.288381576538086,\n",
       "  19.123699188232422,\n",
       "  18.96919822692871,\n",
       "  18.815082550048828,\n",
       "  18.671274185180664,\n",
       "  18.528772354125977,\n",
       "  18.392942428588867,\n",
       "  18.258310317993164,\n",
       "  18.126737594604492,\n",
       "  17.998180389404297,\n",
       "  17.871795654296875,\n",
       "  17.747692108154297,\n",
       "  17.628469467163086,\n",
       "  17.51189613342285,\n",
       "  17.399362564086914,\n",
       "  17.2866153717041,\n",
       "  17.17519760131836,\n",
       "  17.06281089782715,\n",
       "  16.950159072875977,\n",
       "  16.83728790283203,\n",
       "  16.726985931396484,\n",
       "  16.61772346496582,\n",
       "  16.509851455688477,\n",
       "  16.400299072265625,\n",
       "  16.289865493774414,\n",
       "  16.180387496948242,\n",
       "  16.070722579956055,\n",
       "  15.960143089294434,\n",
       "  15.8483247756958,\n",
       "  15.7355318069458,\n",
       "  15.622173309326172,\n",
       "  15.508827209472656,\n",
       "  15.393837928771973,\n",
       "  15.277414321899414,\n",
       "  15.157186508178711,\n",
       "  15.036246299743652,\n",
       "  14.910724639892578,\n",
       "  14.781494140625,\n",
       "  14.647324562072754,\n",
       "  14.509127616882324,\n",
       "  14.365694046020508,\n",
       "  14.216282844543457,\n",
       "  14.061853408813477,\n",
       "  13.900961875915527,\n",
       "  13.734330177307129,\n",
       "  13.561264991760254,\n",
       "  13.38358211517334,\n",
       "  13.20195484161377,\n",
       "  13.016942024230957,\n",
       "  12.828168869018555,\n",
       "  12.63610553741455,\n",
       "  12.441718101501465,\n",
       "  12.245793342590332,\n",
       "  12.053478240966797,\n",
       "  11.863982200622559,\n",
       "  11.678759574890137,\n",
       "  11.495899200439453,\n",
       "  11.317994117736816,\n",
       "  11.145647048950195,\n",
       "  10.981391906738281,\n",
       "  10.824277877807617,\n",
       "  10.674175262451172,\n",
       "  10.52906608581543,\n",
       "  10.390132904052734,\n",
       "  10.257171630859375,\n",
       "  10.132184982299805,\n",
       "  10.013433456420898,\n",
       "  9.90087890625,\n",
       "  9.793936729431152,\n",
       "  9.692767143249512,\n",
       "  9.597097396850586,\n",
       "  9.507610321044922,\n",
       "  9.423624992370605,\n",
       "  9.346068382263184,\n",
       "  9.27331829071045,\n",
       "  9.204614639282227,\n",
       "  9.139790534973145,\n",
       "  9.078469276428223,\n",
       "  9.0214204788208,\n",
       "  8.967681884765625,\n",
       "  8.91687297821045,\n",
       "  8.868292808532715,\n",
       "  8.822565078735352,\n",
       "  8.78065299987793,\n",
       "  8.74134635925293,\n",
       "  8.70542049407959,\n",
       "  8.672914505004883,\n",
       "  8.642175674438477,\n",
       "  8.613038063049316,\n",
       "  8.584426879882812,\n",
       "  8.556577682495117,\n",
       "  8.530722618103027,\n",
       "  8.506937980651855,\n",
       "  8.484233856201172,\n",
       "  8.462607383728027,\n",
       "  8.442046165466309,\n",
       "  8.422740936279297,\n",
       "  8.404669761657715,\n",
       "  8.387468338012695,\n",
       "  8.370821952819824,\n",
       "  8.354132652282715,\n",
       "  8.338141441345215,\n",
       "  8.3228759765625,\n",
       "  8.307913780212402,\n",
       "  8.293158531188965,\n",
       "  8.278433799743652,\n",
       "  8.264416694641113,\n",
       "  8.250382423400879,\n",
       "  8.23645305633545,\n",
       "  8.22284984588623,\n",
       "  8.209698677062988,\n",
       "  8.196687698364258,\n",
       "  8.183941841125488,\n",
       "  8.170774459838867,\n",
       "  8.158215522766113,\n",
       "  8.145519256591797,\n",
       "  8.132926940917969,\n",
       "  8.119956970214844,\n",
       "  8.106734275817871,\n",
       "  8.093610763549805,\n",
       "  8.080611228942871,\n",
       "  8.067712783813477,\n",
       "  8.055038452148438,\n",
       "  8.041573524475098,\n",
       "  8.02778148651123,\n",
       "  8.014060020446777,\n",
       "  8.000322341918945,\n",
       "  7.986495494842529,\n",
       "  7.9727559089660645,\n",
       "  7.9589738845825195,\n",
       "  7.945286750793457,\n",
       "  7.931577205657959,\n",
       "  7.918074131011963,\n",
       "  7.904397964477539,\n",
       "  7.890523433685303,\n",
       "  7.876429080963135,\n",
       "  7.862447261810303,\n",
       "  7.8483734130859375,\n",
       "  7.8346638679504395,\n",
       "  7.821019649505615,\n",
       "  7.807265758514404,\n",
       "  7.792593955993652,\n",
       "  7.778282642364502,\n",
       "  7.764105319976807,\n",
       "  7.749739170074463,\n",
       "  7.735787391662598,\n",
       "  7.721700668334961,\n",
       "  7.707928657531738,\n",
       "  7.693879127502441,\n",
       "  7.680048942565918,\n",
       "  7.666133403778076,\n",
       "  7.652905464172363,\n",
       "  7.6383748054504395,\n",
       "  7.624436855316162,\n",
       "  7.610224723815918,\n",
       "  7.596866607666016,\n",
       "  7.583795070648193,\n",
       "  7.569031238555908,\n",
       "  7.554976463317871,\n",
       "  7.541083335876465,\n",
       "  7.527146339416504,\n",
       "  7.51344108581543,\n",
       "  7.499405860900879,\n",
       "  7.48559045791626,\n",
       "  7.472118377685547,\n",
       "  7.45864200592041,\n",
       "  7.445077896118164,\n",
       "  7.431780815124512,\n",
       "  7.418955326080322,\n",
       "  7.405139923095703,\n",
       "  7.390935897827148,\n",
       "  7.376698017120361,\n",
       "  7.362737655639648,\n",
       "  7.349282264709473,\n",
       "  7.335850238800049,\n",
       "  7.322643280029297,\n",
       "  7.309269428253174,\n",
       "  7.295865058898926,\n",
       "  7.282295227050781,\n",
       "  7.268914222717285,\n",
       "  7.255485534667969,\n",
       "  7.24183464050293,\n",
       "  7.228523254394531,\n",
       "  7.214768409729004,\n",
       "  7.20043420791626,\n",
       "  7.186356544494629,\n",
       "  7.17250919342041,\n",
       "  7.158530235290527,\n",
       "  7.1452107429504395,\n",
       "  7.1309967041015625,\n",
       "  7.117615699768066,\n",
       "  7.1041364669799805,\n",
       "  7.090454578399658,\n",
       "  7.076376914978027,\n",
       "  7.062439441680908,\n",
       "  7.048584461212158,\n",
       "  7.035180568695068,\n",
       "  7.021701335906982,\n",
       "  7.007889270782471,\n",
       "  6.994067668914795,\n",
       "  6.980323791503906,\n",
       "  6.966579914093018,\n",
       "  6.953007221221924,\n",
       "  6.938878536224365,\n",
       "  6.925171852111816,\n",
       "  6.910545349121094,\n",
       "  6.896240711212158,\n",
       "  6.882143020629883,\n",
       "  6.868166446685791,\n",
       "  6.854341506958008,\n",
       "  6.840552806854248,\n",
       "  6.826613903045654,\n",
       "  6.813055992126465,\n",
       "  6.799167633056641,\n",
       "  6.7853193283081055,\n",
       "  6.771467685699463,\n",
       "  6.757103443145752,\n",
       "  6.743232727050781,\n",
       "  6.729502201080322,\n",
       "  6.715824604034424,\n",
       "  6.702131748199463,\n",
       "  6.688225269317627,\n",
       "  6.674160480499268,\n",
       "  6.660312175750732,\n",
       "  6.64661169052124,\n",
       "  6.63286018371582,\n",
       "  6.619720458984375,\n",
       "  6.605792999267578,\n",
       "  6.592244625091553,\n",
       "  6.577931880950928,\n",
       "  6.563745975494385,\n",
       "  6.550053596496582,\n",
       "  6.536427021026611,\n",
       "  6.522748947143555,\n",
       "  6.509212017059326,\n",
       "  6.495472431182861,\n",
       "  6.482430934906006,\n",
       "  6.4680376052856445,\n",
       "  6.454744815826416,\n",
       "  6.441028594970703,\n",
       "  6.427481174468994,\n",
       "  6.413833141326904,\n",
       "  6.400480270385742,\n",
       "  6.386754035949707,\n",
       "  6.373372554779053,\n",
       "  6.360074043273926,\n",
       "  6.346850872039795,\n",
       "  6.333506107330322,\n",
       "  6.320669174194336,\n",
       "  6.307787895202637,\n",
       "  6.293783187866211,\n",
       "  6.280610084533691,\n",
       "  6.267364501953125,\n",
       "  6.25441837310791,\n",
       "  6.241372108459473,\n",
       "  6.228145599365234,\n",
       "  6.215376853942871,\n",
       "  6.202633380889893,\n",
       "  6.189644813537598,\n",
       "  6.176088333129883,\n",
       "  6.1631364822387695,\n",
       "  6.1500725746154785,\n",
       "  6.137825012207031,\n",
       "  6.124446868896484,\n",
       "  6.111942291259766,\n",
       "  6.0982537269592285,\n",
       "  6.0846991539001465,\n",
       "  6.071596145629883,\n",
       "  6.058990955352783,\n",
       "  6.046056270599365,\n",
       "  6.0333380699157715,\n",
       "  6.021535873413086,\n",
       "  6.008303642272949,\n",
       "  5.995789051055908,\n",
       "  5.983147144317627,\n",
       "  5.971351623535156,\n",
       "  5.958327770233154,\n",
       "  5.945794582366943,\n",
       "  5.933116912841797,\n",
       "  5.9205193519592285,\n",
       "  5.90818977355957,\n",
       "  5.8955183029174805,\n",
       "  5.883349895477295,\n",
       "  5.871026515960693,\n",
       "  5.859092712402344,\n",
       "  5.8467607498168945,\n",
       "  5.835077285766602,\n",
       "  5.823139190673828,\n",
       "  5.810976982116699,\n",
       "  5.799371719360352,\n",
       "  5.78825044631958,\n",
       "  5.776798725128174,\n",
       "  5.7646260261535645,\n",
       "  5.75253438949585,\n",
       "  5.7404561042785645,\n",
       "  5.728484630584717,\n",
       "  5.716436386108398,\n",
       "  5.704654216766357,\n",
       "  5.6928181648254395,\n",
       "  5.6809983253479,\n",
       "  5.669265270233154,\n",
       "  5.658341407775879,\n",
       "  5.647164344787598,\n",
       "  5.635807037353516,\n",
       "  5.624316215515137,\n",
       "  5.613000392913818,\n",
       "  5.601711273193359,\n",
       "  5.590398788452148,\n",
       "  5.579528331756592,\n",
       "  5.568142414093018,\n",
       "  5.557415962219238,\n",
       "  5.546380519866943,\n",
       "  5.535298824310303,\n",
       "  5.5242180824279785,\n",
       "  5.5133280754089355,\n",
       "  5.502238750457764,\n",
       "  5.491855621337891,\n",
       "  5.480816841125488,\n",
       "  5.4703779220581055,\n",
       "  5.460074424743652,\n",
       "  5.4493794441223145,\n",
       "  5.438663005828857,\n",
       "  5.4281816482543945,\n",
       "  5.417468547821045,\n",
       "  5.4069719314575195,\n",
       "  5.396426200866699,\n",
       "  5.386476516723633,\n",
       "  5.375125885009766,\n",
       "  5.364627838134766,\n",
       "  5.354325771331787,\n",
       "  5.344667434692383,\n",
       "  5.335013389587402,\n",
       "  5.32493782043457,\n",
       "  5.314774990081787,\n",
       "  5.304080009460449,\n",
       "  5.294045448303223,\n",
       "  5.2841572761535645,\n",
       "  5.27416467666626,\n",
       "  5.264334678649902,\n",
       "  5.254271984100342,\n",
       "  5.244447708129883,\n",
       "  5.234864234924316,\n",
       "  5.225204944610596,\n",
       "  5.215817451477051,\n",
       "  5.206636905670166,\n",
       "  5.197366714477539,\n",
       "  5.187776565551758,\n",
       "  5.178246021270752,\n",
       "  5.168722152709961,\n",
       "  5.1589250564575195,\n",
       "  5.149125099182129,\n",
       "  5.139730930328369,\n",
       "  5.130153656005859,\n",
       "  5.120633602142334,\n",
       "  5.111249923706055,\n",
       "  5.101726055145264,\n",
       "  5.0924153327941895,\n",
       "  5.082674503326416,\n",
       "  5.073577404022217,\n",
       "  5.065148830413818,\n",
       "  5.056884765625,\n",
       "  5.047493934631348,\n",
       "  5.038068771362305,\n",
       "  5.0288166999816895,\n",
       "  5.019896030426025,\n",
       "  5.01124382019043,\n",
       "  5.002642631530762,\n",
       "  4.994256496429443,\n",
       "  4.985956192016602,\n",
       "  4.977439880371094,\n",
       "  4.968790531158447,\n",
       "  4.960184097290039,\n",
       "  4.951486110687256,\n",
       "  4.942770004272461,\n",
       "  4.934175968170166,\n",
       "  4.925623416900635,\n",
       "  4.917207717895508,\n",
       "  4.908855438232422,\n",
       "  4.900645732879639,\n",
       "  4.892284870147705,\n",
       "  4.884528160095215,\n",
       "  4.8762664794921875,\n",
       "  4.867708683013916,\n",
       "  4.860053539276123,\n",
       "  4.8517279624938965,\n",
       "  4.843786716461182,\n",
       "  4.835606098175049,\n",
       "  4.827925682067871,\n",
       "  4.819888591766357,\n",
       "  4.8125762939453125,\n",
       "  4.804715156555176,\n",
       "  4.797021389007568,\n",
       "  4.789144515991211,\n",
       "  4.78141450881958,\n",
       "  4.773436546325684,\n",
       "  4.76590633392334,\n",
       "  4.758258819580078,\n",
       "  4.751025199890137,\n",
       "  4.743801116943359,\n",
       "  4.736496925354004,\n",
       "  4.729145526885986,\n",
       "  4.721543312072754,\n",
       "  4.714313507080078,\n",
       "  4.7072367668151855,\n",
       "  4.700000762939453,\n",
       "  4.692687034606934,\n",
       "  4.68529748916626,\n",
       "  4.678280830383301,\n",
       "  4.671544551849365,\n",
       "  4.664592266082764,\n",
       "  4.657729625701904,\n",
       "  4.6509013175964355,\n",
       "  4.643918037414551,\n",
       "  4.6372456550598145,\n",
       "  4.6304931640625,\n",
       "  4.624117851257324,\n",
       "  4.617832660675049,\n",
       "  4.611039161682129,\n",
       "  4.603957653045654,\n",
       "  4.5972137451171875,\n",
       "  4.5903215408325195,\n",
       "  4.583876609802246,\n",
       "  4.577321529388428,\n",
       "  4.571239948272705,\n",
       "  4.564877986907959,\n",
       "  4.558711528778076,\n",
       "  4.552581787109375,\n",
       "  4.546228408813477,\n",
       "  4.539504528045654,\n",
       "  4.533073425292969,\n",
       "  4.5272746086120605,\n",
       "  4.5211710929870605,\n",
       "  4.514866828918457,\n",
       "  4.509015083312988,\n",
       "  4.5029683113098145,\n",
       "  4.496969699859619,\n",
       "  4.490995407104492,\n",
       "  4.484575271606445,\n",
       "  4.478496074676514,\n",
       "  4.472725868225098,\n",
       "  4.467075347900391,\n",
       "  4.460772514343262,\n",
       "  4.455367088317871,\n",
       "  4.4503278732299805,\n",
       "  4.444248676300049,\n",
       "  4.438171863555908,\n",
       "  4.431836128234863,\n",
       "  4.426264762878418,\n",
       "  4.4202399253845215,\n",
       "  4.414316177368164,\n",
       "  4.4085235595703125,\n",
       "  4.403024196624756,\n",
       "  4.397221088409424,\n",
       "  4.392126560211182,\n",
       "  4.386519432067871,\n",
       "  4.380643844604492,\n",
       "  4.374708652496338,\n",
       "  4.3688554763793945,\n",
       "  4.363353252410889,\n",
       "  4.358342170715332,\n",
       "  4.353035926818848,\n",
       "  4.3476691246032715,\n",
       "  4.342123508453369,\n",
       "  4.336592674255371,\n",
       "  4.330930233001709,\n",
       "  4.325518608093262,\n",
       "  4.3201422691345215,\n",
       "  4.3145928382873535,\n",
       "  4.309088230133057,\n",
       "  4.303598403930664,\n",
       "  4.298139572143555,\n",
       "  4.293155193328857,\n",
       "  4.287618160247803,\n",
       "  4.282315254211426,\n",
       "  4.277486801147461,\n",
       "  4.271749973297119,\n",
       "  4.266044616699219,\n",
       "  4.260777473449707,\n",
       "  4.255802631378174,\n",
       "  4.250430107116699,\n",
       "  4.245261192321777,\n",
       "  4.2398362159729,\n",
       "  4.234645366668701,\n",
       "  4.229737758636475,\n",
       "  4.22423791885376,\n",
       "  4.2188720703125,\n",
       "  4.213595867156982,\n",
       "  4.208202838897705,\n",
       "  4.20322847366333,\n",
       "  4.198110580444336,\n",
       "  4.193480014801025,\n",
       "  4.18878698348999,\n",
       "  4.1839599609375,\n",
       "  4.179245471954346,\n",
       "  4.17400598526001,\n",
       "  4.169125080108643,\n",
       "  4.163825511932373,\n",
       "  4.158688545227051,\n",
       "  4.153803825378418,\n",
       "  4.1488728523254395,\n",
       "  4.144150257110596,\n",
       "  4.139153003692627,\n",
       "  4.134512424468994,\n",
       "  4.129975318908691,\n",
       "  4.125249862670898,\n",
       "  4.120514869689941,\n",
       "  4.115869998931885,\n",
       "  4.1110334396362305,\n",
       "  4.106131076812744,\n",
       "  4.101498603820801,\n",
       "  4.096726894378662,\n",
       "  4.092262268066406,\n",
       "  4.087700366973877,\n",
       "  4.08348274230957,\n",
       "  4.078854084014893,\n",
       "  4.074487686157227,\n",
       "  4.070764541625977,\n",
       "  4.066311359405518,\n",
       "  4.062234401702881,\n",
       "  4.057971954345703,\n",
       "  4.053318977355957,\n",
       "  4.0489821434021,\n",
       "  4.044661998748779]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_Low_LR = []\n",
    "historyTr_Low_LR = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_Low_LR()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036).history\n",
    "    historyVal_Low_LR.append(history['val_loss'])\n",
    "    historyTr_Low_LR.append(history['loss'])\n",
    "model=create_model_Low_LR()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=1036).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_Low_LR_mean=np.mean(historyVal_Low_LR, axis=0)\n",
    "historyTr_Low_LR_mean=np.mean(historyTr_Low_LR, axis=0)\n",
    "\n",
    "historyVal_Low_LR_sd=np.std(historyVal_Low_LR, axis=0)\n",
    "historyTr_Low_LR_sd=np.std(historyTr_Low_LR, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_Low_LR_mean)),historyTr_Low_LR_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_Low_LR_mean)), historyTr_Low_LR_mean - historyTr_Low_LR_sd, \n",
    "                 historyTr_Low_LR_mean + historyTr_Low_LR_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_Low_LR_mean)), historyVal_Low_LR_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_Low_LR_mean)), historyVal_Low_LR_mean - historyVal_Low_LR_sd, \n",
    "                 historyVal_Low_LR_mean + historyVal_Low_LR_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,80)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 81, +3))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low regularization (lr=0.01) result:\n",
      "MEE on the validation 7.383029651641846 with standard deviation 0.4687370612742865\n",
      "MEE on the training 7.350340843200684 with standard deviation 0.07202775080340545\n"
     ]
    }
   ],
   "source": [
    "print(\"Low regularization (lr=0.01) result:\")\n",
    "print(\"MEE on the validation\",historyVal_Low_LR_mean[-1],\"with standard deviation\",historyVal_Low_LR_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_Low_LR_mean[-1],\"with standard deviation\",historyTr_Low_LR_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_MB():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.1, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 47.3277 - val_loss: 30.2117\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 22.3621 - val_loss: 16.7536\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 14.5474 - val_loss: 10.6436\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 8.8987 - val_loss: 8.1628\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 7.8537 - val_loss: 7.6813\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 7.2426 - val_loss: 7.0509\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 6.6129 - val_loss: 6.4110\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 5.9470 - val_loss: 5.7800\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 5.1586 - val_loss: 5.0538\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 4.5657 - val_loss: 4.6399\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 4.2084 - val_loss: 4.3590\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 3.9639 - val_loss: 4.1083\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 3.7776 - val_loss: 4.1010\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.5805 - val_loss: 3.8553\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 3.4663 - val_loss: 4.0777\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.4237 - val_loss: 3.6571\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.2652 - val_loss: 3.5786\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.2009 - val_loss: 3.5604\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.1422 - val_loss: 3.6742\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.1711 - val_loss: 3.5224\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0662 - val_loss: 3.5210\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0508 - val_loss: 3.4995\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0996 - val_loss: 3.4613\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9909 - val_loss: 3.7956\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.0647 - val_loss: 3.4462\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.0086 - val_loss: 3.4105\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.9711 - val_loss: 3.3738\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9484 - val_loss: 3.3874\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8804 - val_loss: 3.3930\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8813 - val_loss: 3.6148\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9274 - val_loss: 3.3123\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.8772 - val_loss: 3.3688\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.8514 - val_loss: 3.5722\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.8726 - val_loss: 3.3145\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7992 - val_loss: 3.3072\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.8406 - val_loss: 3.4605\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8060 - val_loss: 3.2835\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7742 - val_loss: 3.2733\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7569 - val_loss: 3.6158\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.8389 - val_loss: 3.7100\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8234 - val_loss: 3.3144\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7428 - val_loss: 3.3427\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7564 - val_loss: 3.4994\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7862 - val_loss: 3.3300\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7796 - val_loss: 3.4622\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7505 - val_loss: 3.2486\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7267 - val_loss: 3.2587\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7441 - val_loss: 3.2292\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6978 - val_loss: 3.4406\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6925 - val_loss: 3.2460\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6792 - val_loss: 3.3576\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7329 - val_loss: 3.2207\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6564 - val_loss: 3.2562\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.7287 - val_loss: 3.2117\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6598 - val_loss: 3.2159\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6536 - val_loss: 3.2057\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6432 - val_loss: 3.1800\n",
      "Epoch 58/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.7089 - val_loss: 3.2319\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6258 - val_loss: 3.2786\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6425 - val_loss: 3.4094\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6687 - val_loss: 3.2549\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6319 - val_loss: 3.3889\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6766 - val_loss: 3.3113\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6215 - val_loss: 3.2767\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5981 - val_loss: 3.3010\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6143 - val_loss: 3.2109\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6417 - val_loss: 3.1866\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5949 - val_loss: 3.3090\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5841 - val_loss: 3.1941\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5841 - val_loss: 3.2567\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5905 - val_loss: 3.2141\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5664 - val_loss: 3.1834\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5802 - val_loss: 3.3174\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5721 - val_loss: 3.2323\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5600 - val_loss: 3.3879\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5754 - val_loss: 3.3069\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5466 - val_loss: 3.1322\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5589 - val_loss: 3.2402\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5718 - val_loss: 3.1923\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5558 - val_loss: 3.3059\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5360 - val_loss: 3.2815\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5556 - val_loss: 3.4032\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5514 - val_loss: 3.1874\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5386 - val_loss: 3.3003\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5283 - val_loss: 3.1967\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5265 - val_loss: 3.2118\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5396 - val_loss: 3.1978\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5616 - val_loss: 3.2980\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5350 - val_loss: 3.2163\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5202 - val_loss: 3.1697\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5514 - val_loss: 3.2264\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5156 - val_loss: 3.3288\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5397 - val_loss: 3.7451\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5764 - val_loss: 3.2023\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4893 - val_loss: 3.2392\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5241 - val_loss: 3.1908\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4998 - val_loss: 3.2018\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4857 - val_loss: 3.2405\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5481 - val_loss: 3.2388\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4962 - val_loss: 3.2213\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4911 - val_loss: 3.2196\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5304 - val_loss: 3.2172\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4768 - val_loss: 3.1225\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.4705 - val_loss: 3.1169\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4816 - val_loss: 3.2319\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4812 - val_loss: 3.1857\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4608 - val_loss: 3.1725\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4729 - val_loss: 3.2366\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4521 - val_loss: 3.1950\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4422 - val_loss: 3.3222\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5132 - val_loss: 3.2306\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4676 - val_loss: 3.2361\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4744 - val_loss: 3.1387\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4452 - val_loss: 3.2064\n",
      "Epoch 115/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4760 - val_loss: 3.1580\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4225 - val_loss: 3.1904\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4868 - val_loss: 3.3257\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4831 - val_loss: 3.1769\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4498 - val_loss: 3.1640\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4964 - val_loss: 3.2567\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4672 - val_loss: 3.2534\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4583 - val_loss: 3.1545\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4058 - val_loss: 3.4314\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4543 - val_loss: 3.1747\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4331 - val_loss: 3.1613\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4332 - val_loss: 3.2201\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4238 - val_loss: 3.3463\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4467 - val_loss: 3.2996\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4225 - val_loss: 3.2199\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4234 - val_loss: 3.1847\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4111 - val_loss: 3.1617\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4933 - val_loss: 3.1977\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3963 - val_loss: 3.2411\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4213 - val_loss: 3.1156\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4170 - val_loss: 3.1975\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4065 - val_loss: 3.3123\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4109 - val_loss: 3.1625\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4369 - val_loss: 3.1216\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3976 - val_loss: 3.3646\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4561 - val_loss: 3.1197\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3976 - val_loss: 3.2026\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.4093 - val_loss: 3.2157\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4349 - val_loss: 3.1629\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3781 - val_loss: 3.3128\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4226 - val_loss: 3.2892\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4932 - val_loss: 3.1913\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4003 - val_loss: 3.2350\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3743 - val_loss: 3.1944\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3815 - val_loss: 3.2756\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3926 - val_loss: 3.1157\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3650 - val_loss: 3.3862\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3530 - val_loss: 3.1420\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.3906 - val_loss: 3.1858\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3851 - val_loss: 3.1506\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3517 - val_loss: 3.1471\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3661 - val_loss: 3.1063\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3665 - val_loss: 3.3567\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3793 - val_loss: 3.1367\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3372 - val_loss: 3.1655\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3819 - val_loss: 3.1220\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3587 - val_loss: 3.2599\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3919 - val_loss: 3.1416\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3658 - val_loss: 3.1687\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3833 - val_loss: 3.1680\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3401 - val_loss: 3.1469\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3543 - val_loss: 3.1306\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3419 - val_loss: 3.1286\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3441 - val_loss: 3.2359\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3446 - val_loss: 3.2381\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3599 - val_loss: 3.1421\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3237 - val_loss: 3.1937\n",
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3729 - val_loss: 3.1759\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.3438 - val_loss: 3.1410\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3484 - val_loss: 3.4054\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3639 - val_loss: 3.1823\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3179 - val_loss: 3.1117\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3246 - val_loss: 3.2394\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3314 - val_loss: 3.1187\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3157 - val_loss: 3.1757\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3153 - val_loss: 3.1385\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3018 - val_loss: 3.1929\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3178 - val_loss: 3.2123\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3298 - val_loss: 3.2569\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3261 - val_loss: 3.3488\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3305 - val_loss: 3.2097\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3352 - val_loss: 3.2677\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3335 - val_loss: 3.1756\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2937 - val_loss: 3.1966\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3258 - val_loss: 3.1289\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3121 - val_loss: 3.2549\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3324 - val_loss: 3.1603\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3066 - val_loss: 3.1870\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3104 - val_loss: 3.3203\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3521 - val_loss: 3.1860\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3161 - val_loss: 3.0986\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3286 - val_loss: 3.2601\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3017 - val_loss: 3.1803\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2897 - val_loss: 3.1754\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3137 - val_loss: 3.1330\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2705 - val_loss: 3.1883\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2877 - val_loss: 3.2819\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2977 - val_loss: 3.1608\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2974 - val_loss: 3.2480\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2879 - val_loss: 3.2118\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2895 - val_loss: 3.2384\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2940 - val_loss: 3.1729\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2603 - val_loss: 3.1349\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2785 - val_loss: 3.2908\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2971 - val_loss: 3.1345\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2527 - val_loss: 3.1615\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2845 - val_loss: 3.2295\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2655 - val_loss: 3.2236\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2441 - val_loss: 3.1453\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2390 - val_loss: 3.1108\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2684 - val_loss: 3.1643\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2547 - val_loss: 3.3001\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2576 - val_loss: 3.1170\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2331 - val_loss: 3.1188\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2348 - val_loss: 3.1148\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2392 - val_loss: 3.1586\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2724 - val_loss: 3.1114\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2656 - val_loss: 3.2831\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2691 - val_loss: 3.1567\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2371 - val_loss: 3.1977\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2545 - val_loss: 3.3109\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2489 - val_loss: 3.1646\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2410 - val_loss: 3.2091\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2307 - val_loss: 3.2259\n",
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2407 - val_loss: 3.1773\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2500 - val_loss: 3.1399\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.2105 - val_loss: 3.2545\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2261 - val_loss: 3.2105\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2333 - val_loss: 3.1408\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2186 - val_loss: 3.1608\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2235 - val_loss: 3.1570\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2318 - val_loss: 3.2311\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2579 - val_loss: 3.1686\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2186 - val_loss: 3.1207\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2407 - val_loss: 3.1247\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2129 - val_loss: 3.2043\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2545 - val_loss: 3.2215\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2335 - val_loss: 3.1565\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2307 - val_loss: 3.1941\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2388 - val_loss: 3.0964\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2341 - val_loss: 3.1174\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2313 - val_loss: 3.1927\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2261 - val_loss: 3.1007\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2061 - val_loss: 3.1646\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2069 - val_loss: 3.2774\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2315 - val_loss: 3.3274\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2234 - val_loss: 3.1167\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1879 - val_loss: 3.1312\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2020 - val_loss: 3.2658\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2749 - val_loss: 3.2023\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2175 - val_loss: 3.1579\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1877 - val_loss: 3.2553\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2157 - val_loss: 3.1940\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1836 - val_loss: 3.2468\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1741 - val_loss: 3.2896\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2035 - val_loss: 3.1538\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1945 - val_loss: 3.1409\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2009 - val_loss: 3.1557\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1643 - val_loss: 3.1557\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1874 - val_loss: 3.1722\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1841 - val_loss: 3.1996\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1700 - val_loss: 3.1200\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1926 - val_loss: 3.1379\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1569 - val_loss: 3.1546\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1593 - val_loss: 3.1430\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1638 - val_loss: 3.0916\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1847 - val_loss: 3.1250\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1814 - val_loss: 3.1332\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1596 - val_loss: 3.1501\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1519 - val_loss: 3.1322\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1810 - val_loss: 3.1418\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1846 - val_loss: 3.1602\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1894 - val_loss: 3.2313\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1314 - val_loss: 3.0996\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1703 - val_loss: 3.1566\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1716 - val_loss: 3.1600\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1720 - val_loss: 3.1611\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1542 - val_loss: 3.1922\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1496 - val_loss: 3.1298\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1386 - val_loss: 3.2383\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1519 - val_loss: 3.1324\n",
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1411 - val_loss: 3.1215\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1376 - val_loss: 3.2037\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1712 - val_loss: 3.1082\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1267 - val_loss: 3.1636\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1206 - val_loss: 3.1149\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1561 - val_loss: 3.1468\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1301 - val_loss: 3.1595\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1645 - val_loss: 3.1596\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1430 - val_loss: 3.1693\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1192 - val_loss: 3.1631\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1182 - val_loss: 3.2311\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1471 - val_loss: 3.1076\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1305 - val_loss: 3.1443\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1320 - val_loss: 3.1233\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1580 - val_loss: 3.2260\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1386 - val_loss: 3.1495\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1255 - val_loss: 3.2116\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1266 - val_loss: 3.1143\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1188 - val_loss: 3.1543\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1293 - val_loss: 3.2845\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1319 - val_loss: 3.1577\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1389 - val_loss: 3.1513\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1451 - val_loss: 3.2867\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1172 - val_loss: 3.2185\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1241 - val_loss: 3.2413\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1250 - val_loss: 3.1497\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1197 - val_loss: 3.1805\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1006 - val_loss: 3.1597\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1063 - val_loss: 3.1306\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0939 - val_loss: 3.1671\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0841 - val_loss: 3.2482\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1506 - val_loss: 3.1162\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1200 - val_loss: 3.1940\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1199 - val_loss: 3.1314\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1167 - val_loss: 3.2372\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1352 - val_loss: 3.1790\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1051 - val_loss: 3.1497\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1008 - val_loss: 3.1842\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0829 - val_loss: 3.2486\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0876 - val_loss: 3.1965\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0985 - val_loss: 3.1705\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0580 - val_loss: 3.1119\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0548 - val_loss: 3.1957\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0676 - val_loss: 3.1430\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0825 - val_loss: 3.1583\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0703 - val_loss: 3.1803\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0673 - val_loss: 3.1695\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0750 - val_loss: 3.2792\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0714 - val_loss: 3.1588\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0681 - val_loss: 3.2089\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0737 - val_loss: 3.2360\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1013 - val_loss: 3.2679\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0870 - val_loss: 3.1527\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 2.1012 - val_loss: 3.1774\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.0430 - val_loss: 3.1748\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0555 - val_loss: 3.1224\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0662 - val_loss: 3.1404\n",
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0754 - val_loss: 3.2357\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1057 - val_loss: 3.1464\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0630 - val_loss: 3.1431\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0466 - val_loss: 3.2292\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1099 - val_loss: 3.1989\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0699 - val_loss: 3.1235\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0460 - val_loss: 3.1842\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0538 - val_loss: 3.2044\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0502 - val_loss: 3.1610\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0469 - val_loss: 3.2108\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0803 - val_loss: 3.2104\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0514 - val_loss: 3.2690\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0588 - val_loss: 3.2690\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0451 - val_loss: 3.1921\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0438 - val_loss: 3.2280\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0456 - val_loss: 3.2939\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.0387 - val_loss: 3.3752\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0929 - val_loss: 3.1446\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0439 - val_loss: 3.1448\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0503 - val_loss: 3.2068\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0307 - val_loss: 3.2295\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0372 - val_loss: 3.3138\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0456 - val_loss: 3.1842\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0503 - val_loss: 3.2015\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0361 - val_loss: 3.2799\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0716 - val_loss: 3.1582\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0225 - val_loss: 3.2459\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0564 - val_loss: 3.3452\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0863 - val_loss: 3.1424\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0435 - val_loss: 3.1578\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0153 - val_loss: 3.1839\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0835 - val_loss: 3.1783\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9824 - val_loss: 3.1415\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0118 - val_loss: 3.1819\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0101 - val_loss: 3.1199\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0347 - val_loss: 3.5092\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1136 - val_loss: 3.1860\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0087 - val_loss: 3.2460\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0146 - val_loss: 3.1884\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0298 - val_loss: 3.3779\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0601 - val_loss: 3.1662\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0462 - val_loss: 3.3193\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9928 - val_loss: 3.2600\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0407 - val_loss: 3.1495\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0163 - val_loss: 3.1123\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0001 - val_loss: 3.1681\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9659 - val_loss: 3.2670\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0114 - val_loss: 3.1816\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9749 - val_loss: 3.1612\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9993 - val_loss: 3.1361\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0046 - val_loss: 3.2093\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0028 - val_loss: 3.2400\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9960 - val_loss: 3.1905\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9754 - val_loss: 3.1723\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9971 - val_loss: 3.1839\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9948 - val_loss: 3.2232\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9792 - val_loss: 3.1520\n",
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9873 - val_loss: 3.2102\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0085 - val_loss: 3.2151\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9700 - val_loss: 3.2202\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9727 - val_loss: 3.2599\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9940 - val_loss: 3.1542\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0029 - val_loss: 3.1858\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9538 - val_loss: 3.1531\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9563 - val_loss: 3.1536\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9632 - val_loss: 3.1572\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9496 - val_loss: 3.1360\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9653 - val_loss: 3.1477\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9725 - val_loss: 3.2720\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9782 - val_loss: 3.1715\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9576 - val_loss: 3.2027\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9651 - val_loss: 3.2083\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9960 - val_loss: 3.3387\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0003 - val_loss: 3.3056\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9644 - val_loss: 3.3009\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9753 - val_loss: 3.1698\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9446 - val_loss: 3.1641\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9606 - val_loss: 3.1852\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9528 - val_loss: 3.1663\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9576 - val_loss: 3.2069\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9590 - val_loss: 3.1816\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9530 - val_loss: 3.2909\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9992 - val_loss: 3.1997\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9366 - val_loss: 3.1452\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9859 - val_loss: 3.1970\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9368 - val_loss: 3.1451\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9815 - val_loss: 3.3509\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9787 - val_loss: 3.2706\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9598 - val_loss: 3.1813\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9296 - val_loss: 3.2343\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9762 - val_loss: 3.1726\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9441 - val_loss: 3.3976\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9706 - val_loss: 3.1618\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9247 - val_loss: 3.2749\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9744 - val_loss: 3.1883\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9354 - val_loss: 3.2365\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9408 - val_loss: 3.1725\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9084 - val_loss: 3.1645\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9714 - val_loss: 3.1843\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9158 - val_loss: 3.2020\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9219 - val_loss: 3.2116\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9151 - val_loss: 3.5405\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9931 - val_loss: 3.2891\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9704 - val_loss: 3.1785\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9338 - val_loss: 3.1760\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9143 - val_loss: 3.1960\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9211 - val_loss: 3.2538\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9322 - val_loss: 3.1743\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9613 - val_loss: 3.1935\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9502 - val_loss: 3.2504\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9303 - val_loss: 3.1648\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8943 - val_loss: 3.1713\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8986 - val_loss: 3.2057\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9118 - val_loss: 3.3194\n",
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9117 - val_loss: 3.1948\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9059 - val_loss: 3.2196\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9402 - val_loss: 3.3061\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9422 - val_loss: 3.1959\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9110 - val_loss: 3.2362\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9074 - val_loss: 3.2290\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8995 - val_loss: 3.2374\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8843 - val_loss: 3.2858\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9040 - val_loss: 3.2493\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9012 - val_loss: 3.2814\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9190 - val_loss: 3.2401\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9227 - val_loss: 3.1901\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9249 - val_loss: 3.1877\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8718 - val_loss: 3.2548\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8958 - val_loss: 3.1845\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8863 - val_loss: 3.2687\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9021 - val_loss: 3.1562\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9150 - val_loss: 3.2701\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8903 - val_loss: 3.2030\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9211 - val_loss: 3.1851\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8919 - val_loss: 3.1913\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8984 - val_loss: 3.2100\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8941 - val_loss: 3.2157\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9380 - val_loss: 3.2324\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8998 - val_loss: 3.2085\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9053 - val_loss: 3.3429\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9105 - val_loss: 3.2314\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8811 - val_loss: 3.2273\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8695 - val_loss: 3.2242\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8780 - val_loss: 3.3258\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9168 - val_loss: 3.2522\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8866 - val_loss: 3.2081\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8712 - val_loss: 3.2156\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8720 - val_loss: 3.3894\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0105 - val_loss: 3.2352\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8816 - val_loss: 3.1932\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8510 - val_loss: 3.2531\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8987 - val_loss: 3.1733\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8626 - val_loss: 3.1813\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8600 - val_loss: 3.2278\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8802 - val_loss: 3.3245\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8676 - val_loss: 3.1746\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8518 - val_loss: 3.2017\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9076 - val_loss: 3.1733\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8573 - val_loss: 3.2175\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8402 - val_loss: 3.2138\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8530 - val_loss: 3.4585\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9157 - val_loss: 3.2501\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8542 - val_loss: 3.2031\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.8520 - val_loss: 3.2159\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8758 - val_loss: 3.2222\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8713 - val_loss: 3.1935\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8299 - val_loss: 3.2413\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8536 - val_loss: 3.2178\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8282 - val_loss: 3.2209\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8765 - val_loss: 3.1519\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8293 - val_loss: 3.2785\n",
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8762 - val_loss: 3.3087\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8449 - val_loss: 3.2246\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8383 - val_loss: 3.2130\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8299 - val_loss: 3.2610\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.8512 - val_loss: 3.2044\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9031 - val_loss: 3.3615\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8479 - val_loss: 3.2300\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8437 - val_loss: 3.3188\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8825 - val_loss: 3.1941\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8365 - val_loss: 3.3223\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8772 - val_loss: 3.1767\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8245 - val_loss: 3.2305\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8508 - val_loss: 3.2613\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8252 - val_loss: 3.2418\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8289 - val_loss: 3.2210\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8169 - val_loss: 3.2738\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8606 - val_loss: 3.3021\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8479 - val_loss: 3.1862\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8259 - val_loss: 3.1981\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7967 - val_loss: 3.3610\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8393 - val_loss: 3.3258\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8369 - val_loss: 3.2750\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8656 - val_loss: 3.2026\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7864 - val_loss: 3.3405\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8204 - val_loss: 3.2785\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8214 - val_loss: 3.2201\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8395 - val_loss: 3.2280\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7870 - val_loss: 3.2396\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8390 - val_loss: 3.4285\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8267 - val_loss: 3.2284\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7846 - val_loss: 3.2437\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8317 - val_loss: 3.2794\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8356 - val_loss: 3.2483\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7962 - val_loss: 3.2563\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8191 - val_loss: 3.3002\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8297 - val_loss: 3.2176\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8134 - val_loss: 3.2632\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8011 - val_loss: 3.2533\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7640 - val_loss: 3.2433\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7904 - val_loss: 3.2334\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8084 - val_loss: 3.3476\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8333 - val_loss: 3.2742\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 1.8243 - val_loss: 3.2014\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7871 - val_loss: 3.2700\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7821 - val_loss: 3.2509\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.7675 - val_loss: 3.3319\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7926 - val_loss: 3.2183\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7892 - val_loss: 3.2437\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7884 - val_loss: 3.2285\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8061 - val_loss: 3.2333\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7755 - val_loss: 3.2633\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.7944 - val_loss: 3.2342\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7551 - val_loss: 3.2423\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8000 - val_loss: 3.2229\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.7925 - val_loss: 3.2870\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7901 - val_loss: 3.2014\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7847 - val_loss: 3.2300\n",
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7902 - val_loss: 3.2748\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7512 - val_loss: 3.3349\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8032 - val_loss: 3.2310\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7580 - val_loss: 3.3559\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8075 - val_loss: 3.2506\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7664 - val_loss: 3.2738\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7888 - val_loss: 3.3797\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8361 - val_loss: 3.2268\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7779 - val_loss: 3.3379\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7542 - val_loss: 3.2573\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7722 - val_loss: 3.2799\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7661 - val_loss: 3.3074\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8287 - val_loss: 3.2396\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.7486 - val_loss: 3.3663\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7997 - val_loss: 3.3518\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7596 - val_loss: 3.3905\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.7793 - val_loss: 3.2548\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7509 - val_loss: 3.2363\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7907 - val_loss: 3.2681\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7836 - val_loss: 3.2914\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8078 - val_loss: 3.2377\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.7229 - val_loss: 3.2758\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.7566 - val_loss: 3.3766\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7846 - val_loss: 3.3003\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7802 - val_loss: 3.2644\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7267 - val_loss: 3.2600\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7399 - val_loss: 3.2648\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7358 - val_loss: 3.4079\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7858 - val_loss: 3.2935\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7622 - val_loss: 3.3002\n",
      "Epoch 1/600\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 49.2764 - val_loss: 33.9497\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 23.3876 - val_loss: 16.0308\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 14.5292 - val_loss: 10.7490\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 9.4998 - val_loss: 8.2307\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 8.2667 - val_loss: 7.7624\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 7.7929 - val_loss: 7.2923\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 7.2900 - val_loss: 6.8564\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 6.8137 - val_loss: 6.3002\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 6.2050 - val_loss: 5.5675\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 5.5701 - val_loss: 4.9304\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7950 - val_loss: 4.3096\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 4.3524 - val_loss: 4.3965\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.1558 - val_loss: 3.7940\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.9126 - val_loss: 3.6296\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.7959 - val_loss: 3.6730\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.6817 - val_loss: 3.3969\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.5642 - val_loss: 3.5029\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.5232 - val_loss: 3.2768\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.3843 - val_loss: 3.2359\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.3355 - val_loss: 3.3153\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.3525 - val_loss: 3.2098\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 3.2565 - val_loss: 3.1320\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 3.2065 - val_loss: 3.0668\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.1885 - val_loss: 2.9068\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.1428 - val_loss: 2.9917\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.1280 - val_loss: 2.9757\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0835 - val_loss: 3.0404\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0968 - val_loss: 2.9314\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0327 - val_loss: 2.8755\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0128 - val_loss: 2.9611\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0185 - val_loss: 2.8406\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.9695 - val_loss: 2.9582\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.9881 - val_loss: 3.0944\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.9744 - val_loss: 2.8724\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.9645 - val_loss: 2.8027\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9392 - val_loss: 2.9088\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8748 - val_loss: 3.2415\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.0327 - val_loss: 3.0890\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9006 - val_loss: 2.8224\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8704 - val_loss: 2.8447\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8635 - val_loss: 2.9338\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8376 - val_loss: 2.9244\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8398 - val_loss: 2.9211\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8160 - val_loss: 2.8262\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8012 - val_loss: 3.1735\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8897 - val_loss: 2.8057\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8390 - val_loss: 2.7890\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8458 - val_loss: 2.8169\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7882 - val_loss: 2.9282\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7973 - val_loss: 2.8004\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7957 - val_loss: 2.8958\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.8040 - val_loss: 2.9040\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7505 - val_loss: 2.8117\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7509 - val_loss: 2.8053\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7558 - val_loss: 2.7861\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7292 - val_loss: 2.9740\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7163 - val_loss: 3.0280\n",
      "Epoch 58/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7309 - val_loss: 2.7513\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.7000 - val_loss: 2.7839\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7116 - val_loss: 3.0299\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7713 - val_loss: 2.8412\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7180 - val_loss: 2.8313\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.7043 - val_loss: 3.3794\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7947 - val_loss: 2.8589\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6953 - val_loss: 3.0483\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7726 - val_loss: 2.8603\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6985 - val_loss: 2.8527\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6721 - val_loss: 2.7719\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6973 - val_loss: 3.0359\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6573 - val_loss: 2.8420\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6609 - val_loss: 2.9820\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6630 - val_loss: 2.8395\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6855 - val_loss: 2.7632\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6842 - val_loss: 3.0200\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6654 - val_loss: 2.7591\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6424 - val_loss: 2.8007\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6572 - val_loss: 2.8057\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6694 - val_loss: 2.8055\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6393 - val_loss: 2.8062\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5990 - val_loss: 2.7519\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5989 - val_loss: 2.8429\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6139 - val_loss: 2.7398\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6479 - val_loss: 2.8704\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6167 - val_loss: 2.8482\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6159 - val_loss: 2.8831\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6166 - val_loss: 2.8674\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6004 - val_loss: 2.8158\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6174 - val_loss: 2.7952\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6108 - val_loss: 2.7625\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6149 - val_loss: 2.7855\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5879 - val_loss: 2.7764\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5703 - val_loss: 2.8418\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5821 - val_loss: 2.7554\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5561 - val_loss: 2.8866\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5669 - val_loss: 2.8293\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5726 - val_loss: 2.8205\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5740 - val_loss: 2.8015\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5927 - val_loss: 2.7895\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5754 - val_loss: 2.7546\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5750 - val_loss: 2.8331\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5504 - val_loss: 2.8110\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5592 - val_loss: 2.7875\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5506 - val_loss: 2.9794\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5735 - val_loss: 2.9729\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5803 - val_loss: 3.0186\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5997 - val_loss: 3.0644\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6276 - val_loss: 2.7921\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5211 - val_loss: 2.8129\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5385 - val_loss: 2.7651\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5341 - val_loss: 2.7558\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5155 - val_loss: 3.1336\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5706 - val_loss: 2.8058\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5278 - val_loss: 3.0555\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5588 - val_loss: 2.7808\n",
      "Epoch 115/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5105 - val_loss: 2.8626\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5281 - val_loss: 2.9923\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5648 - val_loss: 2.8581\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5803 - val_loss: 2.7908\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5024 - val_loss: 2.8621\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4969 - val_loss: 2.8868\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4988 - val_loss: 3.1378\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5656 - val_loss: 2.9126\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5223 - val_loss: 2.7571\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4883 - val_loss: 2.7952\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4921 - val_loss: 2.8055\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5415 - val_loss: 2.8367\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5410 - val_loss: 2.9256\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4850 - val_loss: 2.7436\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4917 - val_loss: 2.7734\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4857 - val_loss: 2.8439\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5096 - val_loss: 2.7656\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5134 - val_loss: 2.9201\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.4887 - val_loss: 2.9412\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.5073 - val_loss: 2.8628\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4772 - val_loss: 2.8022\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4569 - val_loss: 2.7191\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4333 - val_loss: 2.7660\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4947 - val_loss: 2.7925\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4723 - val_loss: 2.8097\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4724 - val_loss: 2.8987\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5133 - val_loss: 2.8821\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4450 - val_loss: 2.8160\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4528 - val_loss: 2.8186\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4404 - val_loss: 2.8319\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4437 - val_loss: 2.9194\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4574 - val_loss: 2.7664\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4558 - val_loss: 2.8264\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4436 - val_loss: 2.8688\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4790 - val_loss: 2.8974\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4789 - val_loss: 2.7905\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4263 - val_loss: 2.7569\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4409 - val_loss: 2.7553\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4386 - val_loss: 2.8290\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4719 - val_loss: 2.7919\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4619 - val_loss: 2.9599\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4534 - val_loss: 3.0003\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4477 - val_loss: 2.7326\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4129 - val_loss: 2.8005\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4233 - val_loss: 2.8358\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4792 - val_loss: 2.9121\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4885 - val_loss: 2.8405\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.4302 - val_loss: 2.8659\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4391 - val_loss: 2.7934\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4286 - val_loss: 2.7639\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3881 - val_loss: 2.8672\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3962 - val_loss: 2.8408\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.4007 - val_loss: 2.8580\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4345 - val_loss: 2.7728\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4133 - val_loss: 2.8406\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4492 - val_loss: 2.7675\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4044 - val_loss: 2.8120\n",
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.4139 - val_loss: 2.7662\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3949 - val_loss: 3.0027\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4445 - val_loss: 2.7954\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3830 - val_loss: 2.8959\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4402 - val_loss: 2.7557\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3824 - val_loss: 2.7448\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4107 - val_loss: 2.7931\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4105 - val_loss: 2.9582\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4190 - val_loss: 2.9138\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4125 - val_loss: 2.8294\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3931 - val_loss: 2.8614\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3963 - val_loss: 2.7421\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3822 - val_loss: 2.7304\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3676 - val_loss: 2.7863\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3679 - val_loss: 2.8512\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3978 - val_loss: 2.8586\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3797 - val_loss: 2.7668\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3591 - val_loss: 2.7281\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3428 - val_loss: 2.7972\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3526 - val_loss: 2.8018\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3865 - val_loss: 2.7374\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3617 - val_loss: 2.7452\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3822 - val_loss: 3.0391\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3647 - val_loss: 2.7554\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3727 - val_loss: 2.8696\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3795 - val_loss: 2.7914\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3479 - val_loss: 2.7765\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3312 - val_loss: 2.7894\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3435 - val_loss: 2.8851\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3914 - val_loss: 2.8051\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3537 - val_loss: 2.7409\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3365 - val_loss: 2.8739\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3176 - val_loss: 2.8939\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3320 - val_loss: 2.7328\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3300 - val_loss: 2.8657\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3671 - val_loss: 2.7724\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3219 - val_loss: 2.8914\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3306 - val_loss: 2.7805\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3173 - val_loss: 2.7189\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3407 - val_loss: 2.8405\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3600 - val_loss: 2.9061\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3292 - val_loss: 2.7718\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3166 - val_loss: 2.7408\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3165 - val_loss: 2.7488\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3195 - val_loss: 2.8580\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3453 - val_loss: 2.7876\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3416 - val_loss: 2.9322\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3346 - val_loss: 2.9252\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3091 - val_loss: 2.7705\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2876 - val_loss: 2.7283\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3044 - val_loss: 2.7839\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3129 - val_loss: 2.7477\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3004 - val_loss: 2.8364\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3329 - val_loss: 2.8251\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3085 - val_loss: 2.8440\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2603 - val_loss: 2.8820\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3032 - val_loss: 2.8846\n",
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2972 - val_loss: 2.7140\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2760 - val_loss: 2.8702\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2950 - val_loss: 2.7292\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2571 - val_loss: 2.8389\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2708 - val_loss: 2.9450\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3421 - val_loss: 2.9094\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3074 - val_loss: 2.7634\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2902 - val_loss: 2.7926\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2796 - val_loss: 2.8447\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2500 - val_loss: 2.7772\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2832 - val_loss: 2.8523\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2959 - val_loss: 2.7203\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2647 - val_loss: 2.7561\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2597 - val_loss: 2.8287\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2685 - val_loss: 2.7087\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2689 - val_loss: 2.7439\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3103 - val_loss: 2.7222\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2677 - val_loss: 2.7341\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2393 - val_loss: 2.7258\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2367 - val_loss: 2.7409\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2789 - val_loss: 2.7392\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2709 - val_loss: 2.7891\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2457 - val_loss: 2.7183\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2455 - val_loss: 2.8637\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2786 - val_loss: 2.7121\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2144 - val_loss: 2.7496\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2585 - val_loss: 2.8465\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2451 - val_loss: 2.8456\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2544 - val_loss: 2.9366\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2508 - val_loss: 2.7686\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2268 - val_loss: 2.7201\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2522 - val_loss: 2.9572\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2374 - val_loss: 2.7429\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2177 - val_loss: 2.8500\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2285 - val_loss: 2.7816\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2687 - val_loss: 2.7991\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2180 - val_loss: 2.7296\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2135 - val_loss: 2.9381\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2486 - val_loss: 2.7546\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2142 - val_loss: 2.8067\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2558 - val_loss: 2.8580\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2256 - val_loss: 2.7743\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1801 - val_loss: 2.7700\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1926 - val_loss: 2.7342\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2103 - val_loss: 2.7742\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1899 - val_loss: 2.8165\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2664 - val_loss: 2.7775\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1974 - val_loss: 2.7619\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2008 - val_loss: 2.8043\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1812 - val_loss: 2.8699\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2314 - val_loss: 2.8360\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1867 - val_loss: 2.8192\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2187 - val_loss: 2.8427\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1962 - val_loss: 2.7784\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2005 - val_loss: 2.7452\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2066 - val_loss: 2.8398\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2123 - val_loss: 2.8137\n",
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1934 - val_loss: 2.8350\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1938 - val_loss: 2.9548\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1918 - val_loss: 2.8033\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2071 - val_loss: 2.8116\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1897 - val_loss: 2.8499\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2037 - val_loss: 2.7590\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1926 - val_loss: 2.7468\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1661 - val_loss: 2.9471\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1774 - val_loss: 2.8120\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2000 - val_loss: 2.7562\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1787 - val_loss: 2.7672\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1720 - val_loss: 2.8137\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1601 - val_loss: 2.7643\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2034 - val_loss: 2.8104\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1692 - val_loss: 2.7335\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1512 - val_loss: 2.7096\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1834 - val_loss: 2.7741\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1751 - val_loss: 3.0849\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1826 - val_loss: 2.8692\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1689 - val_loss: 2.8689\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1920 - val_loss: 2.8110\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1839 - val_loss: 2.7033\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1560 - val_loss: 2.7480\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1554 - val_loss: 2.8292\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1379 - val_loss: 2.7651\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1304 - val_loss: 2.8587\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1184 - val_loss: 2.7473\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1409 - val_loss: 2.8744\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1592 - val_loss: 2.8083\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1209 - val_loss: 2.9296\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2007 - val_loss: 2.7095\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1486 - val_loss: 2.8994\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1536 - val_loss: 2.7392\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1116 - val_loss: 2.8850\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1309 - val_loss: 2.7595\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1478 - val_loss: 3.0273\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1412 - val_loss: 2.8373\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1695 - val_loss: 2.8264\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1009 - val_loss: 2.7479\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1151 - val_loss: 2.8172\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1665 - val_loss: 2.8061\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1520 - val_loss: 2.7616\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1011 - val_loss: 2.8759\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1376 - val_loss: 2.9507\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1912 - val_loss: 2.7412\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1053 - val_loss: 2.7233\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1103 - val_loss: 2.8179\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1284 - val_loss: 2.7428\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1349 - val_loss: 2.7498\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0823 - val_loss: 2.8937\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1077 - val_loss: 2.7877\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1073 - val_loss: 2.7480\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1239 - val_loss: 2.8801\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1192 - val_loss: 2.7903\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1003 - val_loss: 2.7561\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0936 - val_loss: 2.9379\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1111 - val_loss: 2.8038\n",
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1110 - val_loss: 2.7952\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0900 - val_loss: 2.7629\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1019 - val_loss: 2.8346\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1344 - val_loss: 2.7557\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1305 - val_loss: 2.8178\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0783 - val_loss: 2.7425\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0747 - val_loss: 2.8692\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1231 - val_loss: 2.7938\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1029 - val_loss: 2.9395\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1052 - val_loss: 2.9675\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1283 - val_loss: 2.8646\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0892 - val_loss: 2.7325\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0591 - val_loss: 2.8671\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0940 - val_loss: 2.8009\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0884 - val_loss: 2.7740\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0788 - val_loss: 2.7637\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1133 - val_loss: 2.7232\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0634 - val_loss: 2.7962\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0781 - val_loss: 2.7534\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0627 - val_loss: 2.8230\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0413 - val_loss: 2.7238\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0682 - val_loss: 2.7259\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1269 - val_loss: 2.7357\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0515 - val_loss: 2.9185\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0655 - val_loss: 2.7478\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0322 - val_loss: 2.8651\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0927 - val_loss: 2.7458\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0479 - val_loss: 2.7925\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1114 - val_loss: 2.8363\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0642 - val_loss: 2.7511\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0444 - val_loss: 2.9608\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1177 - val_loss: 2.7104\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0224 - val_loss: 3.1395\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0714 - val_loss: 2.7670\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0293 - val_loss: 2.9023\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0666 - val_loss: 3.1403\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1377 - val_loss: 2.8903\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0971 - val_loss: 2.9064\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0456 - val_loss: 2.7906\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0738 - val_loss: 2.7687\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0369 - val_loss: 2.7552\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0329 - val_loss: 2.7580\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0355 - val_loss: 2.7462\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0361 - val_loss: 2.7491\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0176 - val_loss: 2.9826\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0478 - val_loss: 2.8841\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0947 - val_loss: 2.7351\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9972 - val_loss: 2.9011\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0097 - val_loss: 2.7908\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0298 - val_loss: 2.8297\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0249 - val_loss: 2.7906\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0220 - val_loss: 2.8121\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0447 - val_loss: 2.9306\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0555 - val_loss: 2.8202\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0581 - val_loss: 2.9230\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0243 - val_loss: 2.7500\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0077 - val_loss: 2.7585\n",
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0331 - val_loss: 2.8315\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0472 - val_loss: 2.8328\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9950 - val_loss: 2.7685\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0062 - val_loss: 2.7845\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0155 - val_loss: 2.9598\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0109 - val_loss: 2.8355\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0083 - val_loss: 2.7779\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0163 - val_loss: 2.8538\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0278 - val_loss: 3.0735\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0608 - val_loss: 2.7782\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0255 - val_loss: 2.9360\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0042 - val_loss: 2.7447\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0159 - val_loss: 2.7524\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9940 - val_loss: 2.9546\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0102 - val_loss: 2.7582\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9979 - val_loss: 2.7909\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0201 - val_loss: 2.8307\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0887 - val_loss: 2.8899\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0416 - val_loss: 2.8120\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9910 - val_loss: 2.8572\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0294 - val_loss: 2.9393\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0089 - val_loss: 2.9120\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.0017 - val_loss: 2.7919\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0108 - val_loss: 2.9381\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9588 - val_loss: 2.8766\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.9961 - val_loss: 3.0230\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0388 - val_loss: 2.7789\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9801 - val_loss: 2.9350\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0257 - val_loss: 2.8136\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0161 - val_loss: 2.8819\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0028 - val_loss: 2.8988\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.9846 - val_loss: 2.8072\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9519 - val_loss: 2.7940\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9779 - val_loss: 2.8282\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9716 - val_loss: 2.7866\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9744 - val_loss: 2.7398\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9823 - val_loss: 2.7427\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9413 - val_loss: 2.8130\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9416 - val_loss: 2.8734\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9550 - val_loss: 2.8361\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.9661 - val_loss: 2.7971\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9538 - val_loss: 2.8237\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9932 - val_loss: 2.8762\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9503 - val_loss: 2.8899\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9679 - val_loss: 2.7955\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9723 - val_loss: 2.8175\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9713 - val_loss: 2.9409\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0080 - val_loss: 2.8399\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9810 - val_loss: 2.8775\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9565 - val_loss: 2.7994\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9428 - val_loss: 2.8691\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9174 - val_loss: 2.7654\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9385 - val_loss: 2.9839\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0294 - val_loss: 2.7723\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9444 - val_loss: 2.7518\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 1.9961 - val_loss: 2.7970\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9694 - val_loss: 2.8484\n",
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 1.9307 - val_loss: 3.0170\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0061 - val_loss: 2.8974\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 1.9294 - val_loss: 2.7826\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 1.9199 - val_loss: 3.0041\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.9883 - val_loss: 2.7692\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9217 - val_loss: 2.8209\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9426 - val_loss: 2.8189\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9383 - val_loss: 2.9430\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9378 - val_loss: 2.8118\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9484 - val_loss: 2.8703\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9270 - val_loss: 2.7938\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9117 - val_loss: 2.7966\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9161 - val_loss: 2.8714\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9427 - val_loss: 2.9055\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9583 - val_loss: 2.8694\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9268 - val_loss: 2.8660\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9281 - val_loss: 3.0072\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9631 - val_loss: 2.7864\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9447 - val_loss: 2.8196\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9090 - val_loss: 2.7993\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8968 - val_loss: 2.8743\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8981 - val_loss: 2.9052\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9987 - val_loss: 2.8321\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9453 - val_loss: 2.8994\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9231 - val_loss: 2.9511\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9347 - val_loss: 2.8253\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8994 - val_loss: 2.8032\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9280 - val_loss: 2.8604\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9600 - val_loss: 2.9177\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9133 - val_loss: 2.9443\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9366 - val_loss: 2.8629\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9334 - val_loss: 2.8844\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9360 - val_loss: 2.9207\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9498 - val_loss: 2.8434\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9445 - val_loss: 2.7842\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9252 - val_loss: 2.8416\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9140 - val_loss: 2.9583\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9284 - val_loss: 2.9649\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9242 - val_loss: 2.8025\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9252 - val_loss: 2.8876\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8988 - val_loss: 2.9073\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9146 - val_loss: 2.7924\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8800 - val_loss: 2.8226\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8889 - val_loss: 2.9053\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8798 - val_loss: 2.8213\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9205 - val_loss: 2.8866\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8669 - val_loss: 2.8137\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9071 - val_loss: 2.7993\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9033 - val_loss: 2.8507\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9051 - val_loss: 3.0518\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9384 - val_loss: 3.2216\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9719 - val_loss: 2.8579\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8744 - val_loss: 2.9445\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9037 - val_loss: 2.8217\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8957 - val_loss: 2.8356\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9579 - val_loss: 3.0287\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8929 - val_loss: 2.9544\n",
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8751 - val_loss: 2.8011\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8876 - val_loss: 2.9728\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8551 - val_loss: 2.7941\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8627 - val_loss: 2.9376\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8983 - val_loss: 2.8365\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8736 - val_loss: 2.8467\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 1.8911 - val_loss: 2.8755\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 1.9114 - val_loss: 3.0051\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.8978 - val_loss: 2.8512\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8630 - val_loss: 2.9849\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9051 - val_loss: 3.0038\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8710 - val_loss: 2.9396\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8444 - val_loss: 2.9937\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9227 - val_loss: 2.8913\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8853 - val_loss: 2.8296\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8905 - val_loss: 2.9615\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8926 - val_loss: 2.8324\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8513 - val_loss: 3.0002\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8637 - val_loss: 2.9443\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9251 - val_loss: 3.0248\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8866 - val_loss: 2.8591\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8496 - val_loss: 2.9858\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9028 - val_loss: 2.8686\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8248 - val_loss: 2.8524\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9218 - val_loss: 2.8717\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8509 - val_loss: 2.9209\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8738 - val_loss: 2.9610\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8291 - val_loss: 2.8536\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8347 - val_loss: 2.8655\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8635 - val_loss: 2.8584\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8236 - val_loss: 2.8684\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9086 - val_loss: 2.9982\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9138 - val_loss: 2.8809\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8437 - val_loss: 2.8893\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8424 - val_loss: 2.8838\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.8323 - val_loss: 2.8245\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8527 - val_loss: 2.9377\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8521 - val_loss: 2.8500\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8421 - val_loss: 3.0158\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9130 - val_loss: 3.0197\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9032 - val_loss: 2.7981\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8402 - val_loss: 2.8646\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8898 - val_loss: 2.9117\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8111 - val_loss: 2.8992\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8067 - val_loss: 2.9098\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8323 - val_loss: 2.9282\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8106 - val_loss: 2.8868\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8010 - val_loss: 2.9493\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8420 - val_loss: 2.8383\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8177 - val_loss: 2.9687\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8276 - val_loss: 2.7924\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8270 - val_loss: 2.9699\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8338 - val_loss: 2.8987\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.7844 - val_loss: 2.9486\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8111 - val_loss: 2.8661\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7995 - val_loss: 2.9025\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8247 - val_loss: 2.8878\n",
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7891 - val_loss: 2.9919\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8095 - val_loss: 2.9186\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8008 - val_loss: 2.9619\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8734 - val_loss: 2.8508\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8417 - val_loss: 2.9519\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8316 - val_loss: 2.9191\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8224 - val_loss: 2.9922\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8224 - val_loss: 2.8929\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8703 - val_loss: 2.9782\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8493 - val_loss: 2.8590\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.7698 - val_loss: 3.0282\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8549 - val_loss: 2.8104\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.7896 - val_loss: 2.9422\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.7668 - val_loss: 2.9044\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8020 - val_loss: 2.9522\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8185 - val_loss: 2.9035\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.7803 - val_loss: 2.8830\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8190 - val_loss: 2.8682\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.8074 - val_loss: 2.8193\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8936 - val_loss: 2.9623\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7882 - val_loss: 2.8403\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.7890 - val_loss: 2.8570\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8079 - val_loss: 2.8822\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8434 - val_loss: 3.1139\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.8235 - val_loss: 2.9457\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8183 - val_loss: 2.8773\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8368 - val_loss: 2.8606\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.7701 - val_loss: 2.8578\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.7909 - val_loss: 2.8696\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8153 - val_loss: 2.9464\n",
      "Epoch 1/600\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 50.1900 - val_loss: 35.8813\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 24.6064 - val_loss: 17.1501\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 15.8414 - val_loss: 12.0768\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 10.0103 - val_loss: 7.9047\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 8.2035 - val_loss: 7.4697\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 7.7204 - val_loss: 6.9145\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 7.0767 - val_loss: 6.3171\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 6.3557 - val_loss: 5.5043\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 5.5823 - val_loss: 4.9167\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.9554 - val_loss: 4.3834\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 4.4767 - val_loss: 4.0129\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.1774 - val_loss: 3.9089\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.9350 - val_loss: 3.7796\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.8010 - val_loss: 3.6886\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.6782 - val_loss: 3.6370\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.5769 - val_loss: 3.4995\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.4634 - val_loss: 3.4547\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.3617 - val_loss: 3.3737\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.3243 - val_loss: 3.3285\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.2610 - val_loss: 3.3223\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.1909 - val_loss: 3.2560\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.1405 - val_loss: 3.2193\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.1094 - val_loss: 3.4262\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 3.1454 - val_loss: 3.1750\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 3.0546 - val_loss: 3.1403\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0389 - val_loss: 3.1623\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.0070 - val_loss: 3.1943\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.9976 - val_loss: 3.1115\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.9858 - val_loss: 3.0120\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9655 - val_loss: 3.0169\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.9568 - val_loss: 3.0391\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.9281 - val_loss: 3.0151\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9211 - val_loss: 2.9720\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.9157 - val_loss: 3.0648\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.9374 - val_loss: 2.9256\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8948 - val_loss: 2.8962\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.9159 - val_loss: 3.0025\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8767 - val_loss: 2.9566\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8907 - val_loss: 3.1101\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8863 - val_loss: 3.0388\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8693 - val_loss: 2.8852\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.8339 - val_loss: 2.8823\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.8587 - val_loss: 2.9115\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8267 - val_loss: 2.8973\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7978 - val_loss: 3.0229\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.8063 - val_loss: 2.8551\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7993 - val_loss: 2.8349\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8031 - val_loss: 2.8454\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.7786 - val_loss: 2.8754\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8260 - val_loss: 2.9376\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.8064 - val_loss: 2.9106\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7738 - val_loss: 2.9579\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.8391 - val_loss: 2.8484\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7457 - val_loss: 2.8314\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7271 - val_loss: 2.8994\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7945 - val_loss: 2.9330\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7595 - val_loss: 2.8443\n",
      "Epoch 58/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7808 - val_loss: 2.9133\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7301 - val_loss: 2.8778\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7354 - val_loss: 2.9307\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7742 - val_loss: 2.8330\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6951 - val_loss: 2.8581\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7236 - val_loss: 2.7869\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7122 - val_loss: 2.9957\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7539 - val_loss: 2.8977\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7049 - val_loss: 2.7788\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7069 - val_loss: 2.7765\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6899 - val_loss: 2.7974\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6987 - val_loss: 2.7459\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6874 - val_loss: 2.8041\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6894 - val_loss: 2.8434\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7076 - val_loss: 2.7506\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6808 - val_loss: 2.7867\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6949 - val_loss: 2.8619\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7129 - val_loss: 2.7853\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6728 - val_loss: 2.7931\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7001 - val_loss: 2.7803\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6640 - val_loss: 2.7780\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6609 - val_loss: 2.9188\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6517 - val_loss: 2.7863\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6582 - val_loss: 2.7671\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6452 - val_loss: 2.8128\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6320 - val_loss: 2.8478\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6767 - val_loss: 2.7756\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6550 - val_loss: 2.7383\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6447 - val_loss: 2.8202\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6759 - val_loss: 2.8819\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6353 - val_loss: 2.7292\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6116 - val_loss: 2.8068\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6449 - val_loss: 2.7571\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6391 - val_loss: 2.7306\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6104 - val_loss: 2.7134\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6035 - val_loss: 2.7645\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.5987 - val_loss: 2.8052\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6240 - val_loss: 2.8118\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6289 - val_loss: 2.7260\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5831 - val_loss: 2.6710\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6276 - val_loss: 2.7347\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5994 - val_loss: 2.7587\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6018 - val_loss: 2.6913\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6165 - val_loss: 2.7513\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5855 - val_loss: 2.8032\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6025 - val_loss: 2.7208\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5849 - val_loss: 2.7492\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5928 - val_loss: 2.7802\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5886 - val_loss: 2.7981\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5764 - val_loss: 2.7571\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5945 - val_loss: 2.7040\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5876 - val_loss: 2.7072\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5571 - val_loss: 2.7085\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5710 - val_loss: 2.6764\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5572 - val_loss: 2.7343\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5727 - val_loss: 2.7435\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5751 - val_loss: 2.7715\n",
      "Epoch 115/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5588 - val_loss: 2.7566\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5595 - val_loss: 2.8494\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6129 - val_loss: 2.6601\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5353 - val_loss: 2.7231\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5469 - val_loss: 2.7136\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5502 - val_loss: 2.7427\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5871 - val_loss: 2.7284\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5667 - val_loss: 2.6788\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5423 - val_loss: 2.8575\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5980 - val_loss: 2.6865\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5447 - val_loss: 2.6835\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5548 - val_loss: 2.7293\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5567 - val_loss: 2.6598\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5246 - val_loss: 2.7334\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5413 - val_loss: 2.6647\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5345 - val_loss: 2.7509\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5522 - val_loss: 2.7389\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5468 - val_loss: 2.6830\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5308 - val_loss: 2.7550\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5108 - val_loss: 2.7575\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5456 - val_loss: 2.6761\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5342 - val_loss: 2.6983\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5356 - val_loss: 2.7191\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5103 - val_loss: 2.7013\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5092 - val_loss: 2.7667\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5370 - val_loss: 2.6858\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5208 - val_loss: 2.6841\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5352 - val_loss: 2.6980\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5175 - val_loss: 2.7554\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5243 - val_loss: 2.7176\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5207 - val_loss: 2.7021\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5288 - val_loss: 2.6941\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4959 - val_loss: 2.6619\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5193 - val_loss: 2.7090\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4955 - val_loss: 2.7141\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4823 - val_loss: 2.6927\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4871 - val_loss: 2.6739\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5043 - val_loss: 2.7591\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5312 - val_loss: 2.7277\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.5115 - val_loss: 2.6661\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4749 - val_loss: 2.6840\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4742 - val_loss: 2.6973\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4932 - val_loss: 2.7055\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4917 - val_loss: 2.6811\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4919 - val_loss: 2.6470\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4896 - val_loss: 2.6785\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4851 - val_loss: 2.6889\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4967 - val_loss: 2.7563\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.4878 - val_loss: 2.7135\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4933 - val_loss: 2.7023\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4706 - val_loss: 2.6859\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4543 - val_loss: 2.6625\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4715 - val_loss: 2.6681\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4789 - val_loss: 2.7398\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4830 - val_loss: 2.6754\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4661 - val_loss: 2.6854\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4749 - val_loss: 2.6341\n",
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4559 - val_loss: 2.7981\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4485 - val_loss: 2.8491\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4829 - val_loss: 2.6782\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4506 - val_loss: 2.6586\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4505 - val_loss: 2.6677\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4688 - val_loss: 2.7033\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4637 - val_loss: 2.7250\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4573 - val_loss: 2.7051\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4383 - val_loss: 2.7266\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4632 - val_loss: 2.8300\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4603 - val_loss: 2.6605\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4698 - val_loss: 2.6313\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4196 - val_loss: 2.6781\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4252 - val_loss: 2.6392\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4530 - val_loss: 2.7402\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4480 - val_loss: 2.7113\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4473 - val_loss: 2.6981\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4520 - val_loss: 2.6572\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4288 - val_loss: 2.6460\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4228 - val_loss: 2.6744\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.4220 - val_loss: 2.7020\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4308 - val_loss: 2.8329\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4413 - val_loss: 2.6361\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4074 - val_loss: 2.6450\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4134 - val_loss: 2.6831\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4184 - val_loss: 2.7023\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4059 - val_loss: 2.7198\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4248 - val_loss: 2.8428\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4565 - val_loss: 2.7372\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4280 - val_loss: 2.6418\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4069 - val_loss: 2.6510\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3917 - val_loss: 2.6480\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4078 - val_loss: 2.6643\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4000 - val_loss: 2.6438\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4002 - val_loss: 2.6759\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4349 - val_loss: 2.6706\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4018 - val_loss: 2.7208\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4152 - val_loss: 2.6475\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3889 - val_loss: 2.6635\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3779 - val_loss: 2.7506\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4163 - val_loss: 2.7848\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4008 - val_loss: 2.6892\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3988 - val_loss: 2.6664\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4071 - val_loss: 2.6571\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3922 - val_loss: 2.6841\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4248 - val_loss: 2.7636\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4256 - val_loss: 2.6560\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4083 - val_loss: 2.8021\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4175 - val_loss: 2.6660\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3725 - val_loss: 2.8061\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3948 - val_loss: 2.7014\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3786 - val_loss: 2.7051\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3798 - val_loss: 2.7787\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3638 - val_loss: 2.7735\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3878 - val_loss: 2.6867\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3663 - val_loss: 2.6903\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3814 - val_loss: 2.7080\n",
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3698 - val_loss: 2.7125\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3698 - val_loss: 2.7252\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3788 - val_loss: 2.6889\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3578 - val_loss: 2.7022\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3607 - val_loss: 2.6779\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3688 - val_loss: 2.6898\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3829 - val_loss: 2.6555\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3683 - val_loss: 2.6917\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3703 - val_loss: 2.6700\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3532 - val_loss: 2.6955\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3473 - val_loss: 2.6903\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3421 - val_loss: 2.6614\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3396 - val_loss: 2.7430\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3592 - val_loss: 2.6775\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3342 - val_loss: 2.6278\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3274 - val_loss: 2.6578\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3385 - val_loss: 2.6309\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3399 - val_loss: 2.7042\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3430 - val_loss: 2.6419\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3382 - val_loss: 2.6585\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3409 - val_loss: 2.7176\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3478 - val_loss: 2.6921\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3347 - val_loss: 2.6902\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3703 - val_loss: 2.7191\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3039 - val_loss: 2.7639\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3283 - val_loss: 2.7678\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3109 - val_loss: 2.7071\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3130 - val_loss: 2.6716\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3309 - val_loss: 2.6593\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3120 - val_loss: 2.6629\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3403 - val_loss: 2.8443\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3423 - val_loss: 2.6771\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3145 - val_loss: 2.7152\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3035 - val_loss: 2.6987\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3469 - val_loss: 2.7309\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3015 - val_loss: 2.6487\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3031 - val_loss: 2.6840\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3104 - val_loss: 2.6996\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3023 - val_loss: 2.6861\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2998 - val_loss: 2.6843\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3001 - val_loss: 2.7146\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3088 - val_loss: 2.6476\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3065 - val_loss: 2.8130\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3140 - val_loss: 2.7156\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3210 - val_loss: 2.7139\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3049 - val_loss: 2.7213\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2878 - val_loss: 2.6958\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3063 - val_loss: 2.9729\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3379 - val_loss: 2.6738\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2759 - val_loss: 2.7391\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3252 - val_loss: 2.8086\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2989 - val_loss: 2.7151\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2905 - val_loss: 2.7116\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2856 - val_loss: 2.7225\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2931 - val_loss: 2.7004\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3199 - val_loss: 2.6801\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3086 - val_loss: 2.7826\n",
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3061 - val_loss: 2.6652\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2808 - val_loss: 2.6632\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2657 - val_loss: 2.7004\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3005 - val_loss: 2.6861\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2635 - val_loss: 2.6845\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2691 - val_loss: 2.6983\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3110 - val_loss: 2.6982\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2927 - val_loss: 2.6742\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2755 - val_loss: 2.7324\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2651 - val_loss: 2.7240\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2682 - val_loss: 2.6759\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2868 - val_loss: 2.8427\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2805 - val_loss: 2.7785\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2886 - val_loss: 2.6911\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2530 - val_loss: 2.7402\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2729 - val_loss: 2.7961\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2779 - val_loss: 2.6932\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2379 - val_loss: 2.6869\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2710 - val_loss: 2.6567\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2605 - val_loss: 2.7558\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2600 - val_loss: 2.7582\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2419 - val_loss: 2.6247\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2562 - val_loss: 2.6859\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2781 - val_loss: 2.7038\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2596 - val_loss: 2.7929\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2597 - val_loss: 2.8133\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2717 - val_loss: 2.6629\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2515 - val_loss: 2.7653\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2737 - val_loss: 2.9775\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3398 - val_loss: 2.6866\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2417 - val_loss: 2.7395\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2913 - val_loss: 2.8744\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3095 - val_loss: 2.6608\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2396 - val_loss: 2.6489\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2486 - val_loss: 2.7507\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2362 - val_loss: 2.6749\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2289 - val_loss: 2.6913\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2456 - val_loss: 2.6561\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2352 - val_loss: 2.6198\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2586 - val_loss: 2.6938\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2324 - val_loss: 2.6656\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2261 - val_loss: 2.7118\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2813 - val_loss: 2.6894\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2233 - val_loss: 2.6393\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2067 - val_loss: 2.6935\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2656 - val_loss: 2.6965\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2214 - val_loss: 2.6687\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.2061 - val_loss: 2.8047\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2487 - val_loss: 2.6684\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2324 - val_loss: 2.6573\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2107 - val_loss: 2.7125\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2357 - val_loss: 2.6361\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2248 - val_loss: 2.6804\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1927 - val_loss: 2.6338\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.1992 - val_loss: 2.6937\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2156 - val_loss: 2.7317\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2127 - val_loss: 2.8048\n",
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2630 - val_loss: 2.6449\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 2.1989 - val_loss: 2.6469\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2281 - val_loss: 2.6759\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2071 - val_loss: 2.7401\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2348 - val_loss: 2.6632\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1944 - val_loss: 2.7204\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2083 - val_loss: 2.7394\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2270 - val_loss: 2.7585\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2191 - val_loss: 2.7540\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2355 - val_loss: 2.6833\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1984 - val_loss: 2.7012\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1949 - val_loss: 2.7101\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2263 - val_loss: 2.7794\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2081 - val_loss: 2.7085\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2386 - val_loss: 2.9179\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2571 - val_loss: 2.6629\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1975 - val_loss: 2.6679\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1923 - val_loss: 2.6915\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2047 - val_loss: 2.8708\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2588 - val_loss: 2.6585\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1846 - val_loss: 2.9919\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2635 - val_loss: 2.7527\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1817 - val_loss: 2.7555\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1889 - val_loss: 2.7705\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1948 - val_loss: 2.7063\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1881 - val_loss: 2.6652\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1949 - val_loss: 2.7559\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2018 - val_loss: 2.7575\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1932 - val_loss: 2.7212\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1902 - val_loss: 2.7715\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1949 - val_loss: 2.7292\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1746 - val_loss: 2.6871\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1636 - val_loss: 2.6721\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1737 - val_loss: 2.6463\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1656 - val_loss: 2.7288\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1754 - val_loss: 2.6621\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1695 - val_loss: 2.7396\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1562 - val_loss: 2.7906\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1867 - val_loss: 2.6669\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1777 - val_loss: 2.6867\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1874 - val_loss: 2.6936\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1550 - val_loss: 2.6580\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1505 - val_loss: 2.6940\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1668 - val_loss: 2.7234\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1547 - val_loss: 2.6960\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1860 - val_loss: 2.7454\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1648 - val_loss: 2.7648\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1710 - val_loss: 2.7344\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1528 - val_loss: 2.7624\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1859 - val_loss: 2.7151\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1661 - val_loss: 2.7119\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1452 - val_loss: 2.7015\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1547 - val_loss: 2.6719\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1375 - val_loss: 2.6549\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1495 - val_loss: 2.6743\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1802 - val_loss: 2.7075\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1501 - val_loss: 2.7047\n",
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1492 - val_loss: 2.7388\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1395 - val_loss: 2.6661\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1426 - val_loss: 2.7591\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1308 - val_loss: 2.6826\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1320 - val_loss: 2.7022\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1760 - val_loss: 2.7027\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1488 - val_loss: 2.7021\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1730 - val_loss: 2.8404\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1396 - val_loss: 2.6791\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1410 - val_loss: 2.7297\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1528 - val_loss: 2.6825\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1244 - val_loss: 2.7024\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1460 - val_loss: 2.6762\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1230 - val_loss: 2.7247\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1345 - val_loss: 2.6805\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1299 - val_loss: 2.6831\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1553 - val_loss: 2.7165\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1145 - val_loss: 2.6772\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1115 - val_loss: 2.6684\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1222 - val_loss: 2.6701\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1154 - val_loss: 2.6653\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1138 - val_loss: 2.7265\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1171 - val_loss: 2.7025\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1182 - val_loss: 2.6929\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1382 - val_loss: 2.8524\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1496 - val_loss: 2.6837\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1114 - val_loss: 2.6575\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1590 - val_loss: 2.7168\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1324 - val_loss: 2.6902\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1487 - val_loss: 2.6600\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1253 - val_loss: 2.7817\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1092 - val_loss: 2.7257\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0950 - val_loss: 2.7128\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1073 - val_loss: 2.7917\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1079 - val_loss: 2.7166\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1109 - val_loss: 2.7541\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1627 - val_loss: 2.7612\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1474 - val_loss: 2.6755\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1049 - val_loss: 2.7661\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1446 - val_loss: 2.7552\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1096 - val_loss: 2.8168\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1586 - val_loss: 2.6950\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0728 - val_loss: 2.7802\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1313 - val_loss: 2.9089\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1540 - val_loss: 2.7436\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1328 - val_loss: 2.7592\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1121 - val_loss: 2.7604\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1109 - val_loss: 2.7644\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1503 - val_loss: 2.6916\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0979 - val_loss: 2.6863\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1026 - val_loss: 2.7352\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0998 - val_loss: 2.6918\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0816 - val_loss: 2.6932\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0830 - val_loss: 2.6849\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0771 - val_loss: 2.6866\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0697 - val_loss: 2.7256\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0614 - val_loss: 2.7205\n",
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0931 - val_loss: 2.6968\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0822 - val_loss: 2.6975\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0743 - val_loss: 2.7119\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1118 - val_loss: 2.7270\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0818 - val_loss: 2.7011\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0650 - val_loss: 2.7212\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1008 - val_loss: 2.7633\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1078 - val_loss: 2.6643\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0973 - val_loss: 2.6801\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0682 - val_loss: 2.8172\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1108 - val_loss: 2.6639\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0539 - val_loss: 2.7002\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0746 - val_loss: 2.6690\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0771 - val_loss: 2.7760\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1293 - val_loss: 2.7455\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0763 - val_loss: 2.6573\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0705 - val_loss: 2.8234\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1032 - val_loss: 2.7379\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1087 - val_loss: 2.7312\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0655 - val_loss: 2.7757\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1180 - val_loss: 2.7691\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0597 - val_loss: 2.6906\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0786 - val_loss: 2.7981\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0759 - val_loss: 2.6828\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0489 - val_loss: 2.8817\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0735 - val_loss: 2.6811\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0618 - val_loss: 2.6924\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0775 - val_loss: 2.6889\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0448 - val_loss: 2.7358\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0549 - val_loss: 2.9432\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1134 - val_loss: 2.8706\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0973 - val_loss: 2.7029\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0509 - val_loss: 2.7000\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1018 - val_loss: 2.7386\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0356 - val_loss: 2.7175\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0526 - val_loss: 2.7292\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0525 - val_loss: 2.7485\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0429 - val_loss: 2.7197\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0420 - val_loss: 2.6882\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0618 - val_loss: 2.7243\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0384 - val_loss: 2.7604\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0346 - val_loss: 2.7816\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0478 - val_loss: 2.9577\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0822 - val_loss: 2.7451\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0413 - val_loss: 2.6976\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0451 - val_loss: 2.8409\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.0222 - val_loss: 2.7180\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0587 - val_loss: 2.8230\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0382 - val_loss: 2.7592\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0495 - val_loss: 2.7896\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0456 - val_loss: 2.7753\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0559 - val_loss: 2.8527\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0655 - val_loss: 2.7609\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0195 - val_loss: 2.6905\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0406 - val_loss: 2.7882\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0143 - val_loss: 2.7690\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0513 - val_loss: 2.7791\n",
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0435 - val_loss: 2.8012\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0409 - val_loss: 2.7140\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0150 - val_loss: 2.7976\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0415 - val_loss: 2.6906\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0328 - val_loss: 2.7833\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0417 - val_loss: 2.7190\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0765 - val_loss: 2.8048\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0512 - val_loss: 2.7442\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0272 - val_loss: 2.7198\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0213 - val_loss: 2.8089\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0570 - val_loss: 2.7418\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0116 - val_loss: 2.7172\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0256 - val_loss: 2.8233\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0331 - val_loss: 2.6997\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0333 - val_loss: 2.6854\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9989 - val_loss: 2.8343\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0475 - val_loss: 2.7283\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0179 - val_loss: 2.7451\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0003 - val_loss: 2.7362\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0062 - val_loss: 2.7158\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0154 - val_loss: 2.6987\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0147 - val_loss: 2.6988\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0081 - val_loss: 2.7236\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0034 - val_loss: 2.7197\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0140 - val_loss: 2.7251\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0104 - val_loss: 2.6590\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0116 - val_loss: 2.6792\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9822 - val_loss: 2.7326\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0103 - val_loss: 2.7716\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0447 - val_loss: 2.7277\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0055 - val_loss: 2.7659\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0033 - val_loss: 2.8573\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0109 - val_loss: 2.7315\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9825 - val_loss: 2.7413\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9970 - val_loss: 2.7446\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9988 - val_loss: 2.7342\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9950 - val_loss: 2.8143\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0351 - val_loss: 2.8324\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9884 - val_loss: 2.7560\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9788 - val_loss: 2.7087\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9602 - val_loss: 2.7771\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0075 - val_loss: 2.7520\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0133 - val_loss: 2.7495\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9944 - val_loss: 2.8597\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0014 - val_loss: 2.7485\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9672 - val_loss: 2.7892\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9979 - val_loss: 2.7614\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9842 - val_loss: 2.8668\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0150 - val_loss: 2.8373\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0154 - val_loss: 2.7421\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9760 - val_loss: 2.7790\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9695 - val_loss: 2.7720\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9733 - val_loss: 2.7216\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9607 - val_loss: 2.9693\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0439 - val_loss: 2.7843\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9728 - val_loss: 2.7840\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9610 - val_loss: 2.7607\n",
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9895 - val_loss: 3.0266\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0393 - val_loss: 2.7831\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9434 - val_loss: 2.7317\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9456 - val_loss: 2.7679\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9971 - val_loss: 2.7245\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9512 - val_loss: 2.7000\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9590 - val_loss: 2.7841\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9518 - val_loss: 2.7151\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9465 - val_loss: 2.8049\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9562 - val_loss: 2.8695\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9913 - val_loss: 2.8620\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0422 - val_loss: 2.7568\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9840 - val_loss: 2.8153\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0118 - val_loss: 2.7286\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9762 - val_loss: 2.7009\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9556 - val_loss: 2.7884\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9554 - val_loss: 2.6810\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9498 - val_loss: 2.7002\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9431 - val_loss: 2.7622\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9651 - val_loss: 2.7013\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9582 - val_loss: 2.8770\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9719 - val_loss: 2.8352\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9953 - val_loss: 2.6921\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9461 - val_loss: 2.7295\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9511 - val_loss: 2.8122\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9729 - val_loss: 2.7045\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9303 - val_loss: 2.7129\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9118 - val_loss: 2.7264\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9417 - val_loss: 2.8271\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9564 - val_loss: 2.8234\n",
      "Epoch 1/600\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 47.6379 - val_loss: 31.8514\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 21.1444 - val_loss: 17.5987\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 13.9624 - val_loss: 10.9858\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 8.6274 - val_loss: 8.6189\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 7.7548 - val_loss: 8.2069\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 7.2681 - val_loss: 7.4973\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 6.6493 - val_loss: 6.8248\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.8926 - val_loss: 5.8838\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 5.1799 - val_loss: 5.1958\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 4.6809 - val_loss: 4.6569\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 4.2918 - val_loss: 4.3382\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 4.0845 - val_loss: 4.1495\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 3.8420 - val_loss: 3.8894\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.7194 - val_loss: 3.9439\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.6107 - val_loss: 3.8871\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.5781 - val_loss: 3.6386\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.4061 - val_loss: 3.5915\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.3220 - val_loss: 3.5000\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2643 - val_loss: 3.5882\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2456 - val_loss: 3.6419\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2022 - val_loss: 3.5493\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.2224 - val_loss: 3.4150\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0717 - val_loss: 3.3921\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.1688 - val_loss: 3.3776\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0437 - val_loss: 3.2543\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0204 - val_loss: 3.2934\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0245 - val_loss: 3.2331\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9741 - val_loss: 3.3943\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0092 - val_loss: 3.2597\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0020 - val_loss: 3.1810\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9843 - val_loss: 3.3448\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.9207 - val_loss: 3.4427\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.9432 - val_loss: 3.1631\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8562 - val_loss: 3.0587\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8768 - val_loss: 3.2809\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.8654 - val_loss: 3.1185\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8322 - val_loss: 3.1193\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8439 - val_loss: 3.1275\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8330 - val_loss: 3.1580\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8153 - val_loss: 3.1120\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8033 - val_loss: 3.0254\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8420 - val_loss: 3.0176\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8035 - val_loss: 3.0439\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7777 - val_loss: 3.0536\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7914 - val_loss: 3.1027\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7668 - val_loss: 3.0116\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7868 - val_loss: 3.0248\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7617 - val_loss: 3.1906\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7612 - val_loss: 3.0330\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7370 - val_loss: 2.9637\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7400 - val_loss: 3.0205\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7753 - val_loss: 2.9830\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7269 - val_loss: 3.1118\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7065 - val_loss: 3.1155\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7475 - val_loss: 2.9804\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7122 - val_loss: 3.0026\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7412 - val_loss: 2.9767\n",
      "Epoch 58/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7081 - val_loss: 2.9887\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7380 - val_loss: 2.9592\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7090 - val_loss: 2.9610\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7121 - val_loss: 3.0334\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6811 - val_loss: 2.9897\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6650 - val_loss: 3.0519\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6684 - val_loss: 2.9570\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6824 - val_loss: 2.9283\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6955 - val_loss: 3.0007\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7034 - val_loss: 3.0009\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6832 - val_loss: 2.8969\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6448 - val_loss: 3.0857\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6714 - val_loss: 2.9570\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6759 - val_loss: 2.9073\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6426 - val_loss: 3.0606\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6322 - val_loss: 2.9216\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6298 - val_loss: 2.9618\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.6279 - val_loss: 2.8905\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6582 - val_loss: 2.9193\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6403 - val_loss: 2.8728\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6312 - val_loss: 2.8417\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6469 - val_loss: 3.0520\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6383 - val_loss: 2.9990\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6137 - val_loss: 2.9875\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6302 - val_loss: 2.8775\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6104 - val_loss: 2.9111\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6256 - val_loss: 2.9706\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5936 - val_loss: 2.8889\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6117 - val_loss: 2.8932\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6043 - val_loss: 2.9685\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5753 - val_loss: 2.8959\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6132 - val_loss: 2.8675\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6081 - val_loss: 2.9813\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6045 - val_loss: 2.9594\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6323 - val_loss: 2.9733\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5616 - val_loss: 2.9973\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6161 - val_loss: 2.8738\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5852 - val_loss: 2.8282\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5979 - val_loss: 2.8598\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5592 - val_loss: 2.8851\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5603 - val_loss: 3.0256\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5766 - val_loss: 3.0307\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5732 - val_loss: 2.9656\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5679 - val_loss: 2.9318\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5495 - val_loss: 2.9599\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5447 - val_loss: 2.8639\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5760 - val_loss: 2.9460\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5392 - val_loss: 2.9457\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5534 - val_loss: 2.9670\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5867 - val_loss: 2.8393\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5510 - val_loss: 2.9144\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5531 - val_loss: 2.8998\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.5258 - val_loss: 2.9112\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5412 - val_loss: 2.9427\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5594 - val_loss: 2.8691\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.5551 - val_loss: 2.8200\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5369 - val_loss: 2.8853\n",
      "Epoch 115/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5500 - val_loss: 2.8074\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5050 - val_loss: 3.0660\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5410 - val_loss: 3.1464\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5562 - val_loss: 2.9612\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5061 - val_loss: 2.8393\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5081 - val_loss: 2.9042\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5189 - val_loss: 2.8728\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5122 - val_loss: 2.8820\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5111 - val_loss: 2.9502\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5023 - val_loss: 2.8530\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5206 - val_loss: 2.8642\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5218 - val_loss: 2.9846\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5086 - val_loss: 2.7964\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4964 - val_loss: 2.8690\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5116 - val_loss: 2.8463\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4978 - val_loss: 2.8054\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5008 - val_loss: 2.9463\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4979 - val_loss: 2.8057\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4593 - val_loss: 2.9662\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5043 - val_loss: 2.8795\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.5035 - val_loss: 2.8546\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4633 - val_loss: 2.9237\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4991 - val_loss: 2.9654\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5022 - val_loss: 3.0135\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4951 - val_loss: 2.8933\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4687 - val_loss: 2.8958\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4684 - val_loss: 2.8939\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4749 - val_loss: 2.8401\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4589 - val_loss: 2.9328\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4629 - val_loss: 2.8408\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4756 - val_loss: 2.7809\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4584 - val_loss: 2.8474\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4622 - val_loss: 2.7970\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4570 - val_loss: 2.8545\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4426 - val_loss: 2.8242\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4579 - val_loss: 2.9416\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4663 - val_loss: 2.8672\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4516 - val_loss: 2.9446\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4816 - val_loss: 2.8057\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4506 - val_loss: 2.9480\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4581 - val_loss: 2.9365\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4413 - val_loss: 2.8289\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4382 - val_loss: 2.8322\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4524 - val_loss: 2.7966\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4424 - val_loss: 2.7853\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4272 - val_loss: 2.7895\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4429 - val_loss: 2.8259\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4296 - val_loss: 2.8030\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4431 - val_loss: 2.8148\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4195 - val_loss: 2.8123\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4312 - val_loss: 2.8408\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3988 - val_loss: 2.9335\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4192 - val_loss: 2.8007\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4258 - val_loss: 2.8328\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4445 - val_loss: 2.8394\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4609 - val_loss: 2.7856\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3931 - val_loss: 2.8613\n",
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3969 - val_loss: 2.8640\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4291 - val_loss: 2.8734\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4195 - val_loss: 2.9449\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4315 - val_loss: 2.8213\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4069 - val_loss: 2.9561\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4126 - val_loss: 2.8065\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3994 - val_loss: 2.9722\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.4168 - val_loss: 2.8095\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.3766 - val_loss: 2.8261\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3865 - val_loss: 2.8070\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4030 - val_loss: 2.8403\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3726 - val_loss: 2.8009\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4079 - val_loss: 2.8571\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3937 - val_loss: 2.8403\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3770 - val_loss: 3.0016\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3957 - val_loss: 2.8056\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3769 - val_loss: 2.8294\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3830 - val_loss: 2.9065\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3950 - val_loss: 2.7813\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3719 - val_loss: 2.8323\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3855 - val_loss: 2.9358\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3644 - val_loss: 2.9231\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3828 - val_loss: 2.8074\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.3574 - val_loss: 2.7932\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3823 - val_loss: 2.8246\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3510 - val_loss: 2.7900\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3487 - val_loss: 2.8040\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3502 - val_loss: 2.8496\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3602 - val_loss: 2.7797\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3498 - val_loss: 2.8496\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3593 - val_loss: 2.8039\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3644 - val_loss: 2.7994\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.3625 - val_loss: 2.8301\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3370 - val_loss: 2.7975\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3728 - val_loss: 2.8475\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3299 - val_loss: 2.8516\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3562 - val_loss: 2.9616\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3503 - val_loss: 2.8494\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3450 - val_loss: 2.9632\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3593 - val_loss: 2.8523\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3302 - val_loss: 2.8298\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3569 - val_loss: 2.7784\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3366 - val_loss: 2.8329\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3399 - val_loss: 2.8248\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3367 - val_loss: 2.7575\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3463 - val_loss: 2.8191\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3303 - val_loss: 2.7812\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3377 - val_loss: 2.7983\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3148 - val_loss: 2.7953\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3538 - val_loss: 2.7896\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3309 - val_loss: 2.8917\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3420 - val_loss: 2.8112\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3596 - val_loss: 2.8712\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3343 - val_loss: 2.8023\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3323 - val_loss: 2.8773\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3366 - val_loss: 2.8165\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3160 - val_loss: 2.9457\n",
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3312 - val_loss: 2.7623\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3333 - val_loss: 2.7632\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3202 - val_loss: 2.8170\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3033 - val_loss: 2.8760\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3104 - val_loss: 2.7828\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3169 - val_loss: 2.9316\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3132 - val_loss: 2.8029\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3027 - val_loss: 2.8240\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3164 - val_loss: 2.7935\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2986 - val_loss: 2.7975\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3160 - val_loss: 2.9011\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3074 - val_loss: 2.7856\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3219 - val_loss: 2.7928\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3046 - val_loss: 2.8934\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 2.2937 - val_loss: 2.8761\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.3114 - val_loss: 2.7928\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2989 - val_loss: 2.8588\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3032 - val_loss: 2.8420\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2953 - val_loss: 2.7403\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2984 - val_loss: 2.8449\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.2896 - val_loss: 2.7734\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2927 - val_loss: 2.8270\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2755 - val_loss: 2.8476\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2903 - val_loss: 2.9267\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2836 - val_loss: 2.7631\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3093 - val_loss: 2.7772\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2807 - val_loss: 2.8007\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2964 - val_loss: 2.8031\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2900 - val_loss: 2.7822\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2933 - val_loss: 2.8760\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3075 - val_loss: 2.8290\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3007 - val_loss: 2.7598\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.2659 - val_loss: 2.7488\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2854 - val_loss: 2.8086\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2898 - val_loss: 2.8247\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2733 - val_loss: 2.8029\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2865 - val_loss: 2.7815\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2609 - val_loss: 2.8519\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3065 - val_loss: 2.8768\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3028 - val_loss: 2.7973\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2531 - val_loss: 2.8024\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2685 - val_loss: 2.7845\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2530 - val_loss: 2.7821\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2508 - val_loss: 2.7763\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2582 - val_loss: 2.8962\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2519 - val_loss: 2.7812\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2483 - val_loss: 2.9235\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3020 - val_loss: 2.7965\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2857 - val_loss: 2.8884\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2727 - val_loss: 2.8716\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2617 - val_loss: 2.7628\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2483 - val_loss: 2.7934\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2407 - val_loss: 2.8544\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2654 - val_loss: 2.8415\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2412 - val_loss: 2.7724\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2398 - val_loss: 2.7520\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2447 - val_loss: 2.8037\n",
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2472 - val_loss: 2.8285\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2548 - val_loss: 2.8496\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2484 - val_loss: 2.7855\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2545 - val_loss: 2.8254\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2362 - val_loss: 2.7952\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2522 - val_loss: 2.7552\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2273 - val_loss: 2.8376\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2381 - val_loss: 2.7860\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2608 - val_loss: 2.7444\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2222 - val_loss: 2.7919\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2332 - val_loss: 2.7719\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2382 - val_loss: 2.7753\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2197 - val_loss: 2.7893\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2335 - val_loss: 2.9164\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2311 - val_loss: 2.8861\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2576 - val_loss: 2.7524\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2146 - val_loss: 2.8262\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2437 - val_loss: 2.8216\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2228 - val_loss: 2.7557\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2363 - val_loss: 2.7776\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2304 - val_loss: 2.7648\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2581 - val_loss: 2.8865\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2083 - val_loss: 2.7843\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2003 - val_loss: 2.7643\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2265 - val_loss: 2.8745\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2276 - val_loss: 2.7555\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2078 - val_loss: 2.7849\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2199 - val_loss: 2.9297\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.2466 - val_loss: 2.8007\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2380 - val_loss: 2.7573\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2419 - val_loss: 2.8886\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.2178 - val_loss: 2.7848\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.2210 - val_loss: 2.7754\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.2077 - val_loss: 2.8317\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2104 - val_loss: 2.7989\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2107 - val_loss: 2.8208\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1909 - val_loss: 2.7586\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1871 - val_loss: 2.8725\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2062 - val_loss: 2.8823\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2161 - val_loss: 2.9661\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2080 - val_loss: 2.8834\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2193 - val_loss: 2.7420\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2057 - val_loss: 2.8746\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1911 - val_loss: 2.8487\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2141 - val_loss: 2.7706\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1914 - val_loss: 2.8355\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1945 - val_loss: 2.7544\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1851 - val_loss: 2.7763\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2004 - val_loss: 2.8698\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1931 - val_loss: 2.7900\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1764 - val_loss: 2.8749\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1868 - val_loss: 2.8429\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1801 - val_loss: 2.7547\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1939 - val_loss: 2.7596\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1734 - val_loss: 2.7710\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2051 - val_loss: 2.7722\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2092 - val_loss: 2.8434\n",
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1673 - val_loss: 3.0845\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2646 - val_loss: 2.9226\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1759 - val_loss: 2.8626\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1957 - val_loss: 2.8883\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1897 - val_loss: 2.7410\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.1751 - val_loss: 2.7764\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.1869 - val_loss: 2.7835\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1762 - val_loss: 2.7621\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1544 - val_loss: 2.7415\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1645 - val_loss: 2.7929\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1946 - val_loss: 2.8000\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1918 - val_loss: 2.7780\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1535 - val_loss: 2.7657\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1518 - val_loss: 2.8609\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1735 - val_loss: 2.8003\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1607 - val_loss: 2.7767\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1648 - val_loss: 2.7997\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1853 - val_loss: 2.7716\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1545 - val_loss: 2.9086\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1827 - val_loss: 2.9541\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1621 - val_loss: 2.7784\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1578 - val_loss: 2.7713\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1369 - val_loss: 2.7973\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1450 - val_loss: 2.7857\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1684 - val_loss: 2.7694\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1621 - val_loss: 2.8543\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1464 - val_loss: 2.8114\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1493 - val_loss: 2.7621\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1622 - val_loss: 2.7835\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1414 - val_loss: 2.8678\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1587 - val_loss: 2.8761\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1536 - val_loss: 2.7893\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1259 - val_loss: 2.7931\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1229 - val_loss: 2.8630\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.1681 - val_loss: 2.8520\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1387 - val_loss: 2.8111\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1190 - val_loss: 2.7844\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1493 - val_loss: 2.7467\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1275 - val_loss: 2.8143\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1251 - val_loss: 2.7694\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1084 - val_loss: 2.7601\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1469 - val_loss: 2.7986\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1484 - val_loss: 2.7984\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 2.1351 - val_loss: 2.7659\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1261 - val_loss: 2.8280\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1509 - val_loss: 2.8055\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1268 - val_loss: 2.8235\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1404 - val_loss: 2.7877\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1301 - val_loss: 2.7764\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1309 - val_loss: 2.7936\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1127 - val_loss: 2.8250\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1531 - val_loss: 2.7648\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1270 - val_loss: 2.8677\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1247 - val_loss: 2.8204\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1289 - val_loss: 2.7630\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1279 - val_loss: 2.7962\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1093 - val_loss: 2.7765\n",
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1280 - val_loss: 2.8395\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1047 - val_loss: 2.8123\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1124 - val_loss: 2.7867\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1046 - val_loss: 2.8170\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1196 - val_loss: 2.8218\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0919 - val_loss: 2.7799\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0947 - val_loss: 2.7521\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0999 - val_loss: 2.7925\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1320 - val_loss: 2.8487\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1040 - val_loss: 2.8958\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1283 - val_loss: 2.8416\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1169 - val_loss: 2.8032\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0939 - val_loss: 2.7805\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0908 - val_loss: 2.7599\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0895 - val_loss: 2.8870\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1146 - val_loss: 2.8040\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1132 - val_loss: 2.7967\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0995 - val_loss: 2.7496\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0794 - val_loss: 2.7764\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1078 - val_loss: 2.7599\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0678 - val_loss: 2.7912\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0728 - val_loss: 2.7906\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0809 - val_loss: 2.7494\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0878 - val_loss: 2.7823\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0919 - val_loss: 2.7917\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0851 - val_loss: 2.7680\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0984 - val_loss: 2.7582\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0739 - val_loss: 2.7920\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0999 - val_loss: 2.8006\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0923 - val_loss: 2.7812\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0651 - val_loss: 2.8035\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0700 - val_loss: 2.8416\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0781 - val_loss: 2.7809\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0576 - val_loss: 2.7906\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0962 - val_loss: 2.7833\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0816 - val_loss: 2.7485\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0791 - val_loss: 2.8962\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1114 - val_loss: 2.7850\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0775 - val_loss: 2.8103\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0696 - val_loss: 2.7388\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0505 - val_loss: 2.8221\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0698 - val_loss: 2.9121\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0735 - val_loss: 2.7721\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0427 - val_loss: 2.8393\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1089 - val_loss: 2.7667\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0566 - val_loss: 2.8384\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0514 - val_loss: 2.7617\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0681 - val_loss: 2.7863\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0410 - val_loss: 2.7627\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0568 - val_loss: 2.7969\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0792 - val_loss: 2.7646\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0416 - val_loss: 2.8313\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0786 - val_loss: 2.7533\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0358 - val_loss: 2.8609\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0683 - val_loss: 2.7928\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0607 - val_loss: 2.7874\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0438 - val_loss: 2.7744\n",
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0906 - val_loss: 2.7944\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0445 - val_loss: 2.7959\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0450 - val_loss: 2.8030\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0585 - val_loss: 2.8046\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0462 - val_loss: 2.7658\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0269 - val_loss: 2.8290\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0396 - val_loss: 2.7733\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0447 - val_loss: 2.9477\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0597 - val_loss: 2.8103\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0257 - val_loss: 2.7773\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0465 - val_loss: 2.7989\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0484 - val_loss: 2.8411\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0255 - val_loss: 2.9494\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0543 - val_loss: 2.7538\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0476 - val_loss: 2.7625\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0198 - val_loss: 2.8449\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0382 - val_loss: 2.7943\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0220 - val_loss: 2.7955\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0427 - val_loss: 2.7790\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0285 - val_loss: 2.7977\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0225 - val_loss: 2.7785\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9988 - val_loss: 2.8553\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0501 - val_loss: 2.8120\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0580 - val_loss: 2.8880\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0594 - val_loss: 2.8073\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0208 - val_loss: 2.7777\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0135 - val_loss: 2.8306\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0330 - val_loss: 2.8038\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0016 - val_loss: 2.8161\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0351 - val_loss: 2.7761\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0383 - val_loss: 2.9296\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0746 - val_loss: 2.7593\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0025 - val_loss: 2.7847\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0329 - val_loss: 2.8373\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0255 - val_loss: 2.7724\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0087 - val_loss: 2.7819\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0142 - val_loss: 2.8132\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0145 - val_loss: 2.7662\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9912 - val_loss: 2.7884\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9725 - val_loss: 2.7677\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0193 - val_loss: 2.7807\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0018 - val_loss: 2.7938\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9823 - val_loss: 2.8737\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0162 - val_loss: 2.7962\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0045 - val_loss: 2.8304\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9898 - val_loss: 2.7844\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9885 - val_loss: 2.8078\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0107 - val_loss: 2.7674\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9914 - val_loss: 2.7960\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0141 - val_loss: 2.8224\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9944 - val_loss: 2.7769\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9823 - val_loss: 2.7865\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0042 - val_loss: 2.8098\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9909 - val_loss: 2.8024\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9843 - val_loss: 2.8613\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9856 - val_loss: 2.8148\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9943 - val_loss: 2.7733\n",
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0001 - val_loss: 2.8552\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0310 - val_loss: 2.8279\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0158 - val_loss: 2.7665\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9733 - val_loss: 2.8052\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9821 - val_loss: 2.7913\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9667 - val_loss: 2.8298\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9827 - val_loss: 2.7989\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9523 - val_loss: 2.8469\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0059 - val_loss: 2.8301\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9898 - val_loss: 2.7723\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9857 - val_loss: 2.8993\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0076 - val_loss: 2.7737\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9641 - val_loss: 2.7899\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9651 - val_loss: 2.8518\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9924 - val_loss: 2.8298\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9727 - val_loss: 2.8390\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9809 - val_loss: 2.9218\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9926 - val_loss: 2.8325\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9790 - val_loss: 2.7723\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9510 - val_loss: 2.8033\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9632 - val_loss: 2.7806\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9639 - val_loss: 2.8346\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0034 - val_loss: 2.8554\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9971 - val_loss: 2.9166\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0235 - val_loss: 2.7859\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9759 - val_loss: 2.8155\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9761 - val_loss: 2.8537\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9652 - val_loss: 2.7909\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9545 - val_loss: 2.8001\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9516 - val_loss: 2.9231\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9976 - val_loss: 2.7831\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9482 - val_loss: 2.8615\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9578 - val_loss: 2.7830\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9509 - val_loss: 2.7970\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9715 - val_loss: 2.8120\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9354 - val_loss: 2.7999\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9558 - val_loss: 2.8376\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9321 - val_loss: 2.7902\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9434 - val_loss: 2.8283\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9263 - val_loss: 2.8296\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9378 - val_loss: 2.7854\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9113 - val_loss: 2.8654\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9870 - val_loss: 2.8826\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9284 - val_loss: 2.7937\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9351 - val_loss: 2.8539\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9670 - val_loss: 2.8325\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9432 - val_loss: 2.9005\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9375 - val_loss: 2.8131\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9141 - val_loss: 2.8010\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9452 - val_loss: 2.7881\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9467 - val_loss: 2.8191\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9639 - val_loss: 2.8337\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9532 - val_loss: 2.8179\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9419 - val_loss: 2.8588\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9507 - val_loss: 2.8174\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9177 - val_loss: 2.8145\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8899 - val_loss: 2.7991\n",
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9273 - val_loss: 2.8259\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9674 - val_loss: 2.8308\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9153 - val_loss: 2.9359\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9656 - val_loss: 2.9112\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9714 - val_loss: 2.8076\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8995 - val_loss: 2.8526\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9133 - val_loss: 2.8579\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9585 - val_loss: 2.8120\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9024 - val_loss: 2.8290\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8852 - val_loss: 2.8194\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9051 - val_loss: 2.8092\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9165 - val_loss: 2.8370\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9015 - val_loss: 2.8706\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8982 - val_loss: 2.8310\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8759 - val_loss: 2.8466\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9100 - val_loss: 2.8758\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9145 - val_loss: 2.9154\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9139 - val_loss: 2.8030\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8631 - val_loss: 2.7900\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8952 - val_loss: 2.7836\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9273 - val_loss: 2.9206\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9423 - val_loss: 2.7819\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8867 - val_loss: 2.7899\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9039 - val_loss: 2.8165\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8997 - val_loss: 2.7990\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8806 - val_loss: 2.8165\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8916 - val_loss: 2.8740\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8996 - val_loss: 2.9343\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8929 - val_loss: 2.8275\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8809 - val_loss: 2.8194\n",
      "Epoch 1/600\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 49.1656 - val_loss: 32.8653\n",
      "Epoch 2/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 23.9258 - val_loss: 16.8788\n",
      "Epoch 3/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 16.0959 - val_loss: 13.1481\n",
      "Epoch 4/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 10.2814 - val_loss: 8.6639\n",
      "Epoch 5/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 8.0832 - val_loss: 8.2088\n",
      "Epoch 6/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 7.5433 - val_loss: 7.5175\n",
      "Epoch 7/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 6.8315 - val_loss: 6.9931\n",
      "Epoch 8/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 6.1131 - val_loss: 5.9388\n",
      "Epoch 9/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 5.3716 - val_loss: 5.3074\n",
      "Epoch 10/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.7820 - val_loss: 4.7720\n",
      "Epoch 11/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.2858 - val_loss: 4.2951\n",
      "Epoch 12/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.9885 - val_loss: 4.0372\n",
      "Epoch 13/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.8321 - val_loss: 3.9078\n",
      "Epoch 14/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.6752 - val_loss: 3.6607\n",
      "Epoch 15/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.5603 - val_loss: 3.5483\n",
      "Epoch 16/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.4668 - val_loss: 3.7176\n",
      "Epoch 17/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 3.4520 - val_loss: 3.3790\n",
      "Epoch 18/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 3.3721 - val_loss: 3.2916\n",
      "Epoch 19/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 3.2691 - val_loss: 3.2273\n",
      "Epoch 20/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.2539 - val_loss: 3.2323\n",
      "Epoch 21/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 3.2289 - val_loss: 3.1156\n",
      "Epoch 22/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 3.1309 - val_loss: 3.0571\n",
      "Epoch 23/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.1040 - val_loss: 3.3122\n",
      "Epoch 24/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.1288 - val_loss: 3.0897\n",
      "Epoch 25/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0797 - val_loss: 3.0663\n",
      "Epoch 26/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0552 - val_loss: 3.0217\n",
      "Epoch 27/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 3.0275 - val_loss: 2.9833\n",
      "Epoch 28/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 3.0609 - val_loss: 3.0286\n",
      "Epoch 29/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 3.0087 - val_loss: 2.9667\n",
      "Epoch 30/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 3.0182 - val_loss: 2.9047\n",
      "Epoch 31/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.9636 - val_loss: 2.9391\n",
      "Epoch 32/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9712 - val_loss: 2.9056\n",
      "Epoch 33/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.9646 - val_loss: 2.9336\n",
      "Epoch 34/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9922 - val_loss: 2.8917\n",
      "Epoch 35/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.9220 - val_loss: 2.9247\n",
      "Epoch 36/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.9171 - val_loss: 2.8603\n",
      "Epoch 37/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.9106 - val_loss: 2.8131\n",
      "Epoch 38/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.8940 - val_loss: 2.8574\n",
      "Epoch 39/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.9250 - val_loss: 2.8109\n",
      "Epoch 40/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.9027 - val_loss: 2.9308\n",
      "Epoch 41/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8884 - val_loss: 2.9004\n",
      "Epoch 42/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9186 - val_loss: 2.8312\n",
      "Epoch 43/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8706 - val_loss: 2.8977\n",
      "Epoch 44/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.9004 - val_loss: 2.8181\n",
      "Epoch 45/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.8519 - val_loss: 2.9383\n",
      "Epoch 46/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8489 - val_loss: 2.7543\n",
      "Epoch 47/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.8733 - val_loss: 2.7951\n",
      "Epoch 48/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8451 - val_loss: 2.7729\n",
      "Epoch 49/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8321 - val_loss: 2.9224\n",
      "Epoch 50/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.8355 - val_loss: 2.7619\n",
      "Epoch 51/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.8283 - val_loss: 2.7901\n",
      "Epoch 52/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7993 - val_loss: 2.7910\n",
      "Epoch 53/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8146 - val_loss: 2.8240\n",
      "Epoch 54/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.8229 - val_loss: 2.6930\n",
      "Epoch 55/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7981 - val_loss: 2.7293\n",
      "Epoch 56/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.8091 - val_loss: 2.7269\n",
      "Epoch 57/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7881 - val_loss: 2.6909\n",
      "Epoch 58/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.7653 - val_loss: 2.7047\n",
      "Epoch 59/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7841 - val_loss: 2.7053\n",
      "Epoch 60/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.8076 - val_loss: 2.6792\n",
      "Epoch 61/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.7526 - val_loss: 2.6891\n",
      "Epoch 62/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.7691 - val_loss: 2.6670\n",
      "Epoch 63/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7595 - val_loss: 2.6801\n",
      "Epoch 64/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7616 - val_loss: 2.7132\n",
      "Epoch 65/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7486 - val_loss: 2.7227\n",
      "Epoch 66/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7848 - val_loss: 2.7352\n",
      "Epoch 67/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7348 - val_loss: 2.6709\n",
      "Epoch 68/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7464 - val_loss: 2.6638\n",
      "Epoch 69/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7516 - val_loss: 2.6985\n",
      "Epoch 70/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7402 - val_loss: 2.6779\n",
      "Epoch 71/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7598 - val_loss: 2.7194\n",
      "Epoch 72/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7440 - val_loss: 2.7062\n",
      "Epoch 73/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7409 - val_loss: 2.6980\n",
      "Epoch 74/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.7301 - val_loss: 2.6623\n",
      "Epoch 75/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7064 - val_loss: 2.6713\n",
      "Epoch 76/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7175 - val_loss: 2.6584\n",
      "Epoch 77/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.7011 - val_loss: 2.6554\n",
      "Epoch 78/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6800 - val_loss: 2.6414\n",
      "Epoch 79/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6949 - val_loss: 2.6660\n",
      "Epoch 80/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6872 - val_loss: 2.7098\n",
      "Epoch 81/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.7201 - val_loss: 2.6421\n",
      "Epoch 82/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6859 - val_loss: 2.6954\n",
      "Epoch 83/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6893 - val_loss: 2.6707\n",
      "Epoch 84/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6726 - val_loss: 2.6555\n",
      "Epoch 85/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6610 - val_loss: 2.6287\n",
      "Epoch 86/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6787 - val_loss: 2.6696\n",
      "Epoch 87/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6676 - val_loss: 2.6534\n",
      "Epoch 88/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6773 - val_loss: 2.6960\n",
      "Epoch 89/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6637 - val_loss: 2.6787\n",
      "Epoch 90/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6843 - val_loss: 2.6769\n",
      "Epoch 91/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6479 - val_loss: 2.7001\n",
      "Epoch 92/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6675 - val_loss: 2.6155\n",
      "Epoch 93/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6674 - val_loss: 2.6667\n",
      "Epoch 94/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6517 - val_loss: 2.7337\n",
      "Epoch 95/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6675 - val_loss: 2.6359\n",
      "Epoch 96/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6375 - val_loss: 2.7258\n",
      "Epoch 97/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6632 - val_loss: 2.6705\n",
      "Epoch 98/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6215 - val_loss: 2.6856\n",
      "Epoch 99/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6525 - val_loss: 2.5959\n",
      "Epoch 100/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.6351 - val_loss: 2.8120\n",
      "Epoch 101/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.6611 - val_loss: 2.6537\n",
      "Epoch 102/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6352 - val_loss: 2.5782\n",
      "Epoch 103/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6365 - val_loss: 2.6099\n",
      "Epoch 104/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6261 - val_loss: 2.6584\n",
      "Epoch 105/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6373 - val_loss: 2.7654\n",
      "Epoch 106/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6413 - val_loss: 2.7163\n",
      "Epoch 107/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.6187 - val_loss: 2.7396\n",
      "Epoch 108/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6518 - val_loss: 2.6529\n",
      "Epoch 109/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6008 - val_loss: 2.7487\n",
      "Epoch 110/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6350 - val_loss: 2.6242\n",
      "Epoch 111/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6288 - val_loss: 2.6596\n",
      "Epoch 112/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.6197 - val_loss: 2.8102\n",
      "Epoch 113/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.6291 - val_loss: 2.6283\n",
      "Epoch 114/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.6360 - val_loss: 2.6473\n",
      "Epoch 115/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6142 - val_loss: 2.5636\n",
      "Epoch 116/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.6107 - val_loss: 2.6149\n",
      "Epoch 117/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5954 - val_loss: 2.6343\n",
      "Epoch 118/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.6228 - val_loss: 2.6534\n",
      "Epoch 119/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5907 - val_loss: 2.7360\n",
      "Epoch 120/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.6311 - val_loss: 2.6115\n",
      "Epoch 121/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5876 - val_loss: 2.5583\n",
      "Epoch 122/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5704 - val_loss: 2.5868\n",
      "Epoch 123/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5621 - val_loss: 2.5760\n",
      "Epoch 124/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5543 - val_loss: 2.6184\n",
      "Epoch 125/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5850 - val_loss: 2.6637\n",
      "Epoch 126/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5934 - val_loss: 2.6143\n",
      "Epoch 127/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5845 - val_loss: 2.5789\n",
      "Epoch 128/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5656 - val_loss: 2.7065\n",
      "Epoch 129/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5634 - val_loss: 2.6042\n",
      "Epoch 130/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5732 - val_loss: 2.5759\n",
      "Epoch 131/600\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 2.5625 - val_loss: 2.5836\n",
      "Epoch 132/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.5596 - val_loss: 2.5650\n",
      "Epoch 133/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5569 - val_loss: 2.6308\n",
      "Epoch 134/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 2.5567 - val_loss: 2.5985\n",
      "Epoch 135/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5765 - val_loss: 2.5630\n",
      "Epoch 136/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5508 - val_loss: 2.6249\n",
      "Epoch 137/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5753 - val_loss: 2.5652\n",
      "Epoch 138/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5612 - val_loss: 2.6389\n",
      "Epoch 139/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5477 - val_loss: 2.6493\n",
      "Epoch 140/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5538 - val_loss: 2.6394\n",
      "Epoch 141/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5344 - val_loss: 2.5833\n",
      "Epoch 142/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5504 - val_loss: 2.7932\n",
      "Epoch 143/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5888 - val_loss: 2.6093\n",
      "Epoch 144/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5350 - val_loss: 2.5775\n",
      "Epoch 145/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.5189 - val_loss: 2.7298\n",
      "Epoch 146/600\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 2.5380 - val_loss: 2.5592\n",
      "Epoch 147/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5211 - val_loss: 2.5875\n",
      "Epoch 148/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5220 - val_loss: 2.7175\n",
      "Epoch 149/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5492 - val_loss: 2.5490\n",
      "Epoch 150/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5021 - val_loss: 2.5590\n",
      "Epoch 151/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5064 - val_loss: 2.5612\n",
      "Epoch 152/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5113 - val_loss: 2.5622\n",
      "Epoch 153/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5373 - val_loss: 2.6067\n",
      "Epoch 154/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5087 - val_loss: 2.5649\n",
      "Epoch 155/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4801 - val_loss: 2.5718\n",
      "Epoch 156/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5332 - val_loss: 2.6000\n",
      "Epoch 157/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5370 - val_loss: 2.5778\n",
      "Epoch 158/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5144 - val_loss: 2.7061\n",
      "Epoch 159/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.5220 - val_loss: 2.6570\n",
      "Epoch 160/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5239 - val_loss: 2.5504\n",
      "Epoch 161/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.5228 - val_loss: 2.5580\n",
      "Epoch 162/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5086 - val_loss: 2.7350\n",
      "Epoch 163/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.5232 - val_loss: 2.5381\n",
      "Epoch 164/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4924 - val_loss: 2.6786\n",
      "Epoch 165/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5089 - val_loss: 2.5852\n",
      "Epoch 166/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4869 - val_loss: 2.6876\n",
      "Epoch 167/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.5009 - val_loss: 2.7012\n",
      "Epoch 168/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5293 - val_loss: 2.5722\n",
      "Epoch 169/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.5047 - val_loss: 2.5390\n",
      "Epoch 170/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4805 - val_loss: 2.6093\n",
      "Epoch 171/600\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 2.4952 - val_loss: 2.6787\n",
      "Epoch 172/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.4878 - val_loss: 2.6238\n",
      "Epoch 173/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.5027 - val_loss: 2.5502\n",
      "Epoch 174/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4717 - val_loss: 2.5713\n",
      "Epoch 175/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.4846 - val_loss: 2.5337\n",
      "Epoch 176/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4604 - val_loss: 2.5157\n",
      "Epoch 177/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4561 - val_loss: 2.6046\n",
      "Epoch 178/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.4697 - val_loss: 2.7057\n",
      "Epoch 179/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4771 - val_loss: 2.5528\n",
      "Epoch 180/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4524 - val_loss: 2.5364\n",
      "Epoch 181/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4834 - val_loss: 2.5796\n",
      "Epoch 182/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4482 - val_loss: 2.5973\n",
      "Epoch 183/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4466 - val_loss: 2.5664\n",
      "Epoch 184/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4768 - val_loss: 2.7537\n",
      "Epoch 185/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.5045 - val_loss: 2.5473\n",
      "Epoch 186/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4532 - val_loss: 2.5270\n",
      "Epoch 187/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.4418 - val_loss: 2.5788\n",
      "Epoch 188/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4426 - val_loss: 2.5222\n",
      "Epoch 189/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4540 - val_loss: 2.5326\n",
      "Epoch 190/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4234 - val_loss: 2.5870\n",
      "Epoch 191/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4641 - val_loss: 2.5885\n",
      "Epoch 192/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4730 - val_loss: 2.6307\n",
      "Epoch 193/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4636 - val_loss: 2.6707\n",
      "Epoch 194/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4232 - val_loss: 2.5520\n",
      "Epoch 195/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4254 - val_loss: 2.5244\n",
      "Epoch 196/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4465 - val_loss: 2.6008\n",
      "Epoch 197/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.4341 - val_loss: 2.6596\n",
      "Epoch 198/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4520 - val_loss: 2.5476\n",
      "Epoch 199/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4119 - val_loss: 2.6817\n",
      "Epoch 200/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4508 - val_loss: 2.5128\n",
      "Epoch 201/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4181 - val_loss: 2.6369\n",
      "Epoch 202/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4345 - val_loss: 2.5255\n",
      "Epoch 203/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3965 - val_loss: 2.5941\n",
      "Epoch 204/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4227 - val_loss: 2.6122\n",
      "Epoch 205/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4648 - val_loss: 2.6772\n",
      "Epoch 206/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.4360 - val_loss: 2.5295\n",
      "Epoch 207/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4378 - val_loss: 2.5744\n",
      "Epoch 208/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3900 - val_loss: 2.6277\n",
      "Epoch 209/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4419 - val_loss: 2.5787\n",
      "Epoch 210/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4085 - val_loss: 2.5692\n",
      "Epoch 211/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4072 - val_loss: 2.5189\n",
      "Epoch 212/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4067 - val_loss: 2.5610\n",
      "Epoch 213/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4117 - val_loss: 2.5942\n",
      "Epoch 214/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3928 - val_loss: 2.5575\n",
      "Epoch 215/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.3845 - val_loss: 2.5302\n",
      "Epoch 216/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3943 - val_loss: 2.5654\n",
      "Epoch 217/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.4014 - val_loss: 2.6166\n",
      "Epoch 218/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4081 - val_loss: 2.5396\n",
      "Epoch 219/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3686 - val_loss: 2.5434\n",
      "Epoch 220/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.4065 - val_loss: 2.5718\n",
      "Epoch 221/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3856 - val_loss: 2.6254\n",
      "Epoch 222/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3966 - val_loss: 2.5382\n",
      "Epoch 223/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.4105 - val_loss: 2.5670\n",
      "Epoch 224/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3598 - val_loss: 2.6375\n",
      "Epoch 225/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.4074 - val_loss: 2.5322\n",
      "Epoch 226/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3726 - val_loss: 2.5236\n",
      "Epoch 227/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3607 - val_loss: 2.5467\n",
      "Epoch 228/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3699 - val_loss: 2.5234\n",
      "Epoch 229/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3693 - val_loss: 2.5317\n",
      "Epoch 230/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3983 - val_loss: 2.5348\n",
      "Epoch 231/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3609 - val_loss: 2.5184\n",
      "Epoch 232/600\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 2.3787 - val_loss: 2.5218\n",
      "Epoch 233/600\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 2.3894 - val_loss: 2.5774\n",
      "Epoch 234/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3827 - val_loss: 2.5251\n",
      "Epoch 235/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3678 - val_loss: 2.6997\n",
      "Epoch 236/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3909 - val_loss: 2.6415\n",
      "Epoch 237/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3800 - val_loss: 2.5583\n",
      "Epoch 238/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3595 - val_loss: 2.6457\n",
      "Epoch 239/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3800 - val_loss: 2.5489\n",
      "Epoch 240/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.3646 - val_loss: 2.5318\n",
      "Epoch 241/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3730 - val_loss: 2.5615\n",
      "Epoch 242/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3863 - val_loss: 2.5914\n",
      "Epoch 243/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3586 - val_loss: 2.5194\n",
      "Epoch 244/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3414 - val_loss: 2.5454\n",
      "Epoch 245/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3338 - val_loss: 2.5457\n",
      "Epoch 246/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3373 - val_loss: 2.5549\n",
      "Epoch 247/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3854 - val_loss: 2.5881\n",
      "Epoch 248/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3771 - val_loss: 2.5536\n",
      "Epoch 249/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3588 - val_loss: 2.5694\n",
      "Epoch 250/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3400 - val_loss: 2.5801\n",
      "Epoch 251/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3401 - val_loss: 2.5471\n",
      "Epoch 252/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3455 - val_loss: 2.5224\n",
      "Epoch 253/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3758 - val_loss: 2.5796\n",
      "Epoch 254/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3498 - val_loss: 2.5276\n",
      "Epoch 255/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3448 - val_loss: 2.5392\n",
      "Epoch 256/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3244 - val_loss: 2.5762\n",
      "Epoch 257/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3255 - val_loss: 2.5506\n",
      "Epoch 258/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3443 - val_loss: 2.5278\n",
      "Epoch 259/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3205 - val_loss: 2.5525\n",
      "Epoch 260/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3173 - val_loss: 2.5785\n",
      "Epoch 261/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3277 - val_loss: 2.6005\n",
      "Epoch 262/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3134 - val_loss: 2.5467\n",
      "Epoch 263/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3111 - val_loss: 2.5166\n",
      "Epoch 264/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3144 - val_loss: 2.5495\n",
      "Epoch 265/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3106 - val_loss: 2.5277\n",
      "Epoch 266/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3032 - val_loss: 2.5373\n",
      "Epoch 267/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2968 - val_loss: 2.6534\n",
      "Epoch 268/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3186 - val_loss: 2.5621\n",
      "Epoch 269/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.3464 - val_loss: 2.5575\n",
      "Epoch 270/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3128 - val_loss: 2.5460\n",
      "Epoch 271/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.3053 - val_loss: 2.5611\n",
      "Epoch 272/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.2826 - val_loss: 2.5253\n",
      "Epoch 273/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.3118 - val_loss: 2.5722\n",
      "Epoch 274/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3001 - val_loss: 2.5396\n",
      "Epoch 275/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3289 - val_loss: 2.5327\n",
      "Epoch 276/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.3302 - val_loss: 2.5083\n",
      "Epoch 277/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3038 - val_loss: 2.5420\n",
      "Epoch 278/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2859 - val_loss: 2.5427\n",
      "Epoch 279/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2915 - val_loss: 2.5475\n",
      "Epoch 280/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2837 - val_loss: 2.5414\n",
      "Epoch 281/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2984 - val_loss: 2.5626\n",
      "Epoch 282/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2953 - val_loss: 2.5046\n",
      "Epoch 283/600\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 2.2809 - val_loss: 2.5382\n",
      "Epoch 284/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3100 - val_loss: 2.5027\n",
      "Epoch 285/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2942 - val_loss: 2.5841\n",
      "Epoch 286/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2948 - val_loss: 2.5933\n",
      "Epoch 287/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2809 - val_loss: 2.5449\n",
      "Epoch 288/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3040 - val_loss: 2.5374\n",
      "Epoch 289/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2837 - val_loss: 2.6294\n",
      "Epoch 290/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3006 - val_loss: 2.6189\n",
      "Epoch 291/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.3114 - val_loss: 2.7115\n",
      "Epoch 292/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2956 - val_loss: 2.5924\n",
      "Epoch 293/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2950 - val_loss: 2.5148\n",
      "Epoch 294/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2683 - val_loss: 2.6788\n",
      "Epoch 295/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2917 - val_loss: 2.5008\n",
      "Epoch 296/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2686 - val_loss: 2.5668\n",
      "Epoch 297/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.3018 - val_loss: 2.5828\n",
      "Epoch 298/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2911 - val_loss: 2.5135\n",
      "Epoch 299/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2811 - val_loss: 2.6136\n",
      "Epoch 300/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.3063 - val_loss: 2.5162\n",
      "Epoch 301/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2617 - val_loss: 2.5329\n",
      "Epoch 302/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2604 - val_loss: 2.5217\n",
      "Epoch 303/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2530 - val_loss: 2.5402\n",
      "Epoch 304/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2862 - val_loss: 2.5200\n",
      "Epoch 305/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2497 - val_loss: 2.5115\n",
      "Epoch 306/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2449 - val_loss: 2.6032\n",
      "Epoch 307/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2572 - val_loss: 2.5502\n",
      "Epoch 308/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2612 - val_loss: 2.5888\n",
      "Epoch 309/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2757 - val_loss: 2.5438\n",
      "Epoch 310/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2600 - val_loss: 2.6054\n",
      "Epoch 311/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.2674 - val_loss: 2.7273\n",
      "Epoch 312/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2770 - val_loss: 2.5205\n",
      "Epoch 313/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2288 - val_loss: 2.5312\n",
      "Epoch 314/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2680 - val_loss: 2.5760\n",
      "Epoch 315/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2543 - val_loss: 2.5642\n",
      "Epoch 316/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2355 - val_loss: 2.5578\n",
      "Epoch 317/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2402 - val_loss: 2.5524\n",
      "Epoch 318/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2490 - val_loss: 2.5315\n",
      "Epoch 319/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2329 - val_loss: 2.5333\n",
      "Epoch 320/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2494 - val_loss: 2.6611\n",
      "Epoch 321/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2455 - val_loss: 2.5545\n",
      "Epoch 322/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2376 - val_loss: 2.5700\n",
      "Epoch 323/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2479 - val_loss: 2.5539\n",
      "Epoch 324/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2337 - val_loss: 2.5444\n",
      "Epoch 325/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2102 - val_loss: 2.5446\n",
      "Epoch 326/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2127 - val_loss: 2.5034\n",
      "Epoch 327/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2264 - val_loss: 2.5282\n",
      "Epoch 328/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2493 - val_loss: 2.5722\n",
      "Epoch 329/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2283 - val_loss: 2.5422\n",
      "Epoch 330/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2320 - val_loss: 2.5084\n",
      "Epoch 331/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2234 - val_loss: 2.5064\n",
      "Epoch 332/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2354 - val_loss: 2.5170\n",
      "Epoch 333/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2149 - val_loss: 2.5930\n",
      "Epoch 334/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2416 - val_loss: 2.5425\n",
      "Epoch 335/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2167 - val_loss: 2.5109\n",
      "Epoch 336/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.2094 - val_loss: 2.5448\n",
      "Epoch 337/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2080 - val_loss: 2.5014\n",
      "Epoch 338/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1942 - val_loss: 2.5652\n",
      "Epoch 339/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.2460 - val_loss: 2.5420\n",
      "Epoch 340/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2297 - val_loss: 2.5323\n",
      "Epoch 341/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.2129 - val_loss: 2.4972\n",
      "Epoch 342/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1938 - val_loss: 2.5600\n",
      "Epoch 343/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2211 - val_loss: 2.5595\n",
      "Epoch 344/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2055 - val_loss: 2.5059\n",
      "Epoch 345/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 2.2075 - val_loss: 2.5108\n",
      "Epoch 346/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1973 - val_loss: 2.5496\n",
      "Epoch 347/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.2319 - val_loss: 2.5436\n",
      "Epoch 348/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2233 - val_loss: 2.6116\n",
      "Epoch 349/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2377 - val_loss: 2.5129\n",
      "Epoch 350/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1832 - val_loss: 2.5387\n",
      "Epoch 351/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2035 - val_loss: 2.5183\n",
      "Epoch 352/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1847 - val_loss: 2.5356\n",
      "Epoch 353/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1858 - val_loss: 2.5087\n",
      "Epoch 354/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1967 - val_loss: 2.6126\n",
      "Epoch 355/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1883 - val_loss: 2.5521\n",
      "Epoch 356/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1717 - val_loss: 2.5205\n",
      "Epoch 357/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1851 - val_loss: 2.6317\n",
      "Epoch 358/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.2015 - val_loss: 2.5008\n",
      "Epoch 359/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.2087 - val_loss: 2.5341\n",
      "Epoch 360/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.1832 - val_loss: 2.5373\n",
      "Epoch 361/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1743 - val_loss: 2.5988\n",
      "Epoch 362/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1895 - val_loss: 2.5793\n",
      "Epoch 363/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1899 - val_loss: 2.5348\n",
      "Epoch 364/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1975 - val_loss: 2.5210\n",
      "Epoch 365/600\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 2.1768 - val_loss: 2.5387\n",
      "Epoch 366/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1556 - val_loss: 2.5528\n",
      "Epoch 367/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1733 - val_loss: 2.5121\n",
      "Epoch 368/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1845 - val_loss: 2.5419\n",
      "Epoch 369/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1687 - val_loss: 2.5885\n",
      "Epoch 370/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.2001 - val_loss: 2.6037\n",
      "Epoch 371/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1621 - val_loss: 2.5099\n",
      "Epoch 372/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1642 - val_loss: 2.5002\n",
      "Epoch 373/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1521 - val_loss: 2.6024\n",
      "Epoch 374/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1793 - val_loss: 2.5539\n",
      "Epoch 375/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1609 - val_loss: 2.5390\n",
      "Epoch 376/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1742 - val_loss: 2.5540\n",
      "Epoch 377/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1677 - val_loss: 2.6447\n",
      "Epoch 378/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1931 - val_loss: 2.5078\n",
      "Epoch 379/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1631 - val_loss: 2.5678\n",
      "Epoch 380/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1392 - val_loss: 2.5263\n",
      "Epoch 381/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1574 - val_loss: 2.5518\n",
      "Epoch 382/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1421 - val_loss: 2.5162\n",
      "Epoch 383/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1610 - val_loss: 2.5222\n",
      "Epoch 384/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1307 - val_loss: 2.5208\n",
      "Epoch 385/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1572 - val_loss: 2.5151\n",
      "Epoch 386/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1581 - val_loss: 2.6375\n",
      "Epoch 387/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1645 - val_loss: 2.5746\n",
      "Epoch 388/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1682 - val_loss: 2.5292\n",
      "Epoch 389/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1477 - val_loss: 2.5113\n",
      "Epoch 390/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1244 - val_loss: 2.5534\n",
      "Epoch 391/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1414 - val_loss: 2.5400\n",
      "Epoch 392/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1444 - val_loss: 2.6055\n",
      "Epoch 393/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1640 - val_loss: 2.5473\n",
      "Epoch 394/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1357 - val_loss: 2.5367\n",
      "Epoch 395/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.1349 - val_loss: 2.5383\n",
      "Epoch 396/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1376 - val_loss: 2.5373\n",
      "Epoch 397/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1202 - val_loss: 2.5412\n",
      "Epoch 398/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1492 - val_loss: 2.5476\n",
      "Epoch 399/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1138 - val_loss: 2.5406\n",
      "Epoch 400/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1284 - val_loss: 2.5768\n",
      "Epoch 401/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1257 - val_loss: 2.5344\n",
      "Epoch 402/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1187 - val_loss: 2.4901\n",
      "Epoch 403/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1189 - val_loss: 2.5448\n",
      "Epoch 404/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1164 - val_loss: 2.6199\n",
      "Epoch 405/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1364 - val_loss: 2.5346\n",
      "Epoch 406/600\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.1253 - val_loss: 2.5256\n",
      "Epoch 407/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1425 - val_loss: 2.6134\n",
      "Epoch 408/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1591 - val_loss: 2.5124\n",
      "Epoch 409/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1133 - val_loss: 2.5728\n",
      "Epoch 410/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0961 - val_loss: 2.6017\n",
      "Epoch 411/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.1828 - val_loss: 2.5792\n",
      "Epoch 412/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1317 - val_loss: 2.5583\n",
      "Epoch 413/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1422 - val_loss: 2.5315\n",
      "Epoch 414/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.1063 - val_loss: 2.5089\n",
      "Epoch 415/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0817 - val_loss: 2.5194\n",
      "Epoch 416/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1022 - val_loss: 2.5393\n",
      "Epoch 417/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1012 - val_loss: 2.5424\n",
      "Epoch 418/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1025 - val_loss: 2.5293\n",
      "Epoch 419/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1123 - val_loss: 2.5325\n",
      "Epoch 420/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0992 - val_loss: 2.5072\n",
      "Epoch 421/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0933 - val_loss: 2.5308\n",
      "Epoch 422/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1176 - val_loss: 2.5595\n",
      "Epoch 423/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1012 - val_loss: 2.5593\n",
      "Epoch 424/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0944 - val_loss: 2.5101\n",
      "Epoch 425/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1151 - val_loss: 2.5641\n",
      "Epoch 426/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.1020 - val_loss: 2.6242\n",
      "Epoch 427/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.1152 - val_loss: 2.5352\n",
      "Epoch 428/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0787 - val_loss: 2.5923\n",
      "Epoch 429/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1014 - val_loss: 2.5566\n",
      "Epoch 430/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0909 - val_loss: 2.5608\n",
      "Epoch 431/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0843 - val_loss: 2.5098\n",
      "Epoch 432/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0765 - val_loss: 2.5930\n",
      "Epoch 433/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.1007 - val_loss: 2.5672\n",
      "Epoch 434/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1134 - val_loss: 2.6246\n",
      "Epoch 435/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0837 - val_loss: 2.5394\n",
      "Epoch 436/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0873 - val_loss: 2.5165\n",
      "Epoch 437/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0909 - val_loss: 2.5542\n",
      "Epoch 438/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0833 - val_loss: 2.5989\n",
      "Epoch 439/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0896 - val_loss: 2.5782\n",
      "Epoch 440/600\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 2.0706 - val_loss: 2.5799\n",
      "Epoch 441/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0651 - val_loss: 2.5324\n",
      "Epoch 442/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0701 - val_loss: 2.5144\n",
      "Epoch 443/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0848 - val_loss: 2.5458\n",
      "Epoch 444/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0627 - val_loss: 2.5282\n",
      "Epoch 445/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0818 - val_loss: 2.5440\n",
      "Epoch 446/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0489 - val_loss: 2.5710\n",
      "Epoch 447/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0913 - val_loss: 2.6559\n",
      "Epoch 448/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.1055 - val_loss: 2.6165\n",
      "Epoch 449/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0735 - val_loss: 2.5460\n",
      "Epoch 450/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0554 - val_loss: 2.5142\n",
      "Epoch 451/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0665 - val_loss: 2.5846\n",
      "Epoch 452/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0708 - val_loss: 2.5921\n",
      "Epoch 453/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0806 - val_loss: 2.6434\n",
      "Epoch 454/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0979 - val_loss: 2.5669\n",
      "Epoch 455/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0655 - val_loss: 2.6093\n",
      "Epoch 456/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0616 - val_loss: 2.5640\n",
      "Epoch 457/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0892 - val_loss: 2.5710\n",
      "Epoch 458/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0421 - val_loss: 2.5207\n",
      "Epoch 459/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.0497 - val_loss: 2.5824\n",
      "Epoch 460/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0572 - val_loss: 2.6501\n",
      "Epoch 461/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0654 - val_loss: 2.6641\n",
      "Epoch 462/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0636 - val_loss: 2.5821\n",
      "Epoch 463/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0466 - val_loss: 2.5618\n",
      "Epoch 464/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0986 - val_loss: 2.5582\n",
      "Epoch 465/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0285 - val_loss: 2.5531\n",
      "Epoch 466/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0339 - val_loss: 2.5239\n",
      "Epoch 467/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0417 - val_loss: 2.5800\n",
      "Epoch 468/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0582 - val_loss: 2.6187\n",
      "Epoch 469/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0777 - val_loss: 2.5323\n",
      "Epoch 470/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0236 - val_loss: 2.5671\n",
      "Epoch 471/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0636 - val_loss: 2.5406\n",
      "Epoch 472/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0209 - val_loss: 2.5820\n",
      "Epoch 473/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0378 - val_loss: 2.5422\n",
      "Epoch 474/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0279 - val_loss: 2.5933\n",
      "Epoch 475/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0320 - val_loss: 2.5651\n",
      "Epoch 476/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0454 - val_loss: 2.5564\n",
      "Epoch 477/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0444 - val_loss: 2.5238\n",
      "Epoch 478/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0161 - val_loss: 2.5343\n",
      "Epoch 479/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0394 - val_loss: 2.5482\n",
      "Epoch 480/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0251 - val_loss: 2.5709\n",
      "Epoch 481/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0279 - val_loss: 2.5483\n",
      "Epoch 482/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0203 - val_loss: 2.5691\n",
      "Epoch 483/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0463 - val_loss: 2.5476\n",
      "Epoch 484/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0252 - val_loss: 2.5449\n",
      "Epoch 485/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0297 - val_loss: 2.5298\n",
      "Epoch 486/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0065 - val_loss: 2.5389\n",
      "Epoch 487/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0182 - val_loss: 2.6183\n",
      "Epoch 488/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0261 - val_loss: 2.5567\n",
      "Epoch 489/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0284 - val_loss: 2.5880\n",
      "Epoch 490/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0240 - val_loss: 2.6423\n",
      "Epoch 491/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0463 - val_loss: 2.6463\n",
      "Epoch 492/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0492 - val_loss: 2.5998\n",
      "Epoch 493/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 2.0193 - val_loss: 2.5723\n",
      "Epoch 494/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0085 - val_loss: 2.5649\n",
      "Epoch 495/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0292 - val_loss: 2.5289\n",
      "Epoch 496/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0044 - val_loss: 2.5680\n",
      "Epoch 497/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9950 - val_loss: 2.5803\n",
      "Epoch 498/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0415 - val_loss: 2.5367\n",
      "Epoch 499/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9990 - val_loss: 2.5526\n",
      "Epoch 500/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9891 - val_loss: 2.5570\n",
      "Epoch 501/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 2.0108 - val_loss: 2.5474\n",
      "Epoch 502/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0060 - val_loss: 2.5430\n",
      "Epoch 503/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9851 - val_loss: 2.5580\n",
      "Epoch 504/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9897 - val_loss: 2.5180\n",
      "Epoch 505/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 2.0158 - val_loss: 2.5973\n",
      "Epoch 506/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9942 - val_loss: 2.5479\n",
      "Epoch 507/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9787 - val_loss: 2.5457\n",
      "Epoch 508/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9792 - val_loss: 2.5630\n",
      "Epoch 509/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0017 - val_loss: 2.5647\n",
      "Epoch 510/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0002 - val_loss: 2.5783\n",
      "Epoch 511/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 2.0171 - val_loss: 2.5465\n",
      "Epoch 512/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9774 - val_loss: 2.7036\n",
      "Epoch 513/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 2.0166 - val_loss: 2.5662\n",
      "Epoch 514/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9788 - val_loss: 2.5706\n",
      "Epoch 515/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9921 - val_loss: 2.6471\n",
      "Epoch 516/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 2.0180 - val_loss: 2.5933\n",
      "Epoch 517/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9843 - val_loss: 2.5660\n",
      "Epoch 518/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9743 - val_loss: 2.5758\n",
      "Epoch 519/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9865 - val_loss: 2.5950\n",
      "Epoch 520/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9906 - val_loss: 2.5964\n",
      "Epoch 521/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 2.0138 - val_loss: 2.5963\n",
      "Epoch 522/600\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 1.9512 - val_loss: 2.5520\n",
      "Epoch 523/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9831 - val_loss: 2.5695\n",
      "Epoch 524/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9772 - val_loss: 2.5951\n",
      "Epoch 525/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9784 - val_loss: 2.5635\n",
      "Epoch 526/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9861 - val_loss: 2.6017\n",
      "Epoch 527/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9721 - val_loss: 2.6168\n",
      "Epoch 528/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9795 - val_loss: 2.5765\n",
      "Epoch 529/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9697 - val_loss: 2.5570\n",
      "Epoch 530/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9547 - val_loss: 2.5805\n",
      "Epoch 531/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9458 - val_loss: 2.5593\n",
      "Epoch 532/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9486 - val_loss: 2.6507\n",
      "Epoch 533/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9746 - val_loss: 2.6837\n",
      "Epoch 534/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9523 - val_loss: 2.6155\n",
      "Epoch 535/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9906 - val_loss: 2.5674\n",
      "Epoch 536/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9923 - val_loss: 2.5538\n",
      "Epoch 537/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9691 - val_loss: 2.5585\n",
      "Epoch 538/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9562 - val_loss: 2.5818\n",
      "Epoch 539/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9572 - val_loss: 2.6196\n",
      "Epoch 540/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9705 - val_loss: 2.6771\n",
      "Epoch 541/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9871 - val_loss: 2.5951\n",
      "Epoch 542/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9274 - val_loss: 2.5583\n",
      "Epoch 543/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9380 - val_loss: 2.5812\n",
      "Epoch 544/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9526 - val_loss: 2.5865\n",
      "Epoch 545/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9527 - val_loss: 2.6174\n",
      "Epoch 546/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9460 - val_loss: 2.5974\n",
      "Epoch 547/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9600 - val_loss: 2.5760\n",
      "Epoch 548/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9246 - val_loss: 2.5847\n",
      "Epoch 549/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9508 - val_loss: 2.5716\n",
      "Epoch 550/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9749 - val_loss: 2.6012\n",
      "Epoch 551/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9456 - val_loss: 2.5902\n",
      "Epoch 552/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9758 - val_loss: 2.6108\n",
      "Epoch 553/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9402 - val_loss: 2.5998\n",
      "Epoch 554/600\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 1.9343 - val_loss: 2.5822\n",
      "Epoch 555/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9213 - val_loss: 2.6295\n",
      "Epoch 556/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9301 - val_loss: 2.5835\n",
      "Epoch 557/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9202 - val_loss: 2.5797\n",
      "Epoch 558/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9660 - val_loss: 2.5729\n",
      "Epoch 559/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9224 - val_loss: 2.6283\n",
      "Epoch 560/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9222 - val_loss: 2.5705\n",
      "Epoch 561/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9298 - val_loss: 2.6096\n",
      "Epoch 562/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9261 - val_loss: 2.5985\n",
      "Epoch 563/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9401 - val_loss: 2.5668\n",
      "Epoch 564/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9126 - val_loss: 2.6013\n",
      "Epoch 565/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9216 - val_loss: 2.6175\n",
      "Epoch 566/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9458 - val_loss: 2.6335\n",
      "Epoch 567/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9504 - val_loss: 2.5839\n",
      "Epoch 568/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9136 - val_loss: 2.5977\n",
      "Epoch 569/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9077 - val_loss: 2.7878\n",
      "Epoch 570/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9639 - val_loss: 2.5843\n",
      "Epoch 571/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9266 - val_loss: 2.6674\n",
      "Epoch 572/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.9442 - val_loss: 2.5689\n",
      "Epoch 573/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9028 - val_loss: 2.6113\n",
      "Epoch 574/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9411 - val_loss: 2.6813\n",
      "Epoch 575/600\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 1.9310 - val_loss: 2.5811\n",
      "Epoch 576/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9164 - val_loss: 2.7470\n",
      "Epoch 577/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9549 - val_loss: 2.5883\n",
      "Epoch 578/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9242 - val_loss: 2.6212\n",
      "Epoch 579/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9156 - val_loss: 2.5967\n",
      "Epoch 580/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9646 - val_loss: 2.6177\n",
      "Epoch 581/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8857 - val_loss: 2.5791\n",
      "Epoch 582/600\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.8962 - val_loss: 2.6436\n",
      "Epoch 583/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.8945 - val_loss: 2.7229\n",
      "Epoch 584/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9391 - val_loss: 2.5604\n",
      "Epoch 585/600\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.9116 - val_loss: 2.6312\n",
      "Epoch 586/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9085 - val_loss: 2.5882\n",
      "Epoch 587/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9037 - val_loss: 2.6183\n",
      "Epoch 588/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9363 - val_loss: 2.7067\n",
      "Epoch 589/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.9620 - val_loss: 2.6145\n",
      "Epoch 590/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.9311 - val_loss: 2.6229\n",
      "Epoch 591/600\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 1.9256 - val_loss: 2.6322\n",
      "Epoch 592/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9291 - val_loss: 2.6576\n",
      "Epoch 593/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9064 - val_loss: 2.7100\n",
      "Epoch 594/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.9397 - val_loss: 2.6108\n",
      "Epoch 595/600\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 1.8971 - val_loss: 2.6059\n",
      "Epoch 596/600\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 1.8936 - val_loss: 2.6101\n",
      "Epoch 597/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8889 - val_loss: 2.6079\n",
      "Epoch 598/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.8650 - val_loss: 2.5891\n",
      "Epoch 599/600\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 1.8840 - val_loss: 2.6079\n",
      "Epoch 600/600\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 1.9100 - val_loss: 2.5899\n",
      "Epoch 1/600\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 46.7173\n",
      "Epoch 2/600\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 20.1523\n",
      "Epoch 3/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 11.9575\n",
      "Epoch 4/600\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8.4281\n",
      "Epoch 5/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.9295\n",
      "Epoch 6/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3405\n",
      "Epoch 7/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6625\n",
      "Epoch 8/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.9312\n",
      "Epoch 9/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.1827\n",
      "Epoch 10/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.5927\n",
      "Epoch 11/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1710\n",
      "Epoch 12/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.9074\n",
      "Epoch 13/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.7248\n",
      "Epoch 14/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.5682\n",
      "Epoch 15/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.4109\n",
      "Epoch 16/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.2862\n",
      "Epoch 17/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.2441\n",
      "Epoch 18/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1881\n",
      "Epoch 19/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1354\n",
      "Epoch 20/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.0727\n",
      "Epoch 21/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.0818\n",
      "Epoch 22/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.0227\n",
      "Epoch 23/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.9676\n",
      "Epoch 24/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.9779\n",
      "Epoch 25/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.9357\n",
      "Epoch 26/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.9272\n",
      "Epoch 27/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.9250\n",
      "Epoch 28/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.9200\n",
      "Epoch 29/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8587\n",
      "Epoch 30/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8756\n",
      "Epoch 31/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8550\n",
      "Epoch 32/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8530\n",
      "Epoch 33/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8428\n",
      "Epoch 34/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8153\n",
      "Epoch 35/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8054\n",
      "Epoch 36/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8224\n",
      "Epoch 37/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.8032\n",
      "Epoch 38/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7948\n",
      "Epoch 39/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7701\n",
      "Epoch 40/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7735\n",
      "Epoch 41/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7733\n",
      "Epoch 42/600\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7789\n",
      "Epoch 43/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7445\n",
      "Epoch 44/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7551\n",
      "Epoch 45/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7542\n",
      "Epoch 46/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7317\n",
      "Epoch 47/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7284\n",
      "Epoch 48/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7297\n",
      "Epoch 49/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7329\n",
      "Epoch 50/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.7260\n",
      "Epoch 51/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7124\n",
      "Epoch 52/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7211\n",
      "Epoch 53/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7037\n",
      "Epoch 54/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6999\n",
      "Epoch 55/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.7344\n",
      "Epoch 56/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6899\n",
      "Epoch 57/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7039\n",
      "Epoch 58/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.7041\n",
      "Epoch 59/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6739\n",
      "Epoch 60/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6978\n",
      "Epoch 61/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6750\n",
      "Epoch 62/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6653\n",
      "Epoch 63/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6642\n",
      "Epoch 64/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6395\n",
      "Epoch 65/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6810\n",
      "Epoch 66/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6909\n",
      "Epoch 67/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6445\n",
      "Epoch 68/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6431\n",
      "Epoch 69/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6410\n",
      "Epoch 70/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6410\n",
      "Epoch 71/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6548\n",
      "Epoch 72/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6787\n",
      "Epoch 73/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6603\n",
      "Epoch 74/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6486\n",
      "Epoch 75/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6424\n",
      "Epoch 76/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6542\n",
      "Epoch 77/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6542\n",
      "Epoch 78/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6662\n",
      "Epoch 79/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6303\n",
      "Epoch 80/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6146\n",
      "Epoch 81/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6315\n",
      "Epoch 82/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6208\n",
      "Epoch 83/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6195\n",
      "Epoch 84/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6161\n",
      "Epoch 85/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6192\n",
      "Epoch 86/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6214\n",
      "Epoch 87/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5881\n",
      "Epoch 88/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5976\n",
      "Epoch 89/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5781\n",
      "Epoch 90/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6115\n",
      "Epoch 91/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6040\n",
      "Epoch 92/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5756\n",
      "Epoch 93/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5742\n",
      "Epoch 94/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5921\n",
      "Epoch 95/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6153\n",
      "Epoch 96/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5788\n",
      "Epoch 97/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5566\n",
      "Epoch 98/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6071\n",
      "Epoch 99/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5659\n",
      "Epoch 100/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5841\n",
      "Epoch 101/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5310\n",
      "Epoch 102/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5652\n",
      "Epoch 103/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5593\n",
      "Epoch 104/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5411\n",
      "Epoch 105/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5675\n",
      "Epoch 106/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5421\n",
      "Epoch 107/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5719\n",
      "Epoch 108/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5258\n",
      "Epoch 109/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5756\n",
      "Epoch 110/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5261\n",
      "Epoch 111/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5495\n",
      "Epoch 112/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5337\n",
      "Epoch 113/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5313\n",
      "Epoch 114/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5338\n",
      "Epoch 115/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5389\n",
      "Epoch 116/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5292\n",
      "Epoch 117/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5217\n",
      "Epoch 118/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5110\n",
      "Epoch 119/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5028\n",
      "Epoch 120/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5004\n",
      "Epoch 121/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5205\n",
      "Epoch 122/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5137\n",
      "Epoch 123/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5096\n",
      "Epoch 124/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5084\n",
      "Epoch 125/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5109\n",
      "Epoch 126/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5183\n",
      "Epoch 127/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5030\n",
      "Epoch 128/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4800\n",
      "Epoch 129/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4988\n",
      "Epoch 130/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4952\n",
      "Epoch 131/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.5002\n",
      "Epoch 132/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4886\n",
      "Epoch 133/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4806\n",
      "Epoch 134/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4850\n",
      "Epoch 135/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4969\n",
      "Epoch 136/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4930\n",
      "Epoch 137/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4766\n",
      "Epoch 138/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4834\n",
      "Epoch 139/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4835\n",
      "Epoch 140/600\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4925\n",
      "Epoch 141/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4711\n",
      "Epoch 142/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4796\n",
      "Epoch 143/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4760\n",
      "Epoch 144/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4612\n",
      "Epoch 145/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4583\n",
      "Epoch 146/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4634\n",
      "Epoch 147/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4819\n",
      "Epoch 148/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4546\n",
      "Epoch 149/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4611\n",
      "Epoch 150/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4518\n",
      "Epoch 151/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4418\n",
      "Epoch 152/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4578\n",
      "Epoch 153/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4447\n",
      "Epoch 154/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4589\n",
      "Epoch 155/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4469\n",
      "Epoch 156/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4415\n",
      "Epoch 157/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4612\n",
      "Epoch 158/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4361\n",
      "Epoch 159/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4390\n",
      "Epoch 160/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4488\n",
      "Epoch 161/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4508\n",
      "Epoch 162/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4242\n",
      "Epoch 163/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4571\n",
      "Epoch 164/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4491\n",
      "Epoch 165/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4161\n",
      "Epoch 166/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4213\n",
      "Epoch 167/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4429\n",
      "Epoch 168/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4469\n",
      "Epoch 169/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4233\n",
      "Epoch 170/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4189\n",
      "Epoch 171/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4408\n",
      "Epoch 172/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4087\n",
      "Epoch 173/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4036\n",
      "Epoch 174/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4126\n",
      "Epoch 175/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4024\n",
      "Epoch 176/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4083\n",
      "Epoch 177/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4010\n",
      "Epoch 178/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3897\n",
      "Epoch 179/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.4170\n",
      "Epoch 180/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3994\n",
      "Epoch 181/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3883\n",
      "Epoch 182/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4008\n",
      "Epoch 183/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4148\n",
      "Epoch 184/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3907\n",
      "Epoch 185/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3943\n",
      "Epoch 186/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3833\n",
      "Epoch 187/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3780\n",
      "Epoch 188/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3980\n",
      "Epoch 189/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3679\n",
      "Epoch 190/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4000\n",
      "Epoch 191/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3756\n",
      "Epoch 192/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4066\n",
      "Epoch 193/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3766\n",
      "Epoch 194/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3727\n",
      "Epoch 195/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3730\n",
      "Epoch 196/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3731\n",
      "Epoch 197/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3798\n",
      "Epoch 198/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3757\n",
      "Epoch 199/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3698\n",
      "Epoch 200/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3823\n",
      "Epoch 201/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3715\n",
      "Epoch 202/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3699\n",
      "Epoch 203/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3778\n",
      "Epoch 204/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3585\n",
      "Epoch 205/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3494\n",
      "Epoch 206/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3535\n",
      "Epoch 207/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3494\n",
      "Epoch 208/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3505\n",
      "Epoch 209/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3454\n",
      "Epoch 210/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3493\n",
      "Epoch 211/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3928\n",
      "Epoch 212/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3410\n",
      "Epoch 213/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3532\n",
      "Epoch 214/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3302\n",
      "Epoch 215/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3455\n",
      "Epoch 216/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3240\n",
      "Epoch 217/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3726\n",
      "Epoch 218/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3474\n",
      "Epoch 219/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3453\n",
      "Epoch 220/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3273\n",
      "Epoch 221/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3335\n",
      "Epoch 222/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3844\n",
      "Epoch 223/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3155\n",
      "Epoch 224/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3343\n",
      "Epoch 225/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3233\n",
      "Epoch 226/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3212\n",
      "Epoch 227/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3485\n",
      "Epoch 228/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3200\n",
      "Epoch 229/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3185\n",
      "Epoch 230/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3059\n",
      "Epoch 231/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3152\n",
      "Epoch 232/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3201\n",
      "Epoch 233/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3173\n",
      "Epoch 234/600\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3182\n",
      "Epoch 235/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3134\n",
      "Epoch 236/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3256\n",
      "Epoch 237/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3076\n",
      "Epoch 238/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3141\n",
      "Epoch 239/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3013\n",
      "Epoch 240/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3396\n",
      "Epoch 241/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3012\n",
      "Epoch 242/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2884\n",
      "Epoch 243/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3333\n",
      "Epoch 244/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2993\n",
      "Epoch 245/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2934\n",
      "Epoch 246/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3082\n",
      "Epoch 247/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3022\n",
      "Epoch 248/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2782\n",
      "Epoch 249/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3235\n",
      "Epoch 250/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2902\n",
      "Epoch 251/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2940\n",
      "Epoch 252/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2771\n",
      "Epoch 253/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2558\n",
      "Epoch 254/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2760\n",
      "Epoch 255/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3020\n",
      "Epoch 256/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2784\n",
      "Epoch 257/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2901\n",
      "Epoch 258/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2974\n",
      "Epoch 259/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3001\n",
      "Epoch 260/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2781\n",
      "Epoch 261/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2994\n",
      "Epoch 262/600\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2774\n",
      "Epoch 263/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2914\n",
      "Epoch 264/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2944\n",
      "Epoch 265/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2691\n",
      "Epoch 266/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2734\n",
      "Epoch 267/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2645\n",
      "Epoch 268/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2803\n",
      "Epoch 269/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2501\n",
      "Epoch 270/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2627\n",
      "Epoch 271/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2653\n",
      "Epoch 272/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2534\n",
      "Epoch 273/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2643\n",
      "Epoch 274/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2583\n",
      "Epoch 275/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2618\n",
      "Epoch 276/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2441\n",
      "Epoch 277/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2433\n",
      "Epoch 278/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2360\n",
      "Epoch 279/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2573\n",
      "Epoch 280/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2534\n",
      "Epoch 281/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2388\n",
      "Epoch 282/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2357\n",
      "Epoch 283/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2320\n",
      "Epoch 284/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2381\n",
      "Epoch 285/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2508\n",
      "Epoch 286/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2278\n",
      "Epoch 287/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2331\n",
      "Epoch 288/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2238\n",
      "Epoch 289/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2309\n",
      "Epoch 290/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2068\n",
      "Epoch 291/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2328\n",
      "Epoch 292/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2153\n",
      "Epoch 293/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2383\n",
      "Epoch 294/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2302\n",
      "Epoch 295/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2339\n",
      "Epoch 296/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2237\n",
      "Epoch 297/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2162\n",
      "Epoch 298/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2347\n",
      "Epoch 299/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2470\n",
      "Epoch 300/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2334\n",
      "Epoch 301/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2214\n",
      "Epoch 302/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2282\n",
      "Epoch 303/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2141\n",
      "Epoch 304/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2161\n",
      "Epoch 305/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2104\n",
      "Epoch 306/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2380\n",
      "Epoch 307/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2079\n",
      "Epoch 308/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1881\n",
      "Epoch 309/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2388\n",
      "Epoch 310/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2229\n",
      "Epoch 311/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1901\n",
      "Epoch 312/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2367\n",
      "Epoch 313/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2006\n",
      "Epoch 314/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2009\n",
      "Epoch 315/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2162\n",
      "Epoch 316/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2017\n",
      "Epoch 317/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1969\n",
      "Epoch 318/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2005\n",
      "Epoch 319/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1956\n",
      "Epoch 320/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1935\n",
      "Epoch 321/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1882\n",
      "Epoch 322/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2203\n",
      "Epoch 323/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1944\n",
      "Epoch 324/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1828\n",
      "Epoch 325/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1799\n",
      "Epoch 326/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1845\n",
      "Epoch 327/600\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.1978\n",
      "Epoch 328/600\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.2022\n",
      "Epoch 329/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1783\n",
      "Epoch 330/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2071\n",
      "Epoch 331/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2003\n",
      "Epoch 332/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1793\n",
      "Epoch 333/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1745\n",
      "Epoch 334/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1896\n",
      "Epoch 335/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1868\n",
      "Epoch 336/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1816\n",
      "Epoch 337/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1683\n",
      "Epoch 338/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2027\n",
      "Epoch 339/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1767\n",
      "Epoch 340/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2030\n",
      "Epoch 341/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2055\n",
      "Epoch 342/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1837\n",
      "Epoch 343/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1493\n",
      "Epoch 344/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1598\n",
      "Epoch 345/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1538\n",
      "Epoch 346/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1614\n",
      "Epoch 347/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1433\n",
      "Epoch 348/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1699\n",
      "Epoch 349/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1873\n",
      "Epoch 350/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1492\n",
      "Epoch 351/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1531\n",
      "Epoch 352/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1560\n",
      "Epoch 353/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1484\n",
      "Epoch 354/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1476\n",
      "Epoch 355/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1457\n",
      "Epoch 356/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1543\n",
      "Epoch 357/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1314\n",
      "Epoch 358/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1672\n",
      "Epoch 359/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1604\n",
      "Epoch 360/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1396\n",
      "Epoch 361/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1530\n",
      "Epoch 362/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1388\n",
      "Epoch 363/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1592\n",
      "Epoch 364/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1481\n",
      "Epoch 365/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1703\n",
      "Epoch 366/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1528\n",
      "Epoch 367/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1366\n",
      "Epoch 368/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1464\n",
      "Epoch 369/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1692\n",
      "Epoch 370/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1624\n",
      "Epoch 371/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1282\n",
      "Epoch 372/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1441\n",
      "Epoch 373/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1293\n",
      "Epoch 374/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1487\n",
      "Epoch 375/600\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.169 - 0s 2ms/step - loss: 2.1554\n",
      "Epoch 376/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1430\n",
      "Epoch 377/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1237\n",
      "Epoch 378/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1283\n",
      "Epoch 379/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1243\n",
      "Epoch 380/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1332\n",
      "Epoch 381/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1541\n",
      "Epoch 382/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1067\n",
      "Epoch 383/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1040\n",
      "Epoch 384/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1166\n",
      "Epoch 385/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1435\n",
      "Epoch 386/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1378\n",
      "Epoch 387/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1248\n",
      "Epoch 388/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1170\n",
      "Epoch 389/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1105\n",
      "Epoch 390/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1042\n",
      "Epoch 391/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1126\n",
      "Epoch 392/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1270\n",
      "Epoch 393/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1314\n",
      "Epoch 394/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1157\n",
      "Epoch 395/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1143\n",
      "Epoch 396/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1090\n",
      "Epoch 397/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0874\n",
      "Epoch 398/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1129\n",
      "Epoch 399/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1042\n",
      "Epoch 400/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0977\n",
      "Epoch 401/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1078\n",
      "Epoch 402/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1029\n",
      "Epoch 403/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1463\n",
      "Epoch 404/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1186\n",
      "Epoch 405/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0869\n",
      "Epoch 406/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1024\n",
      "Epoch 407/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0773\n",
      "Epoch 408/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0787\n",
      "Epoch 409/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1202\n",
      "Epoch 410/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0807\n",
      "Epoch 411/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1095\n",
      "Epoch 412/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1020\n",
      "Epoch 413/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0952\n",
      "Epoch 414/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0866\n",
      "Epoch 415/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0965\n",
      "Epoch 416/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0762\n",
      "Epoch 417/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0825\n",
      "Epoch 418/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1023\n",
      "Epoch 419/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0751\n",
      "Epoch 420/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.1346\n",
      "Epoch 421/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0747\n",
      "Epoch 422/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0793\n",
      "Epoch 423/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0703\n",
      "Epoch 424/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0845\n",
      "Epoch 425/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0915\n",
      "Epoch 426/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0592\n",
      "Epoch 427/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0583\n",
      "Epoch 428/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0719\n",
      "Epoch 429/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1144\n",
      "Epoch 430/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0558\n",
      "Epoch 431/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0935\n",
      "Epoch 432/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0709\n",
      "Epoch 433/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0681\n",
      "Epoch 434/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0694\n",
      "Epoch 435/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0376\n",
      "Epoch 436/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0578\n",
      "Epoch 437/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0589\n",
      "Epoch 438/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0651\n",
      "Epoch 439/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.0707\n",
      "Epoch 440/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0533\n",
      "Epoch 441/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0499\n",
      "Epoch 442/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0745\n",
      "Epoch 443/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0568\n",
      "Epoch 444/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0368\n",
      "Epoch 445/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0760\n",
      "Epoch 446/600\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.0421\n",
      "Epoch 447/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0295\n",
      "Epoch 448/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0636\n",
      "Epoch 449/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0444\n",
      "Epoch 450/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0349\n",
      "Epoch 451/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0372\n",
      "Epoch 452/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0450\n",
      "Epoch 453/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0374\n",
      "Epoch 454/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0182\n",
      "Epoch 455/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0304\n",
      "Epoch 456/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0695\n",
      "Epoch 457/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0745\n",
      "Epoch 458/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0233\n",
      "Epoch 459/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0398\n",
      "Epoch 460/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0339\n",
      "Epoch 461/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0253\n",
      "Epoch 462/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0095\n",
      "Epoch 463/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0495\n",
      "Epoch 464/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0599\n",
      "Epoch 465/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0285\n",
      "Epoch 466/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0236\n",
      "Epoch 467/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0331\n",
      "Epoch 468/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0297\n",
      "Epoch 469/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0662\n",
      "Epoch 470/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0136\n",
      "Epoch 471/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0425\n",
      "Epoch 472/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0182\n",
      "Epoch 473/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0113\n",
      "Epoch 474/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0325\n",
      "Epoch 475/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0206\n",
      "Epoch 476/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0306\n",
      "Epoch 477/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9961\n",
      "Epoch 478/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0249\n",
      "Epoch 479/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0689\n",
      "Epoch 480/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0000\n",
      "Epoch 481/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0361\n",
      "Epoch 482/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0099\n",
      "Epoch 483/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0270\n",
      "Epoch 484/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0016\n",
      "Epoch 485/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0186\n",
      "Epoch 486/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0150\n",
      "Epoch 487/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0058\n",
      "Epoch 488/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0210\n",
      "Epoch 489/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0098\n",
      "Epoch 490/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9950\n",
      "Epoch 491/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9950\n",
      "Epoch 492/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9869\n",
      "Epoch 493/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9877\n",
      "Epoch 494/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9832\n",
      "Epoch 495/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9843\n",
      "Epoch 496/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9847\n",
      "Epoch 497/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9726\n",
      "Epoch 498/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9752\n",
      "Epoch 499/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9986\n",
      "Epoch 500/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0224\n",
      "Epoch 501/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9955\n",
      "Epoch 502/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0192\n",
      "Epoch 503/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9903\n",
      "Epoch 504/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9873\n",
      "Epoch 505/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9655\n",
      "Epoch 506/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9904\n",
      "Epoch 507/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9822\n",
      "Epoch 508/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9944\n",
      "Epoch 509/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9955\n",
      "Epoch 510/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9622\n",
      "Epoch 511/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9812\n",
      "Epoch 512/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9931\n",
      "Epoch 513/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9730\n",
      "Epoch 514/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9528\n",
      "Epoch 515/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9636\n",
      "Epoch 516/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9680\n",
      "Epoch 517/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9624\n",
      "Epoch 518/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0064\n",
      "Epoch 519/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9956\n",
      "Epoch 520/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9703\n",
      "Epoch 521/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9851\n",
      "Epoch 522/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9760\n",
      "Epoch 523/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9670\n",
      "Epoch 524/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9727\n",
      "Epoch 525/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9483\n",
      "Epoch 526/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9573\n",
      "Epoch 527/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9587\n",
      "Epoch 528/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9960\n",
      "Epoch 529/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9873\n",
      "Epoch 530/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9682\n",
      "Epoch 531/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9410\n",
      "Epoch 532/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9657\n",
      "Epoch 533/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9643\n",
      "Epoch 534/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9567\n",
      "Epoch 535/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9424\n",
      "Epoch 536/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9479\n",
      "Epoch 537/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9804\n",
      "Epoch 538/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9523\n",
      "Epoch 539/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9644\n",
      "Epoch 540/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9566\n",
      "Epoch 541/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9442\n",
      "Epoch 542/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9485\n",
      "Epoch 543/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9337\n",
      "Epoch 544/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9713\n",
      "Epoch 545/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9380\n",
      "Epoch 546/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9774\n",
      "Epoch 547/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.0065\n",
      "Epoch 548/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9306\n",
      "Epoch 549/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9199\n",
      "Epoch 550/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9742\n",
      "Epoch 551/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9275\n",
      "Epoch 552/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9688\n",
      "Epoch 553/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9158\n",
      "Epoch 554/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9443\n",
      "Epoch 555/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9491\n",
      "Epoch 556/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9391\n",
      "Epoch 557/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9241\n",
      "Epoch 558/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9207\n",
      "Epoch 559/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9309\n",
      "Epoch 560/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9414\n",
      "Epoch 561/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9004\n",
      "Epoch 562/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9193\n",
      "Epoch 563/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9424\n",
      "Epoch 564/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9544\n",
      "Epoch 565/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9303\n",
      "Epoch 566/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9514\n",
      "Epoch 567/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9562\n",
      "Epoch 568/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9301\n",
      "Epoch 569/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9195\n",
      "Epoch 570/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9305\n",
      "Epoch 571/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9235\n",
      "Epoch 572/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9263\n",
      "Epoch 573/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9216\n",
      "Epoch 574/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9625\n",
      "Epoch 575/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9047\n",
      "Epoch 576/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9290\n",
      "Epoch 577/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9042\n",
      "Epoch 578/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9016\n",
      "Epoch 579/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8858\n",
      "Epoch 580/600\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.8975\n",
      "Epoch 581/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8854\n",
      "Epoch 582/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9026\n",
      "Epoch 583/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9160\n",
      "Epoch 584/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9501\n",
      "Epoch 585/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9278\n",
      "Epoch 586/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8814\n",
      "Epoch 587/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8817\n",
      "Epoch 588/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8832\n",
      "Epoch 589/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8734\n",
      "Epoch 590/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8956\n",
      "Epoch 591/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8960\n",
      "Epoch 592/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8993\n",
      "Epoch 593/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8783\n",
      "Epoch 594/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9177\n",
      "Epoch 595/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.8748\n",
      "Epoch 596/600\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.8855\n",
      "Epoch 597/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9032\n",
      "Epoch 598/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9547\n",
      "Epoch 599/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9030\n",
      "Epoch 600/600\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.8997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [46.71730422973633,\n",
       "  20.152318954467773,\n",
       "  11.95750904083252,\n",
       "  8.428056716918945,\n",
       "  7.929474830627441,\n",
       "  7.340498924255371,\n",
       "  6.662458419799805,\n",
       "  5.9311981201171875,\n",
       "  5.1826958656311035,\n",
       "  4.5927205085754395,\n",
       "  4.170983791351318,\n",
       "  3.9073517322540283,\n",
       "  3.7247838973999023,\n",
       "  3.568211078643799,\n",
       "  3.410874843597412,\n",
       "  3.2862448692321777,\n",
       "  3.2440707683563232,\n",
       "  3.1881356239318848,\n",
       "  3.1353983879089355,\n",
       "  3.0726704597473145,\n",
       "  3.0817902088165283,\n",
       "  3.0227091312408447,\n",
       "  2.967646598815918,\n",
       "  2.977918863296509,\n",
       "  2.9356770515441895,\n",
       "  2.9272232055664062,\n",
       "  2.9249727725982666,\n",
       "  2.920020818710327,\n",
       "  2.858696222305298,\n",
       "  2.875558853149414,\n",
       "  2.8549695014953613,\n",
       "  2.8530218601226807,\n",
       "  2.842785358428955,\n",
       "  2.8152689933776855,\n",
       "  2.805387020111084,\n",
       "  2.822376251220703,\n",
       "  2.803217649459839,\n",
       "  2.7948267459869385,\n",
       "  2.770052671432495,\n",
       "  2.7735109329223633,\n",
       "  2.773333787918091,\n",
       "  2.778918743133545,\n",
       "  2.7444944381713867,\n",
       "  2.755143404006958,\n",
       "  2.754246711730957,\n",
       "  2.731682777404785,\n",
       "  2.7284321784973145,\n",
       "  2.7296674251556396,\n",
       "  2.7328736782073975,\n",
       "  2.7259864807128906,\n",
       "  2.712367296218872,\n",
       "  2.7211225032806396,\n",
       "  2.7037429809570312,\n",
       "  2.6998698711395264,\n",
       "  2.7343642711639404,\n",
       "  2.6898977756500244,\n",
       "  2.703852891921997,\n",
       "  2.7041432857513428,\n",
       "  2.6739466190338135,\n",
       "  2.6977856159210205,\n",
       "  2.6750311851501465,\n",
       "  2.6652629375457764,\n",
       "  2.6642491817474365,\n",
       "  2.6395304203033447,\n",
       "  2.6810317039489746,\n",
       "  2.6908957958221436,\n",
       "  2.644500255584717,\n",
       "  2.643120288848877,\n",
       "  2.6409900188446045,\n",
       "  2.6410465240478516,\n",
       "  2.6547999382019043,\n",
       "  2.678718090057373,\n",
       "  2.6602654457092285,\n",
       "  2.6485910415649414,\n",
       "  2.6423933506011963,\n",
       "  2.6541831493377686,\n",
       "  2.6541988849639893,\n",
       "  2.666243553161621,\n",
       "  2.630298614501953,\n",
       "  2.6145622730255127,\n",
       "  2.6315207481384277,\n",
       "  2.6207661628723145,\n",
       "  2.61953067779541,\n",
       "  2.6161272525787354,\n",
       "  2.619224786758423,\n",
       "  2.6214053630828857,\n",
       "  2.588144540786743,\n",
       "  2.5975987911224365,\n",
       "  2.5780670642852783,\n",
       "  2.611499309539795,\n",
       "  2.6040353775024414,\n",
       "  2.5755858421325684,\n",
       "  2.5742228031158447,\n",
       "  2.5921273231506348,\n",
       "  2.6152777671813965,\n",
       "  2.5787770748138428,\n",
       "  2.556582450866699,\n",
       "  2.607149600982666,\n",
       "  2.5659408569335938,\n",
       "  2.584101676940918,\n",
       "  2.5309813022613525,\n",
       "  2.565186023712158,\n",
       "  2.559321165084839,\n",
       "  2.5410540103912354,\n",
       "  2.567533254623413,\n",
       "  2.5421173572540283,\n",
       "  2.571920394897461,\n",
       "  2.5257904529571533,\n",
       "  2.5755746364593506,\n",
       "  2.5261149406433105,\n",
       "  2.549546718597412,\n",
       "  2.5337090492248535,\n",
       "  2.531277894973755,\n",
       "  2.5338134765625,\n",
       "  2.538908004760742,\n",
       "  2.529200553894043,\n",
       "  2.52170729637146,\n",
       "  2.5110058784484863,\n",
       "  2.502814531326294,\n",
       "  2.5004427433013916,\n",
       "  2.5204577445983887,\n",
       "  2.513731002807617,\n",
       "  2.5095722675323486,\n",
       "  2.5084328651428223,\n",
       "  2.51090145111084,\n",
       "  2.518251657485962,\n",
       "  2.50300669670105,\n",
       "  2.480022430419922,\n",
       "  2.498817205429077,\n",
       "  2.495168924331665,\n",
       "  2.500192880630493,\n",
       "  2.488617420196533,\n",
       "  2.4805896282196045,\n",
       "  2.4849727153778076,\n",
       "  2.496927499771118,\n",
       "  2.493011474609375,\n",
       "  2.4765660762786865,\n",
       "  2.483426332473755,\n",
       "  2.4835143089294434,\n",
       "  2.4925155639648438,\n",
       "  2.4710822105407715,\n",
       "  2.47957444190979,\n",
       "  2.4759738445281982,\n",
       "  2.461232900619507,\n",
       "  2.4582512378692627,\n",
       "  2.4633893966674805,\n",
       "  2.481926441192627,\n",
       "  2.4546337127685547,\n",
       "  2.4611363410949707,\n",
       "  2.4517593383789062,\n",
       "  2.4418253898620605,\n",
       "  2.4577598571777344,\n",
       "  2.4446914196014404,\n",
       "  2.458890676498413,\n",
       "  2.446913480758667,\n",
       "  2.4415123462677,\n",
       "  2.4611635208129883,\n",
       "  2.4361329078674316,\n",
       "  2.43898606300354,\n",
       "  2.4488377571105957,\n",
       "  2.4507761001586914,\n",
       "  2.42415452003479,\n",
       "  2.457146644592285,\n",
       "  2.449106454849243,\n",
       "  2.416060209274292,\n",
       "  2.4212591648101807,\n",
       "  2.4429209232330322,\n",
       "  2.4468846321105957,\n",
       "  2.423335075378418,\n",
       "  2.4188616275787354,\n",
       "  2.440770149230957,\n",
       "  2.4087395668029785,\n",
       "  2.4036262035369873,\n",
       "  2.412550449371338,\n",
       "  2.402416944503784,\n",
       "  2.408308744430542,\n",
       "  2.4009811878204346,\n",
       "  2.389709234237671,\n",
       "  2.4169676303863525,\n",
       "  2.3993892669677734,\n",
       "  2.388322591781616,\n",
       "  2.4007911682128906,\n",
       "  2.414841651916504,\n",
       "  2.3906993865966797,\n",
       "  2.3943135738372803,\n",
       "  2.3832850456237793,\n",
       "  2.378032684326172,\n",
       "  2.3980002403259277,\n",
       "  2.367905616760254,\n",
       "  2.400007486343384,\n",
       "  2.3756284713745117,\n",
       "  2.406599521636963,\n",
       "  2.3766214847564697,\n",
       "  2.3727481365203857,\n",
       "  2.373032808303833,\n",
       "  2.3731472492218018,\n",
       "  2.379838228225708,\n",
       "  2.375685930252075,\n",
       "  2.3698177337646484,\n",
       "  2.382340908050537,\n",
       "  2.371546506881714,\n",
       "  2.3699073791503906,\n",
       "  2.3777670860290527,\n",
       "  2.358491897583008,\n",
       "  2.349430799484253,\n",
       "  2.3534700870513916,\n",
       "  2.349395513534546,\n",
       "  2.3505406379699707,\n",
       "  2.345386505126953,\n",
       "  2.349342107772827,\n",
       "  2.392827272415161,\n",
       "  2.340972900390625,\n",
       "  2.353236436843872,\n",
       "  2.330151319503784,\n",
       "  2.3455162048339844,\n",
       "  2.323972225189209,\n",
       "  2.372617483139038,\n",
       "  2.3473784923553467,\n",
       "  2.345296621322632,\n",
       "  2.327308416366577,\n",
       "  2.3334591388702393,\n",
       "  2.3844101428985596,\n",
       "  2.315462350845337,\n",
       "  2.334268569946289,\n",
       "  2.323326826095581,\n",
       "  2.321230173110962,\n",
       "  2.3485188484191895,\n",
       "  2.32002592086792,\n",
       "  2.318467140197754,\n",
       "  2.3059327602386475,\n",
       "  2.3152379989624023,\n",
       "  2.3200833797454834,\n",
       "  2.3173115253448486,\n",
       "  2.3182313442230225,\n",
       "  2.313429355621338,\n",
       "  2.3255560398101807,\n",
       "  2.307640790939331,\n",
       "  2.314148187637329,\n",
       "  2.3012917041778564,\n",
       "  2.339592456817627,\n",
       "  2.301175355911255,\n",
       "  2.2883596420288086,\n",
       "  2.3333089351654053,\n",
       "  2.2992794513702393,\n",
       "  2.2934415340423584,\n",
       "  2.3082194328308105,\n",
       "  2.3022472858428955,\n",
       "  2.2781689167022705,\n",
       "  2.32346510887146,\n",
       "  2.290219783782959,\n",
       "  2.293971061706543,\n",
       "  2.27706241607666,\n",
       "  2.2558412551879883,\n",
       "  2.276012659072876,\n",
       "  2.30199933052063,\n",
       "  2.278365135192871,\n",
       "  2.2900733947753906,\n",
       "  2.2974486351013184,\n",
       "  2.3000528812408447,\n",
       "  2.2781224250793457,\n",
       "  2.2993838787078857,\n",
       "  2.2774035930633545,\n",
       "  2.291436195373535,\n",
       "  2.294433355331421,\n",
       "  2.2690908908843994,\n",
       "  2.273430824279785,\n",
       "  2.264488458633423,\n",
       "  2.2803030014038086,\n",
       "  2.2500813007354736,\n",
       "  2.26267671585083,\n",
       "  2.2653164863586426,\n",
       "  2.2533960342407227,\n",
       "  2.2642993927001953,\n",
       "  2.258272171020508,\n",
       "  2.2618396282196045,\n",
       "  2.244121789932251,\n",
       "  2.24328875541687,\n",
       "  2.236025810241699,\n",
       "  2.257302761077881,\n",
       "  2.253366231918335,\n",
       "  2.2387752532958984,\n",
       "  2.2357139587402344,\n",
       "  2.231985092163086,\n",
       "  2.2381205558776855,\n",
       "  2.2507824897766113,\n",
       "  2.2277817726135254,\n",
       "  2.233111619949341,\n",
       "  2.2238337993621826,\n",
       "  2.2308855056762695,\n",
       "  2.206758737564087,\n",
       "  2.2328484058380127,\n",
       "  2.215266704559326,\n",
       "  2.2383220195770264,\n",
       "  2.2301583290100098,\n",
       "  2.2338993549346924,\n",
       "  2.2237071990966797,\n",
       "  2.216236114501953,\n",
       "  2.2346510887145996,\n",
       "  2.2469959259033203,\n",
       "  2.233431577682495,\n",
       "  2.221428871154785,\n",
       "  2.2281618118286133,\n",
       "  2.214083671569824,\n",
       "  2.216062545776367,\n",
       "  2.210381507873535,\n",
       "  2.238022565841675,\n",
       "  2.2079219818115234,\n",
       "  2.188124656677246,\n",
       "  2.2387545108795166,\n",
       "  2.2229232788085938,\n",
       "  2.190121650695801,\n",
       "  2.236670970916748,\n",
       "  2.2005975246429443,\n",
       "  2.2009122371673584,\n",
       "  2.21622896194458,\n",
       "  2.201688289642334,\n",
       "  2.1969356536865234,\n",
       "  2.2004520893096924,\n",
       "  2.1956276893615723,\n",
       "  2.1934585571289062,\n",
       "  2.1882450580596924,\n",
       "  2.2203476428985596,\n",
       "  2.1944031715393066,\n",
       "  2.182765483856201,\n",
       "  2.1799099445343018,\n",
       "  2.184497833251953,\n",
       "  2.197845458984375,\n",
       "  2.2021641731262207,\n",
       "  2.178342819213867,\n",
       "  2.2070791721343994,\n",
       "  2.200349807739258,\n",
       "  2.179316997528076,\n",
       "  2.174464225769043,\n",
       "  2.1895949840545654,\n",
       "  2.1867852210998535,\n",
       "  2.181644916534424,\n",
       "  2.1683244705200195,\n",
       "  2.2026777267456055,\n",
       "  2.176746368408203,\n",
       "  2.203007221221924,\n",
       "  2.205519437789917,\n",
       "  2.1837317943573,\n",
       "  2.1493148803710938,\n",
       "  2.159810781478882,\n",
       "  2.153836488723755,\n",
       "  2.161367893218994,\n",
       "  2.143324851989746,\n",
       "  2.1699085235595703,\n",
       "  2.187258720397949,\n",
       "  2.149218797683716,\n",
       "  2.153068780899048,\n",
       "  2.156038761138916,\n",
       "  2.148419141769409,\n",
       "  2.1476082801818848,\n",
       "  2.145719528198242,\n",
       "  2.1543259620666504,\n",
       "  2.1313931941986084,\n",
       "  2.1671817302703857,\n",
       "  2.1603739261627197,\n",
       "  2.1396093368530273,\n",
       "  2.153005599975586,\n",
       "  2.138781785964966,\n",
       "  2.1591620445251465,\n",
       "  2.148092031478882,\n",
       "  2.170254945755005,\n",
       "  2.152782917022705,\n",
       "  2.136603832244873,\n",
       "  2.146355152130127,\n",
       "  2.1692190170288086,\n",
       "  2.162416458129883,\n",
       "  2.1282222270965576,\n",
       "  2.144076347351074,\n",
       "  2.1292710304260254,\n",
       "  2.148719072341919,\n",
       "  2.1553587913513184,\n",
       "  2.143044948577881,\n",
       "  2.1237146854400635,\n",
       "  2.128275156021118,\n",
       "  2.1242964267730713,\n",
       "  2.133213996887207,\n",
       "  2.154060125350952,\n",
       "  2.106665849685669,\n",
       "  2.103980541229248,\n",
       "  2.1165785789489746,\n",
       "  2.143496513366699,\n",
       "  2.137770175933838,\n",
       "  2.1248159408569336,\n",
       "  2.1170365810394287,\n",
       "  2.1104815006256104,\n",
       "  2.1041722297668457,\n",
       "  2.1126129627227783,\n",
       "  2.1270220279693604,\n",
       "  2.131354331970215,\n",
       "  2.1157443523406982,\n",
       "  2.114298105239868,\n",
       "  2.1090245246887207,\n",
       "  2.0873875617980957,\n",
       "  2.1128969192504883,\n",
       "  2.104233980178833,\n",
       "  2.097707748413086,\n",
       "  2.107754945755005,\n",
       "  2.1029200553894043,\n",
       "  2.1462926864624023,\n",
       "  2.1186437606811523,\n",
       "  2.0869438648223877,\n",
       "  2.1023545265197754,\n",
       "  2.077324867248535,\n",
       "  2.078720808029175,\n",
       "  2.1201767921447754,\n",
       "  2.0806639194488525,\n",
       "  2.1094985008239746,\n",
       "  2.1020455360412598,\n",
       "  2.095202684402466,\n",
       "  2.086617946624756,\n",
       "  2.096450090408325,\n",
       "  2.0762012004852295,\n",
       "  2.0824878215789795,\n",
       "  2.1022839546203613,\n",
       "  2.075071096420288,\n",
       "  2.13457989692688,\n",
       "  2.0746989250183105,\n",
       "  2.0792582035064697,\n",
       "  2.070329189300537,\n",
       "  2.084505081176758,\n",
       "  2.091543436050415,\n",
       "  2.0591979026794434,\n",
       "  2.0583105087280273,\n",
       "  2.071911334991455,\n",
       "  2.1144466400146484,\n",
       "  2.05580735206604,\n",
       "  2.0934863090515137,\n",
       "  2.0708844661712646,\n",
       "  2.0680673122406006,\n",
       "  2.0694241523742676,\n",
       "  2.0376038551330566,\n",
       "  2.0577871799468994,\n",
       "  2.0588536262512207,\n",
       "  2.0651392936706543,\n",
       "  2.0707156658172607,\n",
       "  2.0532853603363037,\n",
       "  2.0499022006988525,\n",
       "  2.0744524002075195,\n",
       "  2.0567522048950195,\n",
       "  2.0367608070373535,\n",
       "  2.0760433673858643,\n",
       "  2.0421383380889893,\n",
       "  2.0295307636260986,\n",
       "  2.0635569095611572,\n",
       "  2.044372320175171,\n",
       "  2.0349087715148926,\n",
       "  2.037160634994507,\n",
       "  2.0450358390808105,\n",
       "  2.03739070892334,\n",
       "  2.0181736946105957,\n",
       "  2.030381917953491,\n",
       "  2.069505453109741,\n",
       "  2.0745301246643066,\n",
       "  2.0233211517333984,\n",
       "  2.039788007736206,\n",
       "  2.0338551998138428,\n",
       "  2.025299549102783,\n",
       "  2.0095221996307373,\n",
       "  2.0495076179504395,\n",
       "  2.059889554977417,\n",
       "  2.0285253524780273,\n",
       "  2.023632287979126,\n",
       "  2.03306245803833,\n",
       "  2.0297062397003174,\n",
       "  2.066189765930176,\n",
       "  2.0135786533355713,\n",
       "  2.042501211166382,\n",
       "  2.0181729793548584,\n",
       "  2.0112569332122803,\n",
       "  2.032524824142456,\n",
       "  2.0206074714660645,\n",
       "  2.030555009841919,\n",
       "  1.9961214065551758,\n",
       "  2.0249009132385254,\n",
       "  2.0689311027526855,\n",
       "  2.00002384185791,\n",
       "  2.0361199378967285,\n",
       "  2.0098507404327393,\n",
       "  2.0270140171051025,\n",
       "  2.0016393661499023,\n",
       "  2.0185723304748535,\n",
       "  2.0150222778320312,\n",
       "  2.0058419704437256,\n",
       "  2.02101731300354,\n",
       "  2.0098490715026855,\n",
       "  1.9950282573699951,\n",
       "  1.994999885559082,\n",
       "  1.9869048595428467,\n",
       "  1.98771071434021,\n",
       "  1.983160376548767,\n",
       "  1.9843367338180542,\n",
       "  1.9847420454025269,\n",
       "  1.972598910331726,\n",
       "  1.9752188920974731,\n",
       "  1.998580813407898,\n",
       "  2.0223865509033203,\n",
       "  1.995537281036377,\n",
       "  2.0191705226898193,\n",
       "  1.9902868270874023,\n",
       "  1.987343192100525,\n",
       "  1.9654772281646729,\n",
       "  1.990437626838684,\n",
       "  1.982212781906128,\n",
       "  1.9943655729293823,\n",
       "  1.9954569339752197,\n",
       "  1.9621660709381104,\n",
       "  1.9811779260635376,\n",
       "  1.9930678606033325,\n",
       "  1.9730340242385864,\n",
       "  1.952832579612732,\n",
       "  1.9636411666870117,\n",
       "  1.9679677486419678,\n",
       "  1.9624398946762085,\n",
       "  2.0063536167144775,\n",
       "  1.9956399202346802,\n",
       "  1.9702612161636353,\n",
       "  1.9850642681121826,\n",
       "  1.976043462753296,\n",
       "  1.9670416116714478,\n",
       "  1.9727375507354736,\n",
       "  1.9482600688934326,\n",
       "  1.9572925567626953,\n",
       "  1.9586838483810425,\n",
       "  1.9960236549377441,\n",
       "  1.9873212575912476,\n",
       "  1.9682016372680664,\n",
       "  1.9409807920455933,\n",
       "  1.9657055139541626,\n",
       "  1.964340329170227,\n",
       "  1.956697702407837,\n",
       "  1.942401647567749,\n",
       "  1.9478689432144165,\n",
       "  1.9803770780563354,\n",
       "  1.9522647857666016,\n",
       "  1.9643584489822388,\n",
       "  1.956633448600769,\n",
       "  1.9442226886749268,\n",
       "  1.9484721422195435,\n",
       "  1.9337340593338013,\n",
       "  1.9712992906570435,\n",
       "  1.9379947185516357,\n",
       "  1.9774284362792969,\n",
       "  2.006460666656494,\n",
       "  1.9305777549743652,\n",
       "  1.9199419021606445,\n",
       "  1.9741768836975098,\n",
       "  1.9275081157684326,\n",
       "  1.9688191413879395,\n",
       "  1.9157928228378296,\n",
       "  1.9443376064300537,\n",
       "  1.9490513801574707,\n",
       "  1.9391133785247803,\n",
       "  1.9240577220916748,\n",
       "  1.9207247495651245,\n",
       "  1.9309285879135132,\n",
       "  1.9414373636245728,\n",
       "  1.9003615379333496,\n",
       "  1.919295072555542,\n",
       "  1.9424182176589966,\n",
       "  1.9543638229370117,\n",
       "  1.930250644683838,\n",
       "  1.9513838291168213,\n",
       "  1.956190824508667,\n",
       "  1.9300862550735474,\n",
       "  1.919451355934143,\n",
       "  1.9305022954940796,\n",
       "  1.9234565496444702,\n",
       "  1.9262818098068237,\n",
       "  1.9216445684432983,\n",
       "  1.962548851966858,\n",
       "  1.904684066772461,\n",
       "  1.9289677143096924,\n",
       "  1.9042448997497559,\n",
       "  1.9016013145446777,\n",
       "  1.8858181238174438,\n",
       "  1.897479772567749,\n",
       "  1.8854422569274902,\n",
       "  1.9026323556900024,\n",
       "  1.916021466255188,\n",
       "  1.9501023292541504,\n",
       "  1.9277820587158203,\n",
       "  1.881422519683838,\n",
       "  1.8817166090011597,\n",
       "  1.8831946849822998,\n",
       "  1.8734074831008911,\n",
       "  1.8956414461135864,\n",
       "  1.8959757089614868,\n",
       "  1.8992542028427124,\n",
       "  1.878283143043518,\n",
       "  1.9176969528198242,\n",
       "  1.8748077154159546,\n",
       "  1.8854554891586304,\n",
       "  1.9031527042388916,\n",
       "  1.9547406435012817,\n",
       "  1.903010606765747,\n",
       "  1.89969801902771]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyVal_MB = []\n",
    "historyTr_MB = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_MB()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=100).history\n",
    "    historyVal_MB.append(history['val_loss'])\n",
    "    historyTr_MB.append(history['loss'])\n",
    "model=create_model_MB()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=600, \n",
    "                      batch_size=100).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_MB_mean=np.mean(historyVal_MB, axis=0)\n",
    "historyTr_MB_mean=np.mean(historyTr_MB, axis=0)\n",
    "\n",
    "historyVal_MB_sd=np.std(historyVal_MB, axis=0)\n",
    "historyTr_MB_sd=np.std(historyTr_MB, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_MB_mean)),historyTr_MB_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_MB_mean)), historyTr_MB_mean - historyTr_MB_sd, \n",
    "                 historyTr_MB_mean + historyTr_MB_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_MB_mean)), historyVal_MB_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_MB_mean)), historyVal_MB_mean - historyVal_MB_sd, \n",
    "                 historyVal_MB_mean + historyVal_MB_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 21, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch (mb=100) result:\n",
      "MEE on the validation 2.895853042602539 with standard deviation 0.232702346020446\n",
      "MEE on the training 1.8649487495422363 with standard deviation 0.06880880003739917\n"
     ]
    }
   ],
   "source": [
    "print(\"Mini batch (mb=100) result:\")\n",
    "print(\"MEE on the validation\",historyVal_MB_mean[-1],\"with standard deviation\",historyVal_MB_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_MB_mean[-1],\"with standard deviation\",historyTr_MB_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_OL():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=0.01, \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 12.6401 - val_loss: 5.0677\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 4.2901 - val_loss: 3.8571\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.5978 - val_loss: 3.6670\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.4240 - val_loss: 3.4831\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.2769 - val_loss: 3.1473\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.1960 - val_loss: 3.5719\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.1642 - val_loss: 3.5089\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.0667 - val_loss: 3.1592\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0005 - val_loss: 3.3057\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9795 - val_loss: 3.0222\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9860 - val_loss: 2.9907\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9064 - val_loss: 2.9053\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8956 - val_loss: 3.0357\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8169 - val_loss: 3.1027\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8291 - val_loss: 2.9784\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8186 - val_loss: 3.2552\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8032 - val_loss: 2.9573\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.8209 - val_loss: 2.8288\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7571 - val_loss: 2.7611\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7273 - val_loss: 2.8121\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7151 - val_loss: 2.7130\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6876 - val_loss: 3.1068\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6708 - val_loss: 3.4050\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6606 - val_loss: 2.8222\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6287 - val_loss: 2.7433\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6429 - val_loss: 2.9771\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6348 - val_loss: 2.9007\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6070 - val_loss: 3.2226\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5783 - val_loss: 2.7980\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5788 - val_loss: 3.2358\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5446 - val_loss: 2.9089\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5651 - val_loss: 2.9630\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5533 - val_loss: 2.8409\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5468 - val_loss: 2.8922\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5296 - val_loss: 2.6713\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5386 - val_loss: 2.7263\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5280 - val_loss: 2.6264\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5125 - val_loss: 2.8003\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5141 - val_loss: 2.8089\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4737 - val_loss: 2.8890\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5020 - val_loss: 2.7963\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4711 - val_loss: 2.6774\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4750 - val_loss: 2.8836\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4649 - val_loss: 2.8382\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4344 - val_loss: 2.9424\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4409 - val_loss: 2.9726\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4236 - val_loss: 2.9556\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4263 - val_loss: 3.0214\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3997 - val_loss: 2.7960\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3699 - val_loss: 3.0138\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3802 - val_loss: 2.8588\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3784 - val_loss: 2.7696\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3576 - val_loss: 2.7384\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3539 - val_loss: 2.7465\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3553 - val_loss: 2.9217\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3441 - val_loss: 2.8851\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3383 - val_loss: 2.8084\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3485 - val_loss: 2.7457\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3378 - val_loss: 2.8917\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3359 - val_loss: 2.7810\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3047 - val_loss: 2.8394\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 2.3140 - val_loss: 2.7995\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3072 - val_loss: 2.8803\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3042 - val_loss: 2.7128\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3033 - val_loss: 2.8327\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2909 - val_loss: 2.9046\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3018 - val_loss: 2.9506\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2766 - val_loss: 3.0436\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2556 - val_loss: 2.8976\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2569 - val_loss: 2.9477\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2541 - val_loss: 3.0298\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2334 - val_loss: 2.9069\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2463 - val_loss: 2.8643\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2482 - val_loss: 2.8990\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2246 - val_loss: 2.7322\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2270 - val_loss: 2.9465\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2204 - val_loss: 2.9816\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2023 - val_loss: 3.1898\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2238 - val_loss: 3.0865\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1951 - val_loss: 2.9800\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1934 - val_loss: 2.7935\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2070 - val_loss: 2.9843\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1843 - val_loss: 3.0953\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1783 - val_loss: 2.9752\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1694 - val_loss: 3.1107\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1611 - val_loss: 2.7940\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1467 - val_loss: 2.9551\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1413 - val_loss: 2.7970\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1303 - val_loss: 2.9574\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1390 - val_loss: 2.9267\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1227 - val_loss: 2.7017\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1195 - val_loss: 2.8320\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1285 - val_loss: 2.9892\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1063 - val_loss: 2.7109\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1121 - val_loss: 3.0764\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1234 - val_loss: 2.8983\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0672 - val_loss: 2.9180\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1010 - val_loss: 2.8014\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0821 - val_loss: 2.9165\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0850 - val_loss: 2.8942\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 13.2661 - val_loss: 5.5255\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 4.2394 - val_loss: 3.6489\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 3.4854 - val_loss: 3.3419\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.3221 - val_loss: 3.1467\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2107 - val_loss: 3.1504\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0942 - val_loss: 2.9748\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.1081 - val_loss: 2.9615\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.0073 - val_loss: 3.2626\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.9930 - val_loss: 3.0423\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9828 - val_loss: 2.9587\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.9173 - val_loss: 3.6046\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9065 - val_loss: 3.3160\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8392 - val_loss: 3.0955\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8571 - val_loss: 2.8828\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8161 - val_loss: 2.8718\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.7885 - val_loss: 3.0474\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8025 - val_loss: 3.2281\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7415 - val_loss: 2.7528\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7620 - val_loss: 2.6481\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7000 - val_loss: 2.8711\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6945 - val_loss: 2.8541\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6823 - val_loss: 2.7794\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6825 - val_loss: 2.9318\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6516 - val_loss: 2.7445\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6340 - val_loss: 2.9748\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6195 - val_loss: 2.9104\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6140 - val_loss: 2.6833\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5921 - val_loss: 3.0851\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5585 - val_loss: 2.7731\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5976 - val_loss: 2.7247\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5804 - val_loss: 2.6834\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5766 - val_loss: 2.8568\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5439 - val_loss: 2.9405\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5190 - val_loss: 2.6833\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5077 - val_loss: 2.8555\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5204 - val_loss: 2.7845\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 2.4880 - val_loss: 2.8338\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4877 - val_loss: 2.8088\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5097 - val_loss: 3.0413\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4857 - val_loss: 2.6744\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4730 - val_loss: 2.9952\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4717 - val_loss: 2.7362\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4643 - val_loss: 2.8091\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4331 - val_loss: 2.6694\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4289 - val_loss: 2.7532\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4424 - val_loss: 2.7277\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4338 - val_loss: 2.7646\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4067 - val_loss: 2.7340\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4256 - val_loss: 2.8903\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3819 - val_loss: 2.9165\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3727 - val_loss: 2.7384\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4003 - val_loss: 2.7233\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3591 - val_loss: 2.8262\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3894 - val_loss: 2.7023\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3132 - val_loss: 2.9009\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3559 - val_loss: 2.7552\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3253 - val_loss: 2.7405\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3554 - val_loss: 2.7105\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2942 - val_loss: 2.6061\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2969 - val_loss: 2.7491\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3208 - val_loss: 2.9399\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2831 - val_loss: 2.6506\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3008 - val_loss: 2.8388\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2772 - val_loss: 2.7086\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3001 - val_loss: 2.6798\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2607 - val_loss: 2.7406\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2747 - val_loss: 2.7482\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2703 - val_loss: 2.7338\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2668 - val_loss: 2.8207\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2616 - val_loss: 2.6939\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2454 - val_loss: 2.7558\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2378 - val_loss: 2.9519\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2185 - val_loss: 2.7152\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2460 - val_loss: 2.8061\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2198 - val_loss: 2.8260\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1757 - val_loss: 2.8371\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2217 - val_loss: 2.8703\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1859 - val_loss: 2.6831\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1757 - val_loss: 2.8446\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1797 - val_loss: 2.8578\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1687 - val_loss: 2.9214\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1682 - val_loss: 2.9275\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1661 - val_loss: 2.7776\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1401 - val_loss: 2.7190\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1639 - val_loss: 2.9184\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1298 - val_loss: 2.8676\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1520 - val_loss: 3.0134\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1189 - val_loss: 2.8344\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1283 - val_loss: 2.8586\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0916 - val_loss: 2.9169\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0885 - val_loss: 2.8141\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1109 - val_loss: 2.8753\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0789 - val_loss: 2.9329\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1128 - val_loss: 2.9901\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0884 - val_loss: 2.9769\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0808 - val_loss: 2.7650\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0830 - val_loss: 2.9662\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0682 - val_loss: 2.7706\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0636 - val_loss: 3.0767\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0677 - val_loss: 2.8940\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.1388 - val_loss: 5.5617\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 4.1425 - val_loss: 3.6086\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 3.4948 - val_loss: 3.1096\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 3.3166 - val_loss: 3.1347\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1705 - val_loss: 2.9595\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1245 - val_loss: 3.0165\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.0407 - val_loss: 3.0510\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.9828 - val_loss: 3.0016\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9795 - val_loss: 2.7441\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.9199 - val_loss: 2.9884\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8933 - val_loss: 2.8822\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8466 - val_loss: 3.0546\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8233 - val_loss: 2.8724\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8396 - val_loss: 2.7083\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7971 - val_loss: 2.7709\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7734 - val_loss: 2.6908\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7224 - val_loss: 2.9656\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7253 - val_loss: 2.7806\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7087 - val_loss: 2.9756\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7056 - val_loss: 2.9231\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6534 - val_loss: 2.9982\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6504 - val_loss: 2.8382\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6498 - val_loss: 2.8887\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6133 - val_loss: 2.6319\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6078 - val_loss: 2.8928\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5819 - val_loss: 2.8792\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5879 - val_loss: 2.7592\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5515 - val_loss: 3.1718\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5698 - val_loss: 2.8466\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5361 - val_loss: 2.7149\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5590 - val_loss: 2.7558\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5533 - val_loss: 2.7815\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5252 - val_loss: 2.7230\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5116 - val_loss: 2.8415\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5206 - val_loss: 2.7021\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5170 - val_loss: 2.6067\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5210 - val_loss: 2.8957\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5040 - val_loss: 2.8948\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4576 - val_loss: 2.7145\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4848 - val_loss: 2.7439\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4551 - val_loss: 2.7684\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4604 - val_loss: 3.0508\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4281 - val_loss: 2.8308\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4105 - val_loss: 2.7005\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4519 - val_loss: 2.6831\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3921 - val_loss: 2.7057\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4167 - val_loss: 2.7722\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3754 - val_loss: 2.9412\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4151 - val_loss: 2.8006\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4091 - val_loss: 2.7350\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3600 - val_loss: 2.7985\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3682 - val_loss: 2.7517\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3559 - val_loss: 2.7454\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3665 - val_loss: 2.8080\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3487 - val_loss: 2.8319\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3394 - val_loss: 3.0693\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3151 - val_loss: 2.8755\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3169 - val_loss: 2.7411\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3161 - val_loss: 3.0303\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3083 - val_loss: 2.9402\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3003 - val_loss: 2.8400\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3022 - val_loss: 2.8908\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2903 - val_loss: 2.8229\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2997 - val_loss: 2.8512\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2637 - val_loss: 2.8997\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2975 - val_loss: 2.7718\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2598 - val_loss: 2.9698\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2539 - val_loss: 2.7080\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 2.2496 - val_loss: 2.8237\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2193 - val_loss: 2.7186\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2433 - val_loss: 2.8307\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2255 - val_loss: 2.7273\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2234 - val_loss: 2.7034\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1975 - val_loss: 2.7980\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2019 - val_loss: 2.8527\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1940 - val_loss: 2.8439\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1810 - val_loss: 2.7255\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2016 - val_loss: 2.8218\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1692 - val_loss: 2.8291\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1766 - val_loss: 2.7455\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1817 - val_loss: 2.7421\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1407 - val_loss: 2.6767\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1783 - val_loss: 2.7821\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1444 - val_loss: 2.7988\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1288 - val_loss: 3.0420\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1176 - val_loss: 2.6769\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1270 - val_loss: 2.7630\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1411 - val_loss: 2.9139\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0944 - val_loss: 2.7716\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0838 - val_loss: 3.0784\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0900 - val_loss: 2.7647\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0741 - val_loss: 2.7898\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0788 - val_loss: 2.8603\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0578 - val_loss: 2.8184\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0823 - val_loss: 2.8442\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0737 - val_loss: 2.9845\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0572 - val_loss: 2.8268\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0433 - val_loss: 2.7457\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0478 - val_loss: 2.7365\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0257 - val_loss: 2.7574\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.2026 - val_loss: 5.6099\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 4.3146 - val_loss: 3.5705\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.5073 - val_loss: 3.2136\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.2800 - val_loss: 3.2145\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1442 - val_loss: 3.2983\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.0987 - val_loss: 3.2855\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9787 - val_loss: 3.1331\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.9523 - val_loss: 3.3279\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8699 - val_loss: 3.0566\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.8815 - val_loss: 3.1448\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.8331 - val_loss: 3.0598\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.8312 - val_loss: 3.1229\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7454 - val_loss: 2.9062\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.7704 - val_loss: 2.9810\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7258 - val_loss: 2.9520\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7196 - val_loss: 3.1071\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7435 - val_loss: 3.1776\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6888 - val_loss: 2.8814\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6529 - val_loss: 3.0750\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6382 - val_loss: 3.3450\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6202 - val_loss: 2.9461\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.6121 - val_loss: 2.9771\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6201 - val_loss: 3.2144\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5631 - val_loss: 3.0492\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5618 - val_loss: 2.9560\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5602 - val_loss: 2.9413\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5157 - val_loss: 3.0984\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5229 - val_loss: 3.0232\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5334 - val_loss: 2.9731\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5098 - val_loss: 2.8029\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4790 - val_loss: 2.9794\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5208 - val_loss: 2.8192\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4946 - val_loss: 2.8698\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4718 - val_loss: 3.0458\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4272 - val_loss: 3.0947\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4584 - val_loss: 3.0531\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4707 - val_loss: 2.8471\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4555 - val_loss: 2.9541\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4317 - val_loss: 2.9190\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3988 - val_loss: 2.8617\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4133 - val_loss: 2.9291\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4314 - val_loss: 2.8662\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4070 - val_loss: 2.8649\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3858 - val_loss: 2.8731\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3987 - val_loss: 3.0286\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3621 - val_loss: 3.0493\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3668 - val_loss: 2.9263\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3760 - val_loss: 2.8428\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3626 - val_loss: 2.8599\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3392 - val_loss: 3.0706\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3140 - val_loss: 2.8538\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3002 - val_loss: 3.0547\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3133 - val_loss: 2.9584\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3194 - val_loss: 2.9586\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2883 - val_loss: 2.9528\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3046 - val_loss: 2.9479\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3086 - val_loss: 2.8326\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2969 - val_loss: 2.8417\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2576 - val_loss: 2.8296\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2879 - val_loss: 2.9016\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2700 - val_loss: 2.9800\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2805 - val_loss: 2.8993\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2417 - val_loss: 2.8497\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2311 - val_loss: 2.9535\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2369 - val_loss: 2.8630\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2535 - val_loss: 2.9351\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2450 - val_loss: 2.9611\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2315 - val_loss: 3.0217\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2111 - val_loss: 2.9589\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2364 - val_loss: 2.9693\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2203 - val_loss: 2.8702\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2175 - val_loss: 2.9872\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2045 - val_loss: 3.0401\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1883 - val_loss: 2.9933\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1809 - val_loss: 2.9039\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1785 - val_loss: 2.8965\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1382 - val_loss: 2.9632\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1546 - val_loss: 2.9878\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1706 - val_loss: 2.9017\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1519 - val_loss: 3.0318\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1517 - val_loss: 3.1003\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1400 - val_loss: 2.9292\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1358 - val_loss: 2.9543\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1040 - val_loss: 3.1474\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1259 - val_loss: 2.9675\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0991 - val_loss: 3.0032\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1117 - val_loss: 2.8532\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0930 - val_loss: 2.9267\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1167 - val_loss: 3.0339\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0998 - val_loss: 2.9629\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1049 - val_loss: 2.9519\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0942 - val_loss: 2.9272\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0968 - val_loss: 2.9101\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0944 - val_loss: 3.0347\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0807 - val_loss: 2.9516\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0737 - val_loss: 2.9470\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0564 - val_loss: 2.9148\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0858 - val_loss: 2.9253\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0542 - val_loss: 2.9958\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.0520 - val_loss: 2.9886\n",
      "Epoch 1/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 13.2478 - val_loss: 5.5914\n",
      "Epoch 2/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 4.0908 - val_loss: 4.3534\n",
      "Epoch 3/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.4801 - val_loss: 3.4388\n",
      "Epoch 4/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.2714 - val_loss: 3.3193\n",
      "Epoch 5/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 3.1599 - val_loss: 3.3229\n",
      "Epoch 6/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 3.0680 - val_loss: 3.0273\n",
      "Epoch 7/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 3.0247 - val_loss: 3.1544\n",
      "Epoch 8/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9728 - val_loss: 3.5559\n",
      "Epoch 9/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.9350 - val_loss: 2.9645\n",
      "Epoch 10/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.8951 - val_loss: 3.2627\n",
      "Epoch 11/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8875 - val_loss: 3.2288\n",
      "Epoch 12/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8579 - val_loss: 3.1163\n",
      "Epoch 13/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.7869 - val_loss: 3.4938\n",
      "Epoch 14/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.8144 - val_loss: 2.9987\n",
      "Epoch 15/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.7800 - val_loss: 2.8432\n",
      "Epoch 16/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7527 - val_loss: 2.9329\n",
      "Epoch 17/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.7582 - val_loss: 3.4243\n",
      "Epoch 18/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.7022 - val_loss: 2.8934\n",
      "Epoch 19/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6883 - val_loss: 2.8158\n",
      "Epoch 20/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6943 - val_loss: 2.9381\n",
      "Epoch 21/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6788 - val_loss: 2.9755\n",
      "Epoch 22/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6772 - val_loss: 2.8352\n",
      "Epoch 23/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6484 - val_loss: 2.9246\n",
      "Epoch 24/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6649 - val_loss: 2.8321\n",
      "Epoch 25/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.6295 - val_loss: 3.1018\n",
      "Epoch 26/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6231 - val_loss: 2.7965\n",
      "Epoch 27/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.5812 - val_loss: 2.8850\n",
      "Epoch 28/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5664 - val_loss: 2.9866\n",
      "Epoch 29/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.6027 - val_loss: 2.9419\n",
      "Epoch 30/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5519 - val_loss: 2.9162\n",
      "Epoch 31/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5506 - val_loss: 2.9510\n",
      "Epoch 32/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.5393 - val_loss: 2.9474\n",
      "Epoch 33/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5232 - val_loss: 2.9101\n",
      "Epoch 34/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.5132 - val_loss: 2.9901\n",
      "Epoch 35/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4875 - val_loss: 2.8666\n",
      "Epoch 36/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4721 - val_loss: 2.8640\n",
      "Epoch 37/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4666 - val_loss: 2.9773\n",
      "Epoch 38/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4738 - val_loss: 3.0584\n",
      "Epoch 39/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4787 - val_loss: 2.8011\n",
      "Epoch 40/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4544 - val_loss: 2.8624\n",
      "Epoch 41/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4657 - val_loss: 2.7758\n",
      "Epoch 42/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.4044 - val_loss: 3.0036\n",
      "Epoch 43/100\n",
      "1036/1036 [==============================] - 4s 4ms/step - loss: 2.4368 - val_loss: 3.0179\n",
      "Epoch 44/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.4272 - val_loss: 2.8451\n",
      "Epoch 45/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4224 - val_loss: 2.8315\n",
      "Epoch 46/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3825 - val_loss: 2.8566\n",
      "Epoch 47/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.4063 - val_loss: 2.8919\n",
      "Epoch 48/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3911 - val_loss: 2.9263\n",
      "Epoch 49/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3620 - val_loss: 2.8873\n",
      "Epoch 50/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3882 - val_loss: 2.9397\n",
      "Epoch 51/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.3487 - val_loss: 2.8300\n",
      "Epoch 52/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3570 - val_loss: 2.9705\n",
      "Epoch 53/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3388 - val_loss: 2.8247\n",
      "Epoch 54/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3560 - val_loss: 2.9373\n",
      "Epoch 55/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3479 - val_loss: 2.9789\n",
      "Epoch 56/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.3150 - val_loss: 3.2422\n",
      "Epoch 57/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2882 - val_loss: 2.8944\n",
      "Epoch 58/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.3190 - val_loss: 2.9786\n",
      "Epoch 59/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2871 - val_loss: 3.1027\n",
      "Epoch 60/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2840 - val_loss: 2.8890\n",
      "Epoch 61/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2819 - val_loss: 2.8720\n",
      "Epoch 62/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2827 - val_loss: 2.9702\n",
      "Epoch 63/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2887 - val_loss: 2.8623\n",
      "Epoch 64/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2778 - val_loss: 2.9193\n",
      "Epoch 65/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2459 - val_loss: 2.8197\n",
      "Epoch 66/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2315 - val_loss: 2.8311\n",
      "Epoch 67/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2084 - val_loss: 2.8349\n",
      "Epoch 68/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2502 - val_loss: 2.9169\n",
      "Epoch 69/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.2285 - val_loss: 2.9458\n",
      "Epoch 70/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.2341 - val_loss: 2.8353\n",
      "Epoch 71/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2206 - val_loss: 2.9226\n",
      "Epoch 72/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.2017 - val_loss: 3.1334\n",
      "Epoch 73/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1937 - val_loss: 3.0124\n",
      "Epoch 74/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1609 - val_loss: 2.9820\n",
      "Epoch 75/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1952 - val_loss: 2.9803\n",
      "Epoch 76/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1987 - val_loss: 2.9119\n",
      "Epoch 77/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1562 - val_loss: 2.8263\n",
      "Epoch 78/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1603 - val_loss: 2.7663\n",
      "Epoch 79/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1513 - val_loss: 2.8476\n",
      "Epoch 80/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1814 - val_loss: 2.8550\n",
      "Epoch 81/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1429 - val_loss: 2.9412\n",
      "Epoch 82/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1569 - val_loss: 2.9020\n",
      "Epoch 83/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1552 - val_loss: 2.8179\n",
      "Epoch 84/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1583 - val_loss: 2.9031\n",
      "Epoch 85/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1329 - val_loss: 2.9451\n",
      "Epoch 86/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1272 - val_loss: 3.0119\n",
      "Epoch 87/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.1255 - val_loss: 3.0193\n",
      "Epoch 88/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1383 - val_loss: 2.8099\n",
      "Epoch 89/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1171 - val_loss: 2.9341\n",
      "Epoch 90/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.1013 - val_loss: 2.8896\n",
      "Epoch 91/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0875 - val_loss: 2.7967\n",
      "Epoch 92/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0839 - val_loss: 2.9033\n",
      "Epoch 93/100\n",
      "1036/1036 [==============================] - 3s 2ms/step - loss: 2.1162 - val_loss: 2.8099\n",
      "Epoch 94/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0909 - val_loss: 3.0485\n",
      "Epoch 95/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0754 - val_loss: 2.8376\n",
      "Epoch 96/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0655 - val_loss: 2.8820\n",
      "Epoch 97/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0677 - val_loss: 2.9821\n",
      "Epoch 98/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0633 - val_loss: 2.9751\n",
      "Epoch 99/100\n",
      "1036/1036 [==============================] - 3s 3ms/step - loss: 2.0434 - val_loss: 2.8592\n",
      "Epoch 100/100\n",
      "1036/1036 [==============================] - 2s 2ms/step - loss: 2.0395 - val_loss: 2.9674\n"
     ]
    }
   ],
   "source": [
    "historyVal_OL = []\n",
    "historyTr_OL = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_OL()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=100, batch_size=1).history\n",
    "    historyVal_OL.append(history['val_loss'])\n",
    "    historyTr_OL.append(history['loss'])\n",
    "#model=create_model_OL()\n",
    "#model.add_loss(MEE_k)\n",
    "#model.fit(X_dev, y_dev, epochs=100, \n",
    "                    #  batch_size=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_OL_mean=np.mean(historyVal_OL, axis=0)\n",
    "historyTr_OL_mean=np.mean(historyTr_OL, axis=0)\n",
    "\n",
    "historyVal_OL_sd=np.std(historyVal_OL, axis=0)\n",
    "historyTr_OL_sd=np.std(historyTr_OL, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_OL_mean)),historyTr_OL_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_OL_mean)), historyTr_OL_mean - historyTr_OL_sd, \n",
    "                 historyTr_OL_mean + historyTr_OL_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_OL_mean)), historyVal_OL_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_OL_mean)), historyVal_OL_mean - historyVal_OL_sd, \n",
    "                 historyVal_OL_mean + historyVal_OL_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(5,14)\n",
    "plt.xlim(-5,100)\n",
    "plt.yticks(np.arange(0, 14, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnLine batch result:\n",
      "MEE on the validation 2.900324821472168 with standard deviation 0.08101321863665002\n",
      "MEE on the training 2.0539805412292482 with standard deviation 0.02080189661650085\n"
     ]
    }
   ],
   "source": [
    "print(\"OnLine batch result:\")\n",
    "print(\"MEE on the validation\",historyVal_OL_mean[-1],\"with standard deviation\",historyVal_OL_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_OL_mean[-1],\"with standard deviation\",historyTr_OL_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_LM():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(mlpr.best_params_['unit1'], input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(mlpr.best_params_['unit2'], activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 55.8411 - val_loss: 53.7189\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 54.2666 - val_loss: 52.0212\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 52.5744 - val_loss: 50.1200\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 50.6766 - val_loss: 47.9417\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 48.5002 - val_loss: 45.4395\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 46.0005 - val_loss: 42.6063\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 43.1723 - val_loss: 39.4774\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 40.0522 - val_loss: 36.1198\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 36.7101 - val_loss: 32.6472\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 33.2656 - val_loss: 29.2418\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 29.9104 - val_loss: 26.1296\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 26.8721 - val_loss: 23.5185\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 24.3379 - val_loss: 21.5125\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 22.4833 - val_loss: 20.1257\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 21.2460 - val_loss: 19.0310\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 20.2628 - val_loss: 18.1388\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 19.4485 - val_loss: 17.3982\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18.7576 - val_loss: 16.7730\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.1604 - val_loss: 16.2328\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 17.6310 - val_loss: 15.7478\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17.1423 - val_loss: 15.2838\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 16.6635 - val_loss: 14.7987\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 16.1559 - val_loss: 14.2366\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 15.5667 - val_loss: 13.5405\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.8410 - val_loss: 12.7241\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.9878 - val_loss: 11.8788\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 13.0874 - val_loss: 11.0457\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.1665 - val_loss: 10.2814\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.2906 - val_loss: 9.6498\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.5575 - val_loss: 9.1549\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.9768 - val_loss: 8.8041\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.5342 - val_loss: 8.5543\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.2075 - val_loss: 8.3884\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.9729 - val_loss: 8.2778\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.8087 - val_loss: 8.2063\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.6958 - val_loss: 8.1584\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.6185 - val_loss: 8.1267\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.5633 - val_loss: 8.1012\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.5198 - val_loss: 8.0798\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4815 - val_loss: 8.0586\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 8.4461 - val_loss: 8.0365\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 8.4122 - val_loss: 8.0123\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.3787 - val_loss: 7.9852\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.3448 - val_loss: 7.9555\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.3100 - val_loss: 7.9227\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.2738 - val_loss: 7.8867\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.2362 - val_loss: 7.8481\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1968 - val_loss: 7.8069\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.1559 - val_loss: 7.7635\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.1137 - val_loss: 7.7185\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.0703 - val_loss: 7.6724\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.0261 - val_loss: 7.6257\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.9812 - val_loss: 7.5787\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.9357 - val_loss: 7.5314\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.8896 - val_loss: 7.4840\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.8430 - val_loss: 7.4363\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7959 - val_loss: 7.3885\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.7484 - val_loss: 7.3404\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.7004 - val_loss: 7.2922\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.6521 - val_loss: 7.2439\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.6035 - val_loss: 7.1958\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.5544 - val_loss: 7.1462\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.5049 - val_loss: 7.0971\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.4547 - val_loss: 7.0460\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.4041 - val_loss: 6.9986\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.3530 - val_loss: 6.9409\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.3016 - val_loss: 6.8994\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.2497 - val_loss: 6.8308\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.1993 - val_loss: 6.8097\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7.1504 - val_loss: 6.7220\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.1058 - val_loss: 6.7468\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.0703 - val_loss: 6.6283\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.0346 - val_loss: 6.7097\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.0151 - val_loss: 6.5596\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.9910 - val_loss: 6.7070\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 6.9954 - val_loss: 6.5116\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.9602 - val_loss: 6.7060\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6.9817 - val_loss: 6.4463\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 6.9023 - val_loss: 6.6262\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 6.8960 - val_loss: 6.3220\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.7742 - val_loss: 6.4708\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.7390 - val_loss: 6.1686\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.6111 - val_loss: 6.2837\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.5537 - val_loss: 6.0126\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.4418 - val_loss: 6.0993\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.3731 - val_loss: 5.8604\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.2745 - val_loss: 5.9249\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.2022 - val_loss: 5.7194\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.1214 - val_loss: 5.7727\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.0504 - val_loss: 5.5867\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.9781 - val_loss: 5.6325\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.9082 - val_loss: 5.4601\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.8416 - val_loss: 5.5023\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.7735 - val_loss: 5.3383\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.7095 - val_loss: 5.3786\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.6439 - val_loss: 5.2189\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.5777 - val_loss: 5.2559\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.5151 - val_loss: 5.1014\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 5.4451 - val_loss: 5.1301\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.3843 - val_loss: 4.9876\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.3162 - val_loss: 5.0025\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.2532 - val_loss: 4.8757\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.1894 - val_loss: 4.8839\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.1275 - val_loss: 4.7688\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.0680 - val_loss: 4.7778\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.0104 - val_loss: 4.6698\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.9544 - val_loss: 4.6823\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.9014 - val_loss: 4.5795\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.8488 - val_loss: 4.5974\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.8009 - val_loss: 4.4978\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.7522 - val_loss: 4.5194\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.7069 - val_loss: 4.4210\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.6614 - val_loss: 4.4534\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.6219 - val_loss: 4.3538\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.5797 - val_loss: 4.3931\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.5425 - val_loss: 4.2927\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.5021 - val_loss: 4.3361\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.4669 - val_loss: 4.2353\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.4280 - val_loss: 4.2818\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.3947 - val_loss: 4.1811\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3566 - val_loss: 4.2307\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.3262 - val_loss: 4.1306\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.2887 - val_loss: 4.1869\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.2631 - val_loss: 4.0868\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4.2280 - val_loss: 4.1403\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.2007 - val_loss: 4.0433\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 4.1679 - val_loss: 4.0940\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.1403 - val_loss: 4.0030\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.1126 - val_loss: 4.0542\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.0870 - val_loss: 3.9689\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.0645 - val_loss: 4.0209\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.0395 - val_loss: 3.9356\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.0184 - val_loss: 3.9892\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.9945 - val_loss: 3.9065\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.9772 - val_loss: 3.9617\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.9544 - val_loss: 3.8819\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.9424 - val_loss: 3.9399\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.9211 - val_loss: 3.8617\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.9133 - val_loss: 3.9228\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.8921 - val_loss: 3.8424\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.8867 - val_loss: 3.9062\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.8637 - val_loss: 3.8222\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.8599 - val_loss: 3.8857\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.8321 - val_loss: 3.7978\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.8296 - val_loss: 3.8615\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.7992 - val_loss: 3.7712\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.7974 - val_loss: 3.8357\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.7664 - val_loss: 3.7394\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7589 - val_loss: 3.8013\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.7279 - val_loss: 3.7068\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.7188 - val_loss: 3.7698\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.6919 - val_loss: 3.6779\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.6829 - val_loss: 3.7422\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.6597 - val_loss: 3.6521\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.6505 - val_loss: 3.7164\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.6294 - val_loss: 3.6276\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.6197 - val_loss: 3.6913\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.6003 - val_loss: 3.6037\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.5899 - val_loss: 3.6665\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.5721 - val_loss: 3.5800\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.5608 - val_loss: 3.6418\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5446 - val_loss: 3.5574\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5332 - val_loss: 3.6182\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5185 - val_loss: 3.5366\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.5077 - val_loss: 3.5963\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.4939 - val_loss: 3.5169\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.4835 - val_loss: 3.5749\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4698 - val_loss: 3.4979\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.4600 - val_loss: 3.5521\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4455 - val_loss: 3.4795\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.4370 - val_loss: 3.5319\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4233 - val_loss: 3.4623\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.4149 - val_loss: 3.5136\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4025 - val_loss: 3.4472\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3945 - val_loss: 3.4975\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.3835 - val_loss: 3.4336\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.3757 - val_loss: 3.4833\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3660 - val_loss: 3.4189\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.3562 - val_loss: 3.4689\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3486 - val_loss: 3.4073\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.3393 - val_loss: 3.4534\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3311 - val_loss: 3.3965\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3231 - val_loss: 3.4407\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.3154 - val_loss: 3.3877\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.3088 - val_loss: 3.4307\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3018 - val_loss: 3.3808\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2963 - val_loss: 3.4229\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.2899 - val_loss: 3.3751\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.2851 - val_loss: 3.4163\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2792 - val_loss: 3.3698\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2745 - val_loss: 3.4097\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.2689 - val_loss: 3.3640\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2635 - val_loss: 3.4025\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2583 - val_loss: 3.3579\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.2525 - val_loss: 3.3950\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.2477 - val_loss: 3.3522\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2418 - val_loss: 3.3878\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2376 - val_loss: 3.3467\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.2316 - val_loss: 3.3809\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2278 - val_loss: 3.3415\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2221 - val_loss: 3.3742\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2183 - val_loss: 3.3354\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.2121 - val_loss: 3.3669\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.2085 - val_loss: 3.3301\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.2028 - val_loss: 3.3592\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.1987 - val_loss: 3.3246\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1935 - val_loss: 3.3523\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.1898 - val_loss: 3.3193\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1847 - val_loss: 3.3466\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1816 - val_loss: 3.3141\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1763 - val_loss: 3.3409\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1736 - val_loss: 3.3091\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1683 - val_loss: 3.3354\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1659 - val_loss: 3.3044\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1609 - val_loss: 3.3304\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1588 - val_loss: 3.3001\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1540 - val_loss: 3.3258\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1522 - val_loss: 3.2961\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1477 - val_loss: 3.3216\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1461 - val_loss: 3.2924\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1418 - val_loss: 3.3180\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1405 - val_loss: 3.2883\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1360 - val_loss: 3.3129\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1322 - val_loss: 3.2824\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1284 - val_loss: 3.3073\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1254 - val_loss: 3.2779\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1223 - val_loss: 3.3031\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1200 - val_loss: 3.2742\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1172 - val_loss: 3.2995\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1153 - val_loss: 3.2707\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1126 - val_loss: 3.2962\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1108 - val_loss: 3.2670\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1080 - val_loss: 3.2927\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1061 - val_loss: 3.2630\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1033 - val_loss: 3.2889\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.1011 - val_loss: 3.2583\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0981 - val_loss: 3.2845\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0957 - val_loss: 3.2523\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0918 - val_loss: 3.2790\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0894 - val_loss: 3.2462\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0853 - val_loss: 3.2732\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0828 - val_loss: 3.2401\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0788 - val_loss: 3.2671\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0760 - val_loss: 3.2338\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0723 - val_loss: 3.2614\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0695 - val_loss: 3.2280\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0664 - val_loss: 3.2566\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0642 - val_loss: 3.2230\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0615 - val_loss: 3.2527\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0598 - val_loss: 3.2182\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0569 - val_loss: 3.2492\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0557 - val_loss: 3.2134\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0524 - val_loss: 3.2458\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0517 - val_loss: 3.2087\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0481 - val_loss: 3.2424\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0478 - val_loss: 3.2041\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0439 - val_loss: 3.2390\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0438 - val_loss: 3.1995\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0397 - val_loss: 3.2340\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0385 - val_loss: 3.1940\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0343 - val_loss: 3.2276\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0318 - val_loss: 3.1879\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0281 - val_loss: 3.2213\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0253 - val_loss: 3.1822\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0221 - val_loss: 3.2160\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0197 - val_loss: 3.1771\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0168 - val_loss: 3.2111\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0147 - val_loss: 3.1722\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0117 - val_loss: 3.2061\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0094 - val_loss: 3.1671\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.0060 - val_loss: 3.1999\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0032 - val_loss: 3.1611\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9992 - val_loss: 3.1930\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9963 - val_loss: 3.1554\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9929 - val_loss: 3.1872\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9903 - val_loss: 3.1505\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9876 - val_loss: 3.1825\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9853 - val_loss: 3.1462\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9831 - val_loss: 3.1787\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9810 - val_loss: 3.1423\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9791 - val_loss: 3.1753\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9771 - val_loss: 3.1387\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9755 - val_loss: 3.1723\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9735 - val_loss: 3.1352\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9721 - val_loss: 3.1695\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9700 - val_loss: 3.1318\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9688 - val_loss: 3.1668\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.9667 - val_loss: 3.1285\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9656 - val_loss: 3.1641\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9634 - val_loss: 3.1254\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9625 - val_loss: 3.1616\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9602 - val_loss: 3.1223\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9595 - val_loss: 3.1592\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9570 - val_loss: 3.1193\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9565 - val_loss: 3.1569\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9540 - val_loss: 3.1164\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9537 - val_loss: 3.1547\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9510 - val_loss: 3.1136\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9510 - val_loss: 3.1526\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9481 - val_loss: 3.1110\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9484 - val_loss: 3.1506\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9454 - val_loss: 3.1085\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9460 - val_loss: 3.1489\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9428 - val_loss: 3.1063\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9439 - val_loss: 3.1473\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9404 - val_loss: 3.1043\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9421 - val_loss: 3.1461\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9382 - val_loss: 3.1028\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9407 - val_loss: 3.1468\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9382 - val_loss: 3.1050\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9427 - val_loss: 3.1491\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9392 - val_loss: 3.1075\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9452 - val_loss: 3.1514\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9404 - val_loss: 3.1098\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9474 - val_loss: 3.1535\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9414 - val_loss: 3.1110\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9487 - val_loss: 3.1549\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9417 - val_loss: 3.1111\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9489 - val_loss: 3.1555\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9411 - val_loss: 3.1103\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9481 - val_loss: 3.1552\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9398 - val_loss: 3.1089\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9465 - val_loss: 3.1543\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9381 - val_loss: 3.1071\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9445 - val_loss: 3.1532\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9361 - val_loss: 3.1053\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9424 - val_loss: 3.1522\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.9342 - val_loss: 3.1038\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9405 - val_loss: 3.1509\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9322 - val_loss: 3.1022\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9385 - val_loss: 3.1493\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9298 - val_loss: 3.1003\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9362 - val_loss: 3.1473\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9272 - val_loss: 3.0980\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9336 - val_loss: 3.1451\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9242 - val_loss: 3.0955\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9307 - val_loss: 3.1426\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9211 - val_loss: 3.0928\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9276 - val_loss: 3.1400\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9178 - val_loss: 3.0899\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9243 - val_loss: 3.1371\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9144 - val_loss: 3.0869\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9208 - val_loss: 3.1341\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9108 - val_loss: 3.0837\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9172 - val_loss: 3.1309\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9070 - val_loss: 3.0803\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9133 - val_loss: 3.1276\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9031 - val_loss: 3.0767\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9091 - val_loss: 3.1241\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8990 - val_loss: 3.0729\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9046 - val_loss: 3.1206\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8948 - val_loss: 3.0689\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9001 - val_loss: 3.1172\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8906 - val_loss: 3.0645\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8952 - val_loss: 3.1136\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8862 - val_loss: 3.0591\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8893 - val_loss: 3.1092\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8809 - val_loss: 3.0530\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8825 - val_loss: 3.1041\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8752 - val_loss: 3.0470\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8757 - val_loss: 3.0990\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8696 - val_loss: 3.0420\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8699 - val_loss: 3.0949\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 2.8648 - val_loss: 3.0383\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8653 - val_loss: 3.0918\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8610 - val_loss: 3.0354\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8617 - val_loss: 3.0896\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8579 - val_loss: 3.0332\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8587 - val_loss: 3.0880\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8553 - val_loss: 3.0314\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8562 - val_loss: 3.0868\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8530 - val_loss: 3.0300\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8541 - val_loss: 3.0859\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8510 - val_loss: 3.0289\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8522 - val_loss: 3.0852\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8492 - val_loss: 3.0279\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.8505 - val_loss: 3.0846\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8475 - val_loss: 3.0270\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8489 - val_loss: 3.0839\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8458 - val_loss: 3.0262\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8473 - val_loss: 3.0830\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8440 - val_loss: 3.0253\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8456 - val_loss: 3.0821\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8422 - val_loss: 3.0245\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8440 - val_loss: 3.0814\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8406 - val_loss: 3.0238\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8425 - val_loss: 3.0808\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8391 - val_loss: 3.0233\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8412 - val_loss: 3.0805\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8378 - val_loss: 3.0231\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8401 - val_loss: 3.0807\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8368 - val_loss: 3.0232\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8394 - val_loss: 3.0812\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8360 - val_loss: 3.0238\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8390 - val_loss: 3.0818\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8352 - val_loss: 3.0244\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8387 - val_loss: 3.0823\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8343 - val_loss: 3.0251\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8386 - val_loss: 3.0828\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8335 - val_loss: 3.0257\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8383 - val_loss: 3.0832\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8326 - val_loss: 3.0261\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8379 - val_loss: 3.0836\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8317 - val_loss: 3.0264\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8376 - val_loss: 3.0842\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8308 - val_loss: 3.0268\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8374 - val_loss: 3.0850\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.8302 - val_loss: 3.0274\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8376 - val_loss: 3.0861\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8299 - val_loss: 3.0285\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8382 - val_loss: 3.0876\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8299 - val_loss: 3.0303\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8394 - val_loss: 3.0899\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8306 - val_loss: 3.0330\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8416 - val_loss: 3.0938\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8325 - val_loss: 3.0384\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8467 - val_loss: 3.1013\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8378 - val_loss: 3.0479\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8553 - val_loss: 3.1096\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.8444 - val_loss: 3.0584\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8643 - val_loss: 3.1166\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8499 - val_loss: 3.0705\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8762 - val_loss: 3.1262\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8584 - val_loss: 3.0838\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.8887 - val_loss: 3.1358\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8672 - val_loss: 3.0910\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8946 - val_loss: 3.1396\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8702 - val_loss: 3.0928\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8950 - val_loss: 3.1392\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8691 - val_loss: 3.0914\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8925 - val_loss: 3.1365\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8658 - val_loss: 3.0882\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8881 - val_loss: 3.1324\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8612 - val_loss: 3.0836\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8822 - val_loss: 3.1273\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8557 - val_loss: 3.0781\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8757 - val_loss: 3.1217\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8495 - val_loss: 3.0717\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8683 - val_loss: 3.1148\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8420 - val_loss: 3.0626\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8580 - val_loss: 3.1071\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8340 - val_loss: 3.0535\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8485 - val_loss: 3.1002\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8269 - val_loss: 3.0456\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8400 - val_loss: 3.0923\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8193 - val_loss: 3.0359\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8299 - val_loss: 3.0823\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8103 - val_loss: 3.0260\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8197 - val_loss: 3.0740\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8024 - val_loss: 3.0171\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8099 - val_loss: 3.0665\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7952 - val_loss: 3.0064\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7979 - val_loss: 3.0551\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7846 - val_loss: 2.9955\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7851 - val_loss: 3.0456\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7762 - val_loss: 2.9887\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7763 - val_loss: 3.0389\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7701 - val_loss: 2.9844\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7705 - val_loss: 3.0318\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7637 - val_loss: 2.9800\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7645 - val_loss: 3.0264\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7589 - val_loss: 2.9771\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7601 - val_loss: 3.0219\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7548 - val_loss: 2.9747\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7564 - val_loss: 3.0185\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7515 - val_loss: 2.9730\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7534 - val_loss: 3.0164\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7490 - val_loss: 2.9718\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7512 - val_loss: 3.0154\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7472 - val_loss: 2.9713\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7498 - val_loss: 3.0152\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7459 - val_loss: 2.9714\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7490 - val_loss: 3.0157\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7452 - val_loss: 2.9719\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7488 - val_loss: 3.0170\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7450 - val_loss: 2.9730\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7492 - val_loss: 3.0189\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7453 - val_loss: 2.9745\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7502 - val_loss: 3.0219\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7462 - val_loss: 2.9765\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7519 - val_loss: 3.0259\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7480 - val_loss: 2.9790\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7542 - val_loss: 3.0313\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7507 - val_loss: 2.9822\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7574 - val_loss: 3.0372\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7542 - val_loss: 2.9862\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7627 - val_loss: 3.0455\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7601 - val_loss: 2.9939\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7721 - val_loss: 3.0539\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7665 - val_loss: 3.0024\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7815 - val_loss: 3.0616\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7724 - val_loss: 3.0132\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7915 - val_loss: 3.0697\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7789 - val_loss: 3.0217\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7990 - val_loss: 3.0760\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7836 - val_loss: 3.0296\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8067 - val_loss: 3.0838\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7897 - val_loss: 3.0381\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8150 - val_loss: 3.0898\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7946 - val_loss: 3.0424\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8179 - val_loss: 3.0913\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7953 - val_loss: 3.0435\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8176 - val_loss: 3.0906\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7940 - val_loss: 3.0421\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8147 - val_loss: 3.0881\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7910 - val_loss: 3.0384\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8096 - val_loss: 3.0839\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7864 - val_loss: 3.0340\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8045 - val_loss: 3.0794\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7818 - val_loss: 3.0298\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7996 - val_loss: 3.0751\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7774 - val_loss: 3.0261\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7952 - val_loss: 3.0714\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7735 - val_loss: 3.0228\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7910 - val_loss: 3.0680\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7698 - val_loss: 3.0193\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7863 - val_loss: 3.0644\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7660 - val_loss: 3.0153\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7811 - val_loss: 3.0609\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7621 - val_loss: 3.0120\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7768 - val_loss: 3.0584\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7591 - val_loss: 3.0097\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7736 - val_loss: 3.0567\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7567 - val_loss: 3.0080\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7712 - val_loss: 3.0554\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7548 - val_loss: 3.0065\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7689 - val_loss: 3.0543\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7530 - val_loss: 3.0031\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7649 - val_loss: 3.0520\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7502 - val_loss: 3.0013\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7622 - val_loss: 3.0505\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7481 - val_loss: 3.0002\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7603 - val_loss: 3.0495\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7465 - val_loss: 2.9997\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7590 - val_loss: 3.0490\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7452 - val_loss: 2.9996\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7582 - val_loss: 3.0488\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7444 - val_loss: 3.0002\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7582 - val_loss: 3.0491\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7440 - val_loss: 3.0012\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7588 - val_loss: 3.0497\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7439 - val_loss: 3.0023\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7593 - val_loss: 3.0502\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7437 - val_loss: 3.0031\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7593 - val_loss: 3.0505\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7433 - val_loss: 3.0036\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7589 - val_loss: 3.0505\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7425 - val_loss: 3.0037\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7581 - val_loss: 3.0502\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7416 - val_loss: 3.0035\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7571 - val_loss: 3.0498\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7404 - val_loss: 3.0032\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7559 - val_loss: 3.0492\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7392 - val_loss: 3.0028\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7546 - val_loss: 3.0486\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7379 - val_loss: 3.0023\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7532 - val_loss: 3.0479\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7365 - val_loss: 3.0017\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7518 - val_loss: 3.0472\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7351 - val_loss: 3.0012\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7504 - val_loss: 3.0465\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7338 - val_loss: 3.0007\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7491 - val_loss: 3.0458\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7324 - val_loss: 3.0001\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7477 - val_loss: 3.0451\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7311 - val_loss: 2.9996\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7464 - val_loss: 3.0444\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7298 - val_loss: 2.9991\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7451 - val_loss: 3.0437\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7285 - val_loss: 2.9986\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7438 - val_loss: 3.0431\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7272 - val_loss: 2.9981\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7424 - val_loss: 3.0423\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7259 - val_loss: 2.9975\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7410 - val_loss: 3.0415\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7245 - val_loss: 2.9965\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7393 - val_loss: 3.0403\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7228 - val_loss: 2.9947\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7366 - val_loss: 3.0373\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7199 - val_loss: 2.9911\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7317 - val_loss: 3.0323\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7151 - val_loss: 2.9855\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7241 - val_loss: 3.0281\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7104 - val_loss: 2.9807\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7178 - val_loss: 3.0228\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7054 - val_loss: 2.9763\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7125 - val_loss: 3.0190\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7017 - val_loss: 2.9733\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7087 - val_loss: 3.0168\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6991 - val_loss: 2.9717\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7063 - val_loss: 3.0157\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6975 - val_loss: 2.9708\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7048 - val_loss: 3.0151\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6963 - val_loss: 2.9705\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7039 - val_loss: 3.0148\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6955 - val_loss: 2.9704\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7034 - val_loss: 3.0148\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6949 - val_loss: 2.9706\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7032 - val_loss: 3.0150\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6945 - val_loss: 2.9710\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 56.8938 - val_loss: 55.1255\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 55.3083 - val_loss: 53.4485\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 53.6320 - val_loss: 51.5818\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 51.7660 - val_loss: 49.4539\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 49.6392 - val_loss: 47.0552\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 47.2421 - val_loss: 44.4109\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 44.5998 - val_loss: 41.5558\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 41.7474 - val_loss: 38.5388\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 38.7344 - val_loss: 35.4399\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 35.6421 - val_loss: 32.3585\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 32.5717 - val_loss: 29.3989\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 29.6268 - val_loss: 26.6823\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 26.9213 - val_loss: 24.3548\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 24.5686 - val_loss: 22.5650\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 22.6648 - val_loss: 21.3044\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 21.3224 - val_loss: 20.3603\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 20.3027 - val_loss: 19.5859\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 19.4708 - val_loss: 18.9312\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 18.7819 - val_loss: 18.3711\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 18.2094 - val_loss: 17.8854\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.7318 - val_loss: 17.4580\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.3275 - val_loss: 17.0723\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 16.9727 - val_loss: 16.7097\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 16.6421 - val_loss: 16.3475\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 16.3074 - val_loss: 15.9527\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.9303 - val_loss: 15.4748\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 15.4545 - val_loss: 14.8640\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 14.8250 - val_loss: 14.1558\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 14.0912 - val_loss: 13.4360\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 13.3697 - val_loss: 12.6920\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 12.6535 - val_loss: 11.8804\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.8986 - val_loss: 11.0270\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.1281 - val_loss: 10.2548\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 10.4351 - val_loss: 9.6543\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 9.8765 - val_loss: 9.2127\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 9.4510 - val_loss: 8.8904\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 9.1363 - val_loss: 8.6588\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 8.9069 - val_loss: 8.4950\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 8.7412 - val_loss: 8.3774\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 8.6198 - val_loss: 8.2896\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 8.5284 - val_loss: 8.2228\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 8.4568 - val_loss: 8.1667\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 8.3972 - val_loss: 8.1167\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.3437 - val_loss: 8.0674\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.2932 - val_loss: 8.0208\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.2434 - val_loss: 7.9720\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 8.1938 - val_loss: 7.9240\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 8.1431 - val_loss: 7.8735\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 8.0910 - val_loss: 7.8232\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 8.0378 - val_loss: 7.7715\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.9833 - val_loss: 7.7199\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.9282 - val_loss: 7.6677\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.8728 - val_loss: 7.6167\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7.8174 - val_loss: 7.5653\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.7622 - val_loss: 7.5154\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.7070 - val_loss: 7.4646\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.6520 - val_loss: 7.4155\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.5971 - val_loss: 7.3646\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.5422 - val_loss: 7.3160\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.4870 - val_loss: 7.2640\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.4317 - val_loss: 7.2158\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 7.3760 - val_loss: 7.1616\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 7.3198 - val_loss: 7.1147\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.2633 - val_loss: 7.0567\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.2063 - val_loss: 7.0131\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.1491 - val_loss: 6.9501\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.0915 - val_loss: 6.9113\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.0341 - val_loss: 6.8421\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 6.9759 - val_loss: 6.8089\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.9183 - val_loss: 6.7325\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 6.8591 - val_loss: 6.7065\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.8020 - val_loss: 6.6211\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 6.7416 - val_loss: 6.6029\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 6.6842 - val_loss: 6.5076\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 6.6227 - val_loss: 6.4938\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 6.5620 - val_loss: 6.3903\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 6.4993 - val_loss: 6.3782\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 6.4347 - val_loss: 6.2687\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.3712 - val_loss: 6.2573\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.3034 - val_loss: 6.1437\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 6.2400 - val_loss: 6.1327\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.1698 - val_loss: 6.0159\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 6.1070 - val_loss: 6.0054\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.0349 - val_loss: 5.8864\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.9734 - val_loss: 5.8766\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.8998 - val_loss: 5.7566\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.8411 - val_loss: 5.7494\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.7670 - val_loss: 5.6313\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.7155 - val_loss: 5.6255\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 5.6384 - val_loss: 5.5084\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.5912 - val_loss: 5.5031\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.5114 - val_loss: 5.3886\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.4684 - val_loss: 5.3827\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.3862 - val_loss: 5.2735\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 5.3500 - val_loss: 5.2683\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 5.2676 - val_loss: 5.1647\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 5.2368 - val_loss: 5.1632\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 5.1587 - val_loss: 5.0610\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.1277 - val_loss: 5.0608\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.0521 - val_loss: 4.9608\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 5.0213 - val_loss: 4.9625\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.9487 - val_loss: 4.8660\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.9198 - val_loss: 4.8647\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.8452 - val_loss: 4.7726\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 4.8174 - val_loss: 4.7730\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.7472 - val_loss: 4.6853\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 4.7214 - val_loss: 4.6902\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.6582 - val_loss: 4.6053\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.6339 - val_loss: 4.6166\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.5798 - val_loss: 4.5335\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.5551 - val_loss: 4.5487\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.5081 - val_loss: 4.4672\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 4.4820 - val_loss: 4.4844\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.4401 - val_loss: 4.4056\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 4.4129 - val_loss: 4.4204\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.3714 - val_loss: 4.3452\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.3433 - val_loss: 4.3577\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 4.3037 - val_loss: 4.2885\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.2771 - val_loss: 4.3038\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.2443 - val_loss: 4.2396\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 4.2199 - val_loss: 4.2567\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.1912 - val_loss: 4.1973\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 4.1697 - val_loss: 4.2169\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 4.1448 - val_loss: 4.1607\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4.1259 - val_loss: 4.1819\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.1026 - val_loss: 4.1272\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 4.0842 - val_loss: 4.1498\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 4.0631 - val_loss: 4.0972\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 4.0450 - val_loss: 4.1204\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 4.0271 - val_loss: 4.0714\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 4.0103 - val_loss: 4.0936\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.9948 - val_loss: 4.0482\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 3.9800 - val_loss: 4.0691\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.9665 - val_loss: 4.0259\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 3.9513 - val_loss: 4.0464\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.9412 - val_loss: 4.0053\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 3.9256 - val_loss: 4.0254\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 3.9190 - val_loss: 3.9860\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.9018 - val_loss: 4.0052\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.8982 - val_loss: 3.9677\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.8791 - val_loss: 3.9852\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.8763 - val_loss: 3.9491\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.8563 - val_loss: 3.9645\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.8523 - val_loss: 3.9294\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8325 - val_loss: 3.9435\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.8284 - val_loss: 3.9096\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.8083 - val_loss: 3.9224\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.8045 - val_loss: 3.8890\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.7829 - val_loss: 3.9005\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.7798 - val_loss: 3.8714\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.7602 - val_loss: 3.8800\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.7572 - val_loss: 3.8551\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.7389 - val_loss: 3.8599\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.7355 - val_loss: 3.8399\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7189 - val_loss: 3.8405\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.7150 - val_loss: 3.8234\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.6973 - val_loss: 3.8206\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.6942 - val_loss: 3.8054\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.6740 - val_loss: 3.8004\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.6732 - val_loss: 3.7873\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.6508 - val_loss: 3.7803\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.6523 - val_loss: 3.7692\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.6281 - val_loss: 3.7600\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.6312 - val_loss: 3.7508\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6057 - val_loss: 3.7382\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.6081 - val_loss: 3.7318\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.5831 - val_loss: 3.7157\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.5841 - val_loss: 3.7132\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5615 - val_loss: 3.6944\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.5616 - val_loss: 3.6955\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5417 - val_loss: 3.6741\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.5407 - val_loss: 3.6783\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.5229 - val_loss: 3.6533\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5189 - val_loss: 3.6601\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5034 - val_loss: 3.6328\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4976 - val_loss: 3.6420\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4842 - val_loss: 3.6125\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.4760 - val_loss: 3.6237\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.4650 - val_loss: 3.5911\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.4526 - val_loss: 3.6044\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.4446 - val_loss: 3.5708\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.4302 - val_loss: 3.5851\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.4244 - val_loss: 3.5516\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.4092 - val_loss: 3.5649\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.4025 - val_loss: 3.5317\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3870 - val_loss: 3.5459\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3823 - val_loss: 3.5134\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.3668 - val_loss: 3.5282\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.3640 - val_loss: 3.4963\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3480 - val_loss: 3.5116\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3465 - val_loss: 3.4796\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.3295 - val_loss: 3.4958\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.3292 - val_loss: 3.4629\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3107 - val_loss: 3.4764\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3077 - val_loss: 3.4455\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2910 - val_loss: 3.4602\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.2892 - val_loss: 3.4306\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.2744 - val_loss: 3.4465\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2737 - val_loss: 3.4177\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2602 - val_loss: 3.4345\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.2603 - val_loss: 3.4061\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2476 - val_loss: 3.4234\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.2474 - val_loss: 3.3953\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2355 - val_loss: 3.4136\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.2365 - val_loss: 3.3851\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.2237 - val_loss: 3.4039\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.2257 - val_loss: 3.3754\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2126 - val_loss: 3.3944\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2148 - val_loss: 3.3659\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.2018 - val_loss: 3.3849\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.2039 - val_loss: 3.3565\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1907 - val_loss: 3.3755\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1925 - val_loss: 3.3466\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1787 - val_loss: 3.3658\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.1808 - val_loss: 3.3369\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1669 - val_loss: 3.3570\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1696 - val_loss: 3.3280\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1562 - val_loss: 3.3492\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.1596 - val_loss: 3.3200\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1470 - val_loss: 3.3426\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1512 - val_loss: 3.3130\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1391 - val_loss: 3.3372\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1442 - val_loss: 3.3069\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1326 - val_loss: 3.3326\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1383 - val_loss: 3.3015\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.1272 - val_loss: 3.3287\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1332 - val_loss: 3.2967\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.1227 - val_loss: 3.3259\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1284 - val_loss: 3.2924\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.1185 - val_loss: 3.3230\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1240 - val_loss: 3.2882\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1147 - val_loss: 3.3207\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1204 - val_loss: 3.2845\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1115 - val_loss: 3.3187\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1173 - val_loss: 3.2810\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1084 - val_loss: 3.3165\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1139 - val_loss: 3.2773\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1049 - val_loss: 3.3139\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1100 - val_loss: 3.2734\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1009 - val_loss: 3.3110\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1055 - val_loss: 3.2694\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0966 - val_loss: 3.3085\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1016 - val_loss: 3.2658\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0927 - val_loss: 3.3055\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0972 - val_loss: 3.2620\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0884 - val_loss: 3.3021\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0921 - val_loss: 3.2580\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0835 - val_loss: 3.2984\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0865 - val_loss: 3.2540\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0783 - val_loss: 3.2945\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0807 - val_loss: 3.2498\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0727 - val_loss: 3.2904\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0744 - val_loss: 3.2453\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0665 - val_loss: 3.2859\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0677 - val_loss: 3.2402\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0592 - val_loss: 3.2807\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0601 - val_loss: 3.2328\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0482 - val_loss: 3.2726\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0494 - val_loss: 3.2260\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0379 - val_loss: 3.2652\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0396 - val_loss: 3.2204\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0295 - val_loss: 3.2597\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0318 - val_loss: 3.2159\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0226 - val_loss: 3.2554\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0256 - val_loss: 3.2123\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0169 - val_loss: 3.2522\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0206 - val_loss: 3.2092\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.0121 - val_loss: 3.2497\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0164 - val_loss: 3.2065\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0079 - val_loss: 3.2478\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0129 - val_loss: 3.2039\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0038 - val_loss: 3.2462\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0098 - val_loss: 3.2017\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0004 - val_loss: 3.2448\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0071 - val_loss: 3.2001\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9977 - val_loss: 3.2440\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0051 - val_loss: 3.1992\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9962 - val_loss: 3.2446\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0047 - val_loss: 3.2000\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9972 - val_loss: 3.2469\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0060 - val_loss: 3.2016\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9993 - val_loss: 3.2492\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0075 - val_loss: 3.2024\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9998 - val_loss: 3.2500\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.0072 - val_loss: 3.2022\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9987 - val_loss: 3.2508\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0068 - val_loss: 3.2020\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9977 - val_loss: 3.2505\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0051 - val_loss: 3.2013\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9955 - val_loss: 3.2494\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0024 - val_loss: 3.2002\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9927 - val_loss: 3.2477\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9991 - val_loss: 3.1988\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9894 - val_loss: 3.2457\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9953 - val_loss: 3.1972\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9858 - val_loss: 3.2435\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9914 - val_loss: 3.1955\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 2.9821 - val_loss: 3.2412\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9874 - val_loss: 3.1917\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9751 - val_loss: 3.2363\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9804 - val_loss: 3.1883\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9686 - val_loss: 3.2319\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9739 - val_loss: 3.1846\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9608 - val_loss: 3.2271\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9666 - val_loss: 3.1814\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9542 - val_loss: 3.2230\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9604 - val_loss: 3.1788\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9487 - val_loss: 3.2197\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9551 - val_loss: 3.1767\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9441 - val_loss: 3.2171\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9507 - val_loss: 3.1751\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9403 - val_loss: 3.2149\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9470 - val_loss: 3.1737\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9369 - val_loss: 3.2130\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9436 - val_loss: 3.1726\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9337 - val_loss: 3.2113\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.9404 - val_loss: 3.1714\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9307 - val_loss: 3.2096\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9373 - val_loss: 3.1704\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9276 - val_loss: 3.2079\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9342 - val_loss: 3.1693\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9246 - val_loss: 3.2061\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9310 - val_loss: 3.1681\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9214 - val_loss: 3.2043\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9277 - val_loss: 3.1669\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9182 - val_loss: 3.2024\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9244 - val_loss: 3.1657\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.9148 - val_loss: 3.2004\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9208 - val_loss: 3.1643\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9113 - val_loss: 3.1982\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9172 - val_loss: 3.1629\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9077 - val_loss: 3.1964\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9142 - val_loss: 3.1619\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9044 - val_loss: 3.1949\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9116 - val_loss: 3.1610\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.9013 - val_loss: 3.1930\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9083 - val_loss: 3.1597\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8978 - val_loss: 3.1908\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9046 - val_loss: 3.1582\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8939 - val_loss: 3.1883\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9005 - val_loss: 3.1565\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8898 - val_loss: 3.1855\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8961 - val_loss: 3.1546\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8853 - val_loss: 3.1825\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8914 - val_loss: 3.1525\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8808 - val_loss: 3.1793\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8867 - val_loss: 3.1505\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8764 - val_loss: 3.1763\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8821 - val_loss: 3.1486\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8722 - val_loss: 3.1733\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8776 - val_loss: 3.1468\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8682 - val_loss: 3.1705\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8734 - val_loss: 3.1451\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8644 - val_loss: 3.1678\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8694 - val_loss: 3.1434\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8608 - val_loss: 3.1657\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8663 - val_loss: 3.1421\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8577 - val_loss: 3.1639\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8633 - val_loss: 3.1409\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8548 - val_loss: 3.1621\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8604 - val_loss: 3.1397\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8519 - val_loss: 3.1603\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8575 - val_loss: 3.1385\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8491 - val_loss: 3.1586\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8546 - val_loss: 3.1373\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8463 - val_loss: 3.1570\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8518 - val_loss: 3.1362\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8436 - val_loss: 3.1554\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8491 - val_loss: 3.1351\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8410 - val_loss: 3.1539\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8464 - val_loss: 3.1341\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8385 - val_loss: 3.1525\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8439 - val_loss: 3.1331\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8360 - val_loss: 3.1512\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8415 - val_loss: 3.1322\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8337 - val_loss: 3.1500\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8393 - val_loss: 3.1314\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8315 - val_loss: 3.1490\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8372 - val_loss: 3.1307\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8294 - val_loss: 3.1480\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8352 - val_loss: 3.1301\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8274 - val_loss: 3.1471\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8334 - val_loss: 3.1295\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8255 - val_loss: 3.1463\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8316 - val_loss: 3.1290\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8237 - val_loss: 3.1456\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.8299 - val_loss: 3.1285\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8220 - val_loss: 3.1449\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8284 - val_loss: 3.1282\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8205 - val_loss: 3.1443\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8269 - val_loss: 3.1279\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8190 - val_loss: 3.1438\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8255 - val_loss: 3.1277\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8177 - val_loss: 3.1434\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8242 - val_loss: 3.1277\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8165 - val_loss: 3.1431\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8231 - val_loss: 3.1278\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8155 - val_loss: 3.1429\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8221 - val_loss: 3.1281\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8148 - val_loss: 3.1428\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8212 - val_loss: 3.1285\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8143 - val_loss: 3.1429\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8205 - val_loss: 3.1289\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8138 - val_loss: 3.1430\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8199 - val_loss: 3.1294\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8133 - val_loss: 3.1431\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8193 - val_loss: 3.1298\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8127 - val_loss: 3.1432\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8188 - val_loss: 3.1302\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8121 - val_loss: 3.1433\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8183 - val_loss: 3.1305\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8115 - val_loss: 3.1439\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.8190 - val_loss: 3.1313\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8115 - val_loss: 3.1453\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8215 - val_loss: 3.1333\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8131 - val_loss: 3.1466\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8230 - val_loss: 3.1348\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8142 - val_loss: 3.1477\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8237 - val_loss: 3.1358\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8146 - val_loss: 3.1484\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8238 - val_loss: 3.1365\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8145 - val_loss: 3.1487\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8234 - val_loss: 3.1368\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8140 - val_loss: 3.1486\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8227 - val_loss: 3.1368\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8131 - val_loss: 3.1482\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8216 - val_loss: 3.1366\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8119 - val_loss: 3.1476\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8202 - val_loss: 3.1362\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8104 - val_loss: 3.1467\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8186 - val_loss: 3.1356\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8087 - val_loss: 3.1457\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8168 - val_loss: 3.1349\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8069 - val_loss: 3.1445\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.8149 - val_loss: 3.1342\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.8049 - val_loss: 3.1433\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 2.8129 - val_loss: 3.1333\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8028 - val_loss: 3.1418\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8107 - val_loss: 3.1324\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8005 - val_loss: 3.1403\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8083 - val_loss: 3.1312\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7980 - val_loss: 3.1385\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8057 - val_loss: 3.1299\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7951 - val_loss: 3.1364\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8027 - val_loss: 3.1281\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7917 - val_loss: 3.1335\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7988 - val_loss: 3.1257\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7873 - val_loss: 3.1295\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.7931 - val_loss: 3.1224\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7814 - val_loss: 3.1246\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7863 - val_loss: 3.1193\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7758 - val_loss: 3.1200\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7805 - val_loss: 3.1170\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7712 - val_loss: 3.1163\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7758 - val_loss: 3.1154\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7676 - val_loss: 3.1133\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7721 - val_loss: 3.1142\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7647 - val_loss: 3.1112\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7692 - val_loss: 3.1133\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7624 - val_loss: 3.1097\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7670 - val_loss: 3.1128\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7605 - val_loss: 3.1086\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7652 - val_loss: 3.1124\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7589 - val_loss: 3.1078\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7636 - val_loss: 3.1122\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7575 - val_loss: 3.1072\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7623 - val_loss: 3.1120\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7562 - val_loss: 3.1067\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7610 - val_loss: 3.1119\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7550 - val_loss: 3.1063\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7599 - val_loss: 3.1118\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7539 - val_loss: 3.1059\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7587 - val_loss: 3.1118\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7529 - val_loss: 3.1056\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.7577 - val_loss: 3.1118\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.7519 - val_loss: 3.1053\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7566 - val_loss: 3.1117\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.7509 - val_loss: 3.1051\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 2.7556 - val_loss: 3.1117\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7500 - val_loss: 3.1049\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 2.7546 - val_loss: 3.1118\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7492 - val_loss: 3.1047\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7537 - val_loss: 3.1118\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.7484 - val_loss: 3.1046\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.7529 - val_loss: 3.1119\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7478 - val_loss: 3.1047\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7521 - val_loss: 3.1120\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7472 - val_loss: 3.1048\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7516 - val_loss: 3.1121\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7467 - val_loss: 3.1050\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7511 - val_loss: 3.1123\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7464 - val_loss: 3.1053\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7507 - val_loss: 3.1125\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.7460 - val_loss: 3.1056\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7504 - val_loss: 3.1127\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7457 - val_loss: 3.1059\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.7501 - val_loss: 3.1128\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.7452 - val_loss: 3.1061\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7497 - val_loss: 3.1129\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7447 - val_loss: 3.1063\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.7493 - val_loss: 3.1129\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 2.7442 - val_loss: 3.1064\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7489 - val_loss: 3.1129\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7436 - val_loss: 3.1064\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7485 - val_loss: 3.1128\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7429 - val_loss: 3.1065\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7480 - val_loss: 3.1128\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7423 - val_loss: 3.1065\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7475 - val_loss: 3.1127\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7416 - val_loss: 3.1066\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7471 - val_loss: 3.1127\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7411 - val_loss: 3.1068\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.7468 - val_loss: 3.1128\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7407 - val_loss: 3.1070\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7466 - val_loss: 3.1132\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7406 - val_loss: 3.1076\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7469 - val_loss: 3.1140\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7412 - val_loss: 3.1085\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.7478 - val_loss: 3.1152\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.7421 - val_loss: 3.1096\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7490 - val_loss: 3.1167\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7432 - val_loss: 3.1108\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7501 - val_loss: 3.1180\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7441 - val_loss: 3.1116\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.7508 - val_loss: 3.1188\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7444 - val_loss: 3.1120\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7508 - val_loss: 3.1192\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7441 - val_loss: 3.1119\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7502 - val_loss: 3.1190\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7432 - val_loss: 3.1114\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.7490 - val_loss: 3.1186\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7418 - val_loss: 3.1105\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7475 - val_loss: 3.1180\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7401 - val_loss: 3.1094\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7456 - val_loss: 3.1172\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7383 - val_loss: 3.1082\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7437 - val_loss: 3.1164\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7364 - val_loss: 3.1069\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7417 - val_loss: 3.1155\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7344 - val_loss: 3.1056\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7396 - val_loss: 3.1147\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7325 - val_loss: 3.1043\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7375 - val_loss: 3.1138\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7306 - val_loss: 3.1030\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7355 - val_loss: 3.1132\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7289 - val_loss: 3.1018\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7336 - val_loss: 3.1109\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7251 - val_loss: 3.0989\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7290 - val_loss: 3.1080\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7207 - val_loss: 3.0959\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7243 - val_loss: 3.1056\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7169 - val_loss: 3.0935\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7204 - val_loss: 3.1038\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7139 - val_loss: 3.0915\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7174 - val_loss: 3.1025\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7115 - val_loss: 3.0899\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7149 - val_loss: 3.1014\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7096 - val_loss: 3.0887\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7129 - val_loss: 3.1006\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7079 - val_loss: 3.0877\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7113 - val_loss: 3.0999\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7065 - val_loss: 3.0870\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.7099 - val_loss: 3.0995\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7053 - val_loss: 3.0864\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7088 - val_loss: 3.0991\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7043 - val_loss: 3.0860\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7079 - val_loss: 3.0990\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7035 - val_loss: 3.0858\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7072 - val_loss: 3.0989\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7029 - val_loss: 3.0857\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7068 - val_loss: 3.0994\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7028 - val_loss: 3.0860\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7070 - val_loss: 3.1006\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7039 - val_loss: 3.0875\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7087 - val_loss: 3.1018\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.7051 - val_loss: 3.0889\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 2.7104 - val_loss: 3.1033\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.7063 - val_loss: 3.0903\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7122 - val_loss: 3.1049\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 2.7077 - val_loss: 3.0918\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7140 - val_loss: 3.1064\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7089 - val_loss: 3.0930\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7155 - val_loss: 3.1076\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7098 - val_loss: 3.0939\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7164 - val_loss: 3.1084\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.7103 - val_loss: 3.0944\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 2.7168 - val_loss: 3.1089\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.7103 - val_loss: 3.0947\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7167 - val_loss: 3.1091\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7100 - val_loss: 3.0947\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7163 - val_loss: 3.1091\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7095 - val_loss: 3.0945\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7157 - val_loss: 3.1089\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7087 - val_loss: 3.0941\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7148 - val_loss: 3.1087\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7078 - val_loss: 3.0937\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7139 - val_loss: 3.1084\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7067 - val_loss: 3.0932\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7128 - val_loss: 3.1080\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7057 - val_loss: 3.0926\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 57.5824 - val_loss: 55.3105\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 56.0606 - val_loss: 53.7853\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 54.5325 - val_loss: 52.1529\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 52.8963 - val_loss: 50.3397\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 51.0777 - val_loss: 48.3121\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 49.0436 - val_loss: 46.0529\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 46.7776 - val_loss: 43.5509\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 44.2688 - val_loss: 40.8008\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 41.5120 - val_loss: 37.8087\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 38.5126 - val_loss: 34.6065\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 35.2995 - val_loss: 31.2922\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 31.9660 - val_loss: 28.0631\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 28.7047 - val_loss: 25.1625\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 25.7558 - val_loss: 22.8153\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 23.3191 - val_loss: 21.2184\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 21.5425 - val_loss: 20.1212\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 20.2776 - val_loss: 19.2748\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.2599 - val_loss: 18.5857\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 18.4217 - val_loss: 18.0088\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 17.7203 - val_loss: 17.5089\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17.1178 - val_loss: 17.0517\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 16.5780 - val_loss: 16.5977\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 16.0616 - val_loss: 16.1060\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 15.5279 - val_loss: 15.5519\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 14.9508 - val_loss: 14.9519\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 14.3398 - val_loss: 14.3345\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 13.7166 - val_loss: 13.6829\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 13.0650 - val_loss: 12.9341\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 12.3299 - val_loss: 12.0429\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 11.4748 - val_loss: 11.1119\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 10.6021 - val_loss: 10.3414\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.8814 - val_loss: 9.7626\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.3389 - val_loss: 9.3379\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.9527 - val_loss: 9.0402\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 8.6884 - val_loss: 8.8320\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.5096 - val_loss: 8.6818\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.3865 - val_loss: 8.5749\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 8.2991 - val_loss: 8.4935\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.2333 - val_loss: 8.4275\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.1781 - val_loss: 8.3689\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 8.1267 - val_loss: 8.3134\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.0759 - val_loss: 8.2600\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.0248 - val_loss: 8.2080\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.9730 - val_loss: 8.1578\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 7.9198 - val_loss: 8.1068\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 7.8646 - val_loss: 8.0555\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.8075 - val_loss: 8.0027\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.7485 - val_loss: 7.9495\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.6879 - val_loss: 7.8943\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.6259 - val_loss: 7.8388\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.5627 - val_loss: 7.7810\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.4986 - val_loss: 7.7232\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.4337 - val_loss: 7.6628\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.3682 - val_loss: 7.6030\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7.3023 - val_loss: 7.5403\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 7.2360 - val_loss: 7.4790\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 7.1694 - val_loss: 7.4144\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 7.1026 - val_loss: 7.3521\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.0356 - val_loss: 7.2854\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.9684 - val_loss: 7.2230\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.9012 - val_loss: 7.1519\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.8339 - val_loss: 7.0937\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.7669 - val_loss: 7.0159\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 6.7002 - val_loss: 6.9676\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.6346 - val_loss: 6.8799\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 6.5717 - val_loss: 6.8511\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.5099 - val_loss: 6.7532\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 6.4524 - val_loss: 6.7448\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.3951 - val_loss: 6.6411\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.3444 - val_loss: 6.6459\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.2888 - val_loss: 6.5396\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.2471 - val_loss: 6.5581\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6.1945 - val_loss: 6.4512\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 6.1639 - val_loss: 6.4623\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.0921 - val_loss: 6.3506\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.0652 - val_loss: 6.3521\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.9766 - val_loss: 6.2316\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.9456 - val_loss: 6.2225\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.8451 - val_loss: 6.0985\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.8102 - val_loss: 6.0830\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 5.7056 - val_loss: 5.9541\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 5.6611 - val_loss: 5.9308\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.5551 - val_loss: 5.8094\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.5122 - val_loss: 5.7824\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 5.4081 - val_loss: 5.6666\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 5.3665 - val_loss: 5.6392\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.2668 - val_loss: 5.5296\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.2272 - val_loss: 5.4979\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.1287 - val_loss: 5.3982\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 5.0958 - val_loss: 5.3748\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.0070 - val_loss: 5.2790\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.9785 - val_loss: 5.2642\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.8978 - val_loss: 5.1696\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.8712 - val_loss: 5.1637\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.7978 - val_loss: 5.0691\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7732 - val_loss: 5.0705\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.7050 - val_loss: 4.9752\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.6822 - val_loss: 4.9818\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.6173 - val_loss: 4.8862\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.5957 - val_loss: 4.8937\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.5314 - val_loss: 4.8004\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.5123 - val_loss: 4.8129\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 4.4531 - val_loss: 4.7215\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.4377 - val_loss: 4.7337\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.3771 - val_loss: 4.6466\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.3673 - val_loss: 4.6590\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.3062 - val_loss: 4.5714\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.2951 - val_loss: 4.5893\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.2402 - val_loss: 4.5004\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.2263 - val_loss: 4.5236\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.1779 - val_loss: 4.4347\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.1619 - val_loss: 4.4605\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 4.1183 - val_loss: 4.3724\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.1003 - val_loss: 4.3977\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.0601 - val_loss: 4.3132\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0416 - val_loss: 4.3377\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.0045 - val_loss: 4.2573\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.9861 - val_loss: 4.2836\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.9547 - val_loss: 4.2071\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.9353 - val_loss: 4.2331\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.9077 - val_loss: 4.1613\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8884 - val_loss: 4.1857\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.8633 - val_loss: 4.1189\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.8451 - val_loss: 4.1420\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.8221 - val_loss: 4.0816\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8071 - val_loss: 4.1057\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.7874 - val_loss: 4.0479\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.7735 - val_loss: 4.0733\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.7558 - val_loss: 4.0176\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.7437 - val_loss: 4.0453\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.7287 - val_loss: 3.9924\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7202 - val_loss: 4.0233\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.7075 - val_loss: 3.9713\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.7013 - val_loss: 4.0055\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.6911 - val_loss: 3.9521\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.6846 - val_loss: 3.9895\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.6771 - val_loss: 3.9341\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6694 - val_loss: 3.9714\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6614 - val_loss: 3.9142\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6518 - val_loss: 3.9504\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.6433 - val_loss: 3.8943\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.6341 - val_loss: 3.9286\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.6247 - val_loss: 3.8729\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.6141 - val_loss: 3.9049\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.6044 - val_loss: 3.8503\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.5923 - val_loss: 3.8800\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.5829 - val_loss: 3.8271\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5698 - val_loss: 3.8537\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.5600 - val_loss: 3.8032\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.5465 - val_loss: 3.8276\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.5373 - val_loss: 3.7797\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.5235 - val_loss: 3.8020\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.5150 - val_loss: 3.7569\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.5013 - val_loss: 3.7769\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.4932 - val_loss: 3.7348\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.4797 - val_loss: 3.7526\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.4719 - val_loss: 3.7130\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4586 - val_loss: 3.7289\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4512 - val_loss: 3.6913\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.4377 - val_loss: 3.7052\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.4307 - val_loss: 3.6697\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.4170 - val_loss: 3.6828\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4110 - val_loss: 3.6496\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3978 - val_loss: 3.6595\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.3906 - val_loss: 3.6294\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3786 - val_loss: 3.6370\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3708 - val_loss: 3.6116\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.3611 - val_loss: 3.6169\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3532 - val_loss: 3.5937\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3444 - val_loss: 3.5975\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.3360 - val_loss: 3.5741\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.3270 - val_loss: 3.5777\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.3182 - val_loss: 3.5547\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3092 - val_loss: 3.5581\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.3007 - val_loss: 3.5363\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.2924 - val_loss: 3.5396\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.2839 - val_loss: 3.5174\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.2758 - val_loss: 3.5222\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 3.2683 - val_loss: 3.5008\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 3.2612 - val_loss: 3.5066\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.2547 - val_loss: 3.4862\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.2483 - val_loss: 3.4924\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 3.2425 - val_loss: 3.4701\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2351 - val_loss: 3.4765\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.2288 - val_loss: 3.4558\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.2225 - val_loss: 3.4612\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.2159 - val_loss: 3.4426\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.2106 - val_loss: 3.4481\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.2049 - val_loss: 3.4316\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.2007 - val_loss: 3.4372\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1958 - val_loss: 3.4233\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1934 - val_loss: 3.4285\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1889 - val_loss: 3.4158\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.1873 - val_loss: 3.4205\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1829 - val_loss: 3.4086\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1818 - val_loss: 3.4132\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.1777 - val_loss: 3.4030\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1782 - val_loss: 3.4083\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1755 - val_loss: 3.4004\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.1781 - val_loss: 3.4060\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 3.1753 - val_loss: 3.4010\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.1816 - val_loss: 3.4055\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.1770 - val_loss: 3.4025\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1857 - val_loss: 3.4048\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1788 - val_loss: 3.4083\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1943 - val_loss: 3.4077\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.1848 - val_loss: 3.4116\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1997 - val_loss: 3.4093\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.1891 - val_loss: 3.4115\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2017 - val_loss: 3.4070\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1890 - val_loss: 3.4073\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1993 - val_loss: 3.4009\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1847 - val_loss: 3.3998\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1933 - val_loss: 3.3917\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1766 - val_loss: 3.3899\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.1846 - val_loss: 3.3801\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1657 - val_loss: 3.3781\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.1738 - val_loss: 3.3670\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.1530 - val_loss: 3.3652\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 3.1617 - val_loss: 3.3541\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1403 - val_loss: 3.3526\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1499 - val_loss: 3.3429\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1291 - val_loss: 3.3413\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1395 - val_loss: 3.3333\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.1195 - val_loss: 3.3315\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1305 - val_loss: 3.3251\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1111 - val_loss: 3.3226\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.1225 - val_loss: 3.3177\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.1035 - val_loss: 3.3145\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.1152 - val_loss: 3.3108\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0964 - val_loss: 3.3067\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1082 - val_loss: 3.3044\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0897 - val_loss: 3.2991\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1015 - val_loss: 3.2984\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0833 - val_loss: 3.2915\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0948 - val_loss: 3.2924\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0770 - val_loss: 3.2840\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0880 - val_loss: 3.2865\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0708 - val_loss: 3.2779\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0821 - val_loss: 3.2814\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0655 - val_loss: 3.2723\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0772 - val_loss: 3.2770\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0609 - val_loss: 3.2655\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0715 - val_loss: 3.2719\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0557 - val_loss: 3.2593\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0660 - val_loss: 3.2671\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0507 - val_loss: 3.2530\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0601 - val_loss: 3.2620\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0455 - val_loss: 3.2469\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0541 - val_loss: 3.2569\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.0402 - val_loss: 3.2410\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0482 - val_loss: 3.2519\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0351 - val_loss: 3.2353\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.0424 - val_loss: 3.2470\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0300 - val_loss: 3.2299\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0369 - val_loss: 3.2423\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.0251 - val_loss: 3.2247\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.0317 - val_loss: 3.2379\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.0205 - val_loss: 3.2199\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.0268 - val_loss: 3.2336\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0160 - val_loss: 3.2154\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0222 - val_loss: 3.2296\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0117 - val_loss: 3.2112\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0179 - val_loss: 3.2259\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0077 - val_loss: 3.2075\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.0141 - val_loss: 3.2225\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0041 - val_loss: 3.2047\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0113 - val_loss: 3.2198\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0012 - val_loss: 3.2022\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0088 - val_loss: 3.2174\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9986 - val_loss: 3.1996\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0063 - val_loss: 3.2150\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9960 - val_loss: 3.1970\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0037 - val_loss: 3.2124\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9933 - val_loss: 3.1943\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0011 - val_loss: 3.2097\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9905 - val_loss: 3.1916\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9985 - val_loss: 3.2071\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9878 - val_loss: 3.1896\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9964 - val_loss: 3.2048\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9855 - val_loss: 3.1878\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9945 - val_loss: 3.2027\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9834 - val_loss: 3.1854\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9920 - val_loss: 3.2002\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9810 - val_loss: 3.1826\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9891 - val_loss: 3.1976\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.9782 - val_loss: 3.1795\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9860 - val_loss: 3.1948\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9755 - val_loss: 3.1763\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9829 - val_loss: 3.1921\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9728 - val_loss: 3.1733\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9799 - val_loss: 3.1895\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9702 - val_loss: 3.1703\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9770 - val_loss: 3.1869\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9678 - val_loss: 3.1674\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9742 - val_loss: 3.1845\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9653 - val_loss: 3.1647\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9715 - val_loss: 3.1821\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9631 - val_loss: 3.1622\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9692 - val_loss: 3.1800\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9610 - val_loss: 3.1600\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9672 - val_loss: 3.1781\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9591 - val_loss: 3.1581\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9653 - val_loss: 3.1763\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9573 - val_loss: 3.1561\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9634 - val_loss: 3.1745\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9553 - val_loss: 3.1539\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9613 - val_loss: 3.1725\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9531 - val_loss: 3.1514\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9589 - val_loss: 3.1703\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9507 - val_loss: 3.1487\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9563 - val_loss: 3.1680\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9480 - val_loss: 3.1458\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9534 - val_loss: 3.1655\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9451 - val_loss: 3.1426\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9502 - val_loss: 3.1629\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9420 - val_loss: 3.1392\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9469 - val_loss: 3.1602\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9389 - val_loss: 3.1358\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9435 - val_loss: 3.1575\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9357 - val_loss: 3.1323\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9401 - val_loss: 3.1549\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9326 - val_loss: 3.1290\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9369 - val_loss: 3.1524\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9297 - val_loss: 3.1261\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9340 - val_loss: 3.1502\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9271 - val_loss: 3.1240\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9323 - val_loss: 3.1486\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9253 - val_loss: 3.1212\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9296 - val_loss: 3.1465\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9229 - val_loss: 3.1188\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9271 - val_loss: 3.1445\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9205 - val_loss: 3.1164\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9246 - val_loss: 3.1424\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9182 - val_loss: 3.1139\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9220 - val_loss: 3.1403\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9157 - val_loss: 3.1113\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9194 - val_loss: 3.1382\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9132 - val_loss: 3.1087\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9168 - val_loss: 3.1359\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9106 - val_loss: 3.1061\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.9141 - val_loss: 3.1337\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9080 - val_loss: 3.1035\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9114 - val_loss: 3.1314\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9054 - val_loss: 3.1009\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9089 - val_loss: 3.1293\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9029 - val_loss: 3.0985\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9064 - val_loss: 3.1273\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9005 - val_loss: 3.0963\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.9041 - val_loss: 3.1255\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8983 - val_loss: 3.0944\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9022 - val_loss: 3.1240\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8964 - val_loss: 3.0928\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9007 - val_loss: 3.1229\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8950 - val_loss: 3.0917\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8997 - val_loss: 3.1223\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8941 - val_loss: 3.0911\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8993 - val_loss: 3.1222\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8938 - val_loss: 3.0911\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8995 - val_loss: 3.1226\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8941 - val_loss: 3.0916\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9002 - val_loss: 3.1234\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8950 - val_loss: 3.0928\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9017 - val_loss: 3.1248\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8964 - val_loss: 3.0949\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9039 - val_loss: 3.1267\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8985 - val_loss: 3.0975\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9068 - val_loss: 3.1292\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9013 - val_loss: 3.1003\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.9100 - val_loss: 3.1327\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9052 - val_loss: 3.1042\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9145 - val_loss: 3.1381\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9113 - val_loss: 3.1107\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9221 - val_loss: 3.1467\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9214 - val_loss: 3.1245\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9377 - val_loss: 3.1622\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9404 - val_loss: 3.1388\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.9544 - val_loss: 3.1752\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9559 - val_loss: 3.1523\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9689 - val_loss: 3.1821\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9638 - val_loss: 3.1605\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9769 - val_loss: 3.1833\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9646 - val_loss: 3.1584\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.9740 - val_loss: 3.1791\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9592 - val_loss: 3.1510\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9655 - val_loss: 3.1727\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9514 - val_loss: 3.1412\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9546 - val_loss: 3.1645\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9418 - val_loss: 3.1294\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9415 - val_loss: 3.1516\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.9280 - val_loss: 3.1148\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9248 - val_loss: 3.1392\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.9137 - val_loss: 3.1026\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9109 - val_loss: 3.1238\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8960 - val_loss: 3.0878\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8944 - val_loss: 3.1089\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8784 - val_loss: 3.0729\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8778 - val_loss: 3.0936\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8597 - val_loss: 3.0543\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8571 - val_loss: 3.0804\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8432 - val_loss: 3.0401\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8403 - val_loss: 3.0707\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8305 - val_loss: 3.0330\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8315 - val_loss: 3.0639\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8226 - val_loss: 3.0288\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8261 - val_loss: 3.0605\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8183 - val_loss: 3.0265\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8231 - val_loss: 3.0593\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8163 - val_loss: 3.0256\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8219 - val_loss: 3.0592\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8155 - val_loss: 3.0253\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8215 - val_loss: 3.0597\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8155 - val_loss: 3.0256\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8218 - val_loss: 3.0615\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8168 - val_loss: 3.0267\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8232 - val_loss: 3.0635\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8185 - val_loss: 3.0278\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8246 - val_loss: 3.0645\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8193 - val_loss: 3.0282\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8250 - val_loss: 3.0646\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8193 - val_loss: 3.0280\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8248 - val_loss: 3.0639\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.8186 - val_loss: 3.0273\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8239 - val_loss: 3.0631\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8176 - val_loss: 3.0267\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8230 - val_loss: 3.0623\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8167 - val_loss: 3.0262\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8223 - val_loss: 3.0616\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8160 - val_loss: 3.0272\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8235 - val_loss: 3.0622\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8169 - val_loss: 3.0282\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8246 - val_loss: 3.0628\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8179 - val_loss: 3.0293\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8255 - val_loss: 3.0634\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8188 - val_loss: 3.0302\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8262 - val_loss: 3.0642\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8197 - val_loss: 3.0310\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8267 - val_loss: 3.0647\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8202 - val_loss: 3.0317\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8272 - val_loss: 3.0651\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8205 - val_loss: 3.0330\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8281 - val_loss: 3.0672\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8228 - val_loss: 3.0377\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8328 - val_loss: 3.0719\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8282 - val_loss: 3.0422\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8373 - val_loss: 3.0770\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8343 - val_loss: 3.0469\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8417 - val_loss: 3.0805\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8383 - val_loss: 3.0494\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8438 - val_loss: 3.0815\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8393 - val_loss: 3.0495\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8433 - val_loss: 3.0804\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.8378 - val_loss: 3.0477\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8407 - val_loss: 3.0777\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.8342 - val_loss: 3.0444\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8366 - val_loss: 3.0736\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8290 - val_loss: 3.0401\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8314 - val_loss: 3.0683\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 2.8222 - val_loss: 3.0352\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8253 - val_loss: 3.0619\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.8140 - val_loss: 3.0294\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8184 - val_loss: 3.0563\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8068 - val_loss: 3.0243\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8124 - val_loss: 3.0516\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8008 - val_loss: 3.0158\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8031 - val_loss: 3.0449\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.7926 - val_loss: 3.0093\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7958 - val_loss: 3.0407\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7872 - val_loss: 3.0052\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7910 - val_loss: 3.0381\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7836 - val_loss: 3.0026\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7879 - val_loss: 3.0364\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7811 - val_loss: 3.0010\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7860 - val_loss: 3.0353\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7794 - val_loss: 2.9999\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7847 - val_loss: 3.0345\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7780 - val_loss: 2.9991\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7836 - val_loss: 3.0337\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7767 - val_loss: 2.9983\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7826 - val_loss: 3.0329\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7753 - val_loss: 2.9966\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7807 - val_loss: 3.0318\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7736 - val_loss: 2.9944\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7782 - val_loss: 3.0306\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7717 - val_loss: 2.9927\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7762 - val_loss: 3.0296\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7700 - val_loss: 2.9916\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7748 - val_loss: 3.0289\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7687 - val_loss: 2.9910\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7740 - val_loss: 3.0286\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7679 - val_loss: 2.9910\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7739 - val_loss: 3.0287\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7675 - val_loss: 2.9913\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7741 - val_loss: 3.0289\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7674 - val_loss: 2.9911\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7739 - val_loss: 3.0288\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7670 - val_loss: 2.9907\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7734 - val_loss: 3.0284\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7662 - val_loss: 2.9917\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7746 - val_loss: 3.0294\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7672 - val_loss: 2.9920\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7750 - val_loss: 3.0297\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7673 - val_loss: 2.9918\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7747 - val_loss: 3.0295\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7667 - val_loss: 2.9913\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7739 - val_loss: 3.0290\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7658 - val_loss: 2.9902\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7723 - val_loss: 3.0280\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7642 - val_loss: 2.9885\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7700 - val_loss: 3.0263\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7618 - val_loss: 2.9869\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7678 - val_loss: 3.0247\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7596 - val_loss: 2.9854\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7657 - val_loss: 3.0229\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7573 - val_loss: 2.9838\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7635 - val_loss: 3.0210\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7550 - val_loss: 2.9823\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7614 - val_loss: 3.0190\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7527 - val_loss: 2.9810\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7595 - val_loss: 3.0172\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7509 - val_loss: 2.9800\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7578 - val_loss: 3.0156\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7493 - val_loss: 2.9790\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7562 - val_loss: 3.0142\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7479 - val_loss: 2.9780\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7546 - val_loss: 3.0127\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7464 - val_loss: 2.9770\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7529 - val_loss: 3.0112\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7448 - val_loss: 2.9759\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7512 - val_loss: 3.0098\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7432 - val_loss: 2.9747\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7494 - val_loss: 3.0083\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7414 - val_loss: 2.9736\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7476 - val_loss: 3.0067\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7394 - val_loss: 2.9723\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7457 - val_loss: 3.0048\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7366 - val_loss: 2.9708\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7434 - val_loss: 3.0028\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7338 - val_loss: 2.9692\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7410 - val_loss: 3.0009\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7312 - val_loss: 2.9677\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7387 - val_loss: 2.9992\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7290 - val_loss: 2.9663\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7367 - val_loss: 2.9977\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7270 - val_loss: 2.9651\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7348 - val_loss: 2.9963\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7252 - val_loss: 2.9640\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7331 - val_loss: 2.9951\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.7236 - val_loss: 2.9630\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7314 - val_loss: 2.9940\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7221 - val_loss: 2.9620\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7299 - val_loss: 2.9930\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7208 - val_loss: 2.9611\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7285 - val_loss: 2.9921\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7195 - val_loss: 2.9602\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7271 - val_loss: 2.9912\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7182 - val_loss: 2.9594\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7258 - val_loss: 2.9905\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7171 - val_loss: 2.9585\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7245 - val_loss: 2.9898\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7159 - val_loss: 2.9577\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7233 - val_loss: 2.9891\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7148 - val_loss: 2.9568\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7221 - val_loss: 2.9884\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.7137 - val_loss: 2.9560\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7209 - val_loss: 2.9878\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7127 - val_loss: 2.9552\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7198 - val_loss: 2.9872\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7117 - val_loss: 2.9544\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7187 - val_loss: 2.9867\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7107 - val_loss: 2.9536\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7177 - val_loss: 2.9862\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7098 - val_loss: 2.9529\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7168 - val_loss: 2.9859\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7089 - val_loss: 2.9522\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7160 - val_loss: 2.9857\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7083 - val_loss: 2.9517\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7155 - val_loss: 2.9859\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7080 - val_loss: 2.9513\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7152 - val_loss: 2.9866\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7083 - val_loss: 2.9514\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7156 - val_loss: 2.9878\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7092 - val_loss: 2.9518\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7164 - val_loss: 2.9894\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7105 - val_loss: 2.9524\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7175 - val_loss: 2.9913\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.7120 - val_loss: 2.9532\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7188 - val_loss: 2.9930\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7135 - val_loss: 2.9539\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7200 - val_loss: 2.9944\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7146 - val_loss: 2.9544\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7208 - val_loss: 2.9951\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7153 - val_loss: 2.9546\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7210 - val_loss: 2.9954\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7154 - val_loss: 2.9544\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7208 - val_loss: 2.9951\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7150 - val_loss: 2.9539\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7201 - val_loss: 2.9945\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7141 - val_loss: 2.9531\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7190 - val_loss: 2.9934\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 56.3761 - val_loss: 56.3303\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 54.7975 - val_loss: 54.6149\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 53.0775 - val_loss: 52.6987\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 51.1573 - val_loss: 50.5508\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 49.0075 - val_loss: 48.1486\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 46.6079 - val_loss: 45.4750\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 43.9427 - val_loss: 42.5416\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 41.0221 - val_loss: 39.3858\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 37.8816 - val_loss: 36.0605\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 34.5766 - val_loss: 32.6822\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 31.2296 - val_loss: 29.4683\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 28.0591 - val_loss: 26.6511\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 25.2969 - val_loss: 24.3756\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 23.1083 - val_loss: 22.7716\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 21.5865 - val_loss: 21.6147\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 20.4712 - val_loss: 20.6796\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 19.5776 - val_loss: 19.9049\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 18.8415 - val_loss: 19.2524\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 18.2282 - val_loss: 18.6963\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 17.7111 - val_loss: 18.2148\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 17.2648 - val_loss: 17.7830\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 16.8628 - val_loss: 17.3706\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.4745 - val_loss: 16.9463\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 16.0693 - val_loss: 16.4928\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 15.6308 - val_loss: 15.9910\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 15.1410 - val_loss: 15.3771\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 14.5344 - val_loss: 14.6189\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 13.7736 - val_loss: 13.8193\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 12.9746 - val_loss: 13.0534\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 12.2197 - val_loss: 12.3260\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 11.5101 - val_loss: 11.6589\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 10.8779 - val_loss: 11.0757\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 10.3362 - val_loss: 10.5795\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.8746 - val_loss: 10.1605\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 9.4854 - val_loss: 9.8225\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 9.1739 - val_loss: 9.5616\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.9377 - val_loss: 9.3634\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.7626 - val_loss: 9.2122\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.6325 - val_loss: 9.0958\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.5337 - val_loss: 9.0047\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 8.4558 - val_loss: 8.9308\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.3909 - val_loss: 8.8687\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3330 - val_loss: 8.8126\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 8.2781 - val_loss: 8.7594\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 8.2252 - val_loss: 8.7085\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1735 - val_loss: 8.6583\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.1219 - val_loss: 8.6084\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.0701 - val_loss: 8.5580\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.0177 - val_loss: 8.5071\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.9651 - val_loss: 8.4558\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.9125 - val_loss: 8.4042\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.8598 - val_loss: 8.3519\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.8070 - val_loss: 8.2989\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.7539 - val_loss: 8.2451\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.7003 - val_loss: 8.1906\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.6462 - val_loss: 8.1350\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.5912 - val_loss: 8.0783\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.5353 - val_loss: 8.0204\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 7.4783 - val_loss: 7.9612\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.4202 - val_loss: 7.9009\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 7.3609 - val_loss: 7.8394\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.3004 - val_loss: 7.7768\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 7.2388 - val_loss: 7.7130\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 7.1759 - val_loss: 7.6480\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 7.1117 - val_loss: 7.5817\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.0464 - val_loss: 7.5140\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.9798 - val_loss: 7.4451\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.9121 - val_loss: 7.3749\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.8434 - val_loss: 7.3041\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.7741 - val_loss: 7.2325\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.7040 - val_loss: 7.1605\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 6.6332 - val_loss: 7.0875\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.5618 - val_loss: 7.0152\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 6.4899 - val_loss: 6.9409\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.4176 - val_loss: 6.8705\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.3454 - val_loss: 6.7939\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.2738 - val_loss: 6.7324\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 6.2038 - val_loss: 6.6547\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.1365 - val_loss: 6.6117\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.0770 - val_loss: 6.5534\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.0379 - val_loss: 6.5602\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.0205 - val_loss: 6.6048\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 6.1210 - val_loss: 6.7870\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6.2322 - val_loss: 6.9186\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.5046 - val_loss: 6.8265\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.2883 - val_loss: 6.7892\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.3910 - val_loss: 6.5225\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 6.0081 - val_loss: 6.4333\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.0156 - val_loss: 6.2552\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 5.7593 - val_loss: 6.1564\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 5.7277 - val_loss: 6.0336\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 5.5532 - val_loss: 5.9335\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 5.5022 - val_loss: 5.8588\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.3934 - val_loss: 5.7672\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.3386 - val_loss: 5.6911\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 5.2422 - val_loss: 5.6192\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.1983 - val_loss: 5.5443\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.1109 - val_loss: 5.4820\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.0708 - val_loss: 5.4167\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.9981 - val_loss: 5.3604\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.9611 - val_loss: 5.2993\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.8967 - val_loss: 5.2501\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.8625 - val_loss: 5.1918\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.8055 - val_loss: 5.1473\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.7720 - val_loss: 5.0922\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.7206 - val_loss: 5.0496\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.6867 - val_loss: 4.9951\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.6352 - val_loss: 4.9557\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.6038 - val_loss: 4.9037\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.5531 - val_loss: 4.8709\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.5284 - val_loss: 4.8223\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.4819 - val_loss: 4.7949\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.4612 - val_loss: 4.7495\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 4.4207 - val_loss: 4.7250\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.4009 - val_loss: 4.6825\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.3661 - val_loss: 4.6594\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.3454 - val_loss: 4.6192\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 4.3145 - val_loss: 4.6001\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.2970 - val_loss: 4.5621\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.2698 - val_loss: 4.5465\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 4.2550 - val_loss: 4.5105\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2308 - val_loss: 4.4965\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.2169 - val_loss: 4.4618\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.1940 - val_loss: 4.4477\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 4.1803 - val_loss: 4.4159\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.1587 - val_loss: 4.4004\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.1452 - val_loss: 4.3704\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.1226 - val_loss: 4.3531\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.1094 - val_loss: 4.3267\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0875 - val_loss: 4.3076\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 4.0750 - val_loss: 4.2861\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.0547 - val_loss: 4.2661\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.0435 - val_loss: 4.2476\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.022 - 0s 78ms/step - loss: 4.0226 - val_loss: 4.2259\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.0124 - val_loss: 4.2096\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.9896 - val_loss: 4.1871\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.9818 - val_loss: 4.1746\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.9591 - val_loss: 4.1511\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.9535 - val_loss: 4.1427\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9313 - val_loss: 4.1192\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.9286 - val_loss: 4.1149\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.9075 - val_loss: 4.0910\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.9068 - val_loss: 4.0912\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.8880 - val_loss: 4.0649\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.8865 - val_loss: 4.0703\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.8707 - val_loss: 4.0399\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.8669 - val_loss: 4.0483\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.8514 - val_loss: 4.0134\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.8455 - val_loss: 4.0241\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 3.8292 - val_loss: 3.9854\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 3.8221 - val_loss: 3.9975\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.8041 - val_loss: 3.9562\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.7969 - val_loss: 3.9694\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.7774 - val_loss: 3.9264\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.7708 - val_loss: 3.9415\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.7508 - val_loss: 3.8970\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7446 - val_loss: 3.9106\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.7205 - val_loss: 3.8659\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7157 - val_loss: 3.8820\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 3.6932 - val_loss: 3.8372\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.6891 - val_loss: 3.8557\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.6683 - val_loss: 3.8105\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.6644 - val_loss: 3.8322\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.6464 - val_loss: 3.7856\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.6416 - val_loss: 3.8100\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.6258 - val_loss: 3.7615\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.6196 - val_loss: 3.7879\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.6057 - val_loss: 3.7375\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.5973 - val_loss: 3.7643\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.5843 - val_loss: 3.7112\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.5721 - val_loss: 3.7377\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.5600 - val_loss: 3.6858\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.5472 - val_loss: 3.7107\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5353 - val_loss: 3.6614\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.5228 - val_loss: 3.6845\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.5112 - val_loss: 3.6398\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.5010 - val_loss: 3.6623\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.4907 - val_loss: 3.6211\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.4826 - val_loss: 3.6434\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.4734 - val_loss: 3.6042\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4664 - val_loss: 3.6281\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.4594 - val_loss: 3.5885\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.4523 - val_loss: 3.6141\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.4469 - val_loss: 3.5740\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.4396 - val_loss: 3.6002\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.4345 - val_loss: 3.5597\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.4270 - val_loss: 3.5863\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.4219 - val_loss: 3.5455\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.4143 - val_loss: 3.5743\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.4106 - val_loss: 3.5332\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.4039 - val_loss: 3.5650\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.4020 - val_loss: 3.5235\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.3965 - val_loss: 3.5588\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.3964 - val_loss: 3.5149\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3905 - val_loss: 3.5534\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3916 - val_loss: 3.5061\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3841 - val_loss: 3.5471\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3858 - val_loss: 3.4965\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3764 - val_loss: 3.5392\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.3786 - val_loss: 3.4866\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.3680 - val_loss: 3.5304\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3706 - val_loss: 3.4767\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.3591 - val_loss: 3.5212\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3621 - val_loss: 3.4672\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.3507 - val_loss: 3.5122\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.3537 - val_loss: 3.4580\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3424 - val_loss: 3.5032\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.3454 - val_loss: 3.4495\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3347 - val_loss: 3.4945\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.3374 - val_loss: 3.4407\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.3265 - val_loss: 3.4853\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.3289 - val_loss: 3.4315\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.3179 - val_loss: 3.4756\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.3200 - val_loss: 3.4223\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.3090 - val_loss: 3.4658\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.3111 - val_loss: 3.4132\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3002 - val_loss: 3.4562\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.3023 - val_loss: 3.4044\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2917 - val_loss: 3.4468\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2937 - val_loss: 3.3961\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.2834 - val_loss: 3.4377\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.2855 - val_loss: 3.3882\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2756 - val_loss: 3.4290\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.2777 - val_loss: 3.3808\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.2682 - val_loss: 3.4210\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2706 - val_loss: 3.3741\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.2617 - val_loss: 3.4139\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.2643 - val_loss: 3.3681\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.2560 - val_loss: 3.4072\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2583 - val_loss: 3.3616\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2499 - val_loss: 3.3998\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2517 - val_loss: 3.3544\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.2429 - val_loss: 3.3917\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2445 - val_loss: 3.3469\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2355 - val_loss: 3.3832\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.2370 - val_loss: 3.3393\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.2278 - val_loss: 3.3744\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.2292 - val_loss: 3.3316\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.2200 - val_loss: 3.3652\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.2209 - val_loss: 3.3236\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2117 - val_loss: 3.3555\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.2122 - val_loss: 3.3156\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2032 - val_loss: 3.3467\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.2043 - val_loss: 3.3082\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.1957 - val_loss: 3.3392\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1976 - val_loss: 3.3017\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 3.1892 - val_loss: 3.3327\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1918 - val_loss: 3.2957\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1835 - val_loss: 3.3267\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1864 - val_loss: 3.2898\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1780 - val_loss: 3.3208\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1812 - val_loss: 3.2840\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1727 - val_loss: 3.3148\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1760 - val_loss: 3.2780\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1672 - val_loss: 3.3085\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.1704 - val_loss: 3.2722\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1618 - val_loss: 3.3025\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.1651 - val_loss: 3.2673\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1573 - val_loss: 3.2981\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.1611 - val_loss: 3.2625\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.1532 - val_loss: 3.2937\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.1571 - val_loss: 3.2576\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1489 - val_loss: 3.2889\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1528 - val_loss: 3.2524\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1443 - val_loss: 3.2836\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1481 - val_loss: 3.2470\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.1395 - val_loss: 3.2781\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.1432 - val_loss: 3.2416\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1347 - val_loss: 3.2726\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1382 - val_loss: 3.2364\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1299 - val_loss: 3.2672\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1333 - val_loss: 3.2313\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.1253 - val_loss: 3.2618\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.1284 - val_loss: 3.2262\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1206 - val_loss: 3.2565\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.1235 - val_loss: 3.2212\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1160 - val_loss: 3.2512\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1186 - val_loss: 3.2162\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.1113 - val_loss: 3.2460\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.1138 - val_loss: 3.2112\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1066 - val_loss: 3.2407\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.1089 - val_loss: 3.2061\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1019 - val_loss: 3.2355\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1041 - val_loss: 3.2010\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0971 - val_loss: 3.2302\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0993 - val_loss: 3.1959\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0922 - val_loss: 3.2249\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0944 - val_loss: 3.1906\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0872 - val_loss: 3.2195\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0896 - val_loss: 3.1854\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.0822 - val_loss: 3.2138\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0844 - val_loss: 3.1799\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0769 - val_loss: 3.2056\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0770 - val_loss: 3.1724\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.0692 - val_loss: 3.1973\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.0697 - val_loss: 3.1655\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0621 - val_loss: 3.1898\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0631 - val_loss: 3.1592\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.0556 - val_loss: 3.1820\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0565 - val_loss: 3.1532\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0495 - val_loss: 3.1741\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0498 - val_loss: 3.1475\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0437 - val_loss: 3.1674\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0440 - val_loss: 3.1427\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0388 - val_loss: 3.1622\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0395 - val_loss: 3.1390\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0351 - val_loss: 3.1583\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0361 - val_loss: 3.1361\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0323 - val_loss: 3.1555\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0336 - val_loss: 3.1333\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0299 - val_loss: 3.1530\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0314 - val_loss: 3.1307\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0276 - val_loss: 3.1505\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0293 - val_loss: 3.1280\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0253 - val_loss: 3.1481\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0272 - val_loss: 3.1254\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.0230 - val_loss: 3.1456\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0251 - val_loss: 3.1228\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0208 - val_loss: 3.1432\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0231 - val_loss: 3.1202\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0185 - val_loss: 3.1407\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0210 - val_loss: 3.1176\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0162 - val_loss: 3.1381\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0189 - val_loss: 3.1150\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0138 - val_loss: 3.1356\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 3.0168 - val_loss: 3.1123\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0115 - val_loss: 3.1330\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0146 - val_loss: 3.1097\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0091 - val_loss: 3.1303\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0124 - val_loss: 3.1070\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0066 - val_loss: 3.1276\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.0102 - val_loss: 3.1044\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0042 - val_loss: 3.1249\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.0079 - val_loss: 3.1017\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0017 - val_loss: 3.1221\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0056 - val_loss: 3.0991\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9992 - val_loss: 3.1193\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0033 - val_loss: 3.0964\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9967 - val_loss: 3.1163\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0008 - val_loss: 3.0937\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.9941 - val_loss: 3.1121\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.9970 - val_loss: 3.0906\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.9906 - val_loss: 3.1079\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9934 - val_loss: 3.0874\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9872 - val_loss: 3.1041\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9901 - val_loss: 3.0844\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9840 - val_loss: 3.1006\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9871 - val_loss: 3.0815\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9811 - val_loss: 3.0974\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9845 - val_loss: 3.0789\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9785 - val_loss: 3.0944\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9820 - val_loss: 3.0764\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 2.9760 - val_loss: 3.0918\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9798 - val_loss: 3.0740\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9738 - val_loss: 3.0894\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9779 - val_loss: 3.0718\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9718 - val_loss: 3.0873\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9762 - val_loss: 3.0698\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9700 - val_loss: 3.0859\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9751 - val_loss: 3.0681\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9687 - val_loss: 3.0851\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9746 - val_loss: 3.0668\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9679 - val_loss: 3.0847\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9743 - val_loss: 3.0657\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9674 - val_loss: 3.0845\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9743 - val_loss: 3.0648\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9671 - val_loss: 3.0845\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9743 - val_loss: 3.0641\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9671 - val_loss: 3.0849\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9746 - val_loss: 3.0639\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9677 - val_loss: 3.0860\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.9756 - val_loss: 3.0643\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9689 - val_loss: 3.0882\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9774 - val_loss: 3.0650\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9706 - val_loss: 3.0910\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9797 - val_loss: 3.0661\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9727 - val_loss: 3.0941\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9822 - val_loss: 3.0674\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 2.9751 - val_loss: 3.0970\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9848 - val_loss: 3.0688\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9778 - val_loss: 3.0996\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9871 - val_loss: 3.0701\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9804 - val_loss: 3.1015\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9888 - val_loss: 3.0703\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9817 - val_loss: 3.1018\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9891 - val_loss: 3.0692\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9813 - val_loss: 3.1004\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9879 - val_loss: 3.0669\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9793 - val_loss: 3.0974\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9852 - val_loss: 3.0637\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.9759 - val_loss: 3.0932\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9813 - val_loss: 3.0598\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.9716 - val_loss: 3.0879\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9767 - val_loss: 3.0553\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9666 - val_loss: 3.0819\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9713 - val_loss: 3.0504\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9610 - val_loss: 3.0751\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9652 - val_loss: 3.0449\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9546 - val_loss: 3.0671\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9580 - val_loss: 3.0388\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.9473 - val_loss: 3.0582\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9500 - val_loss: 3.0327\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9399 - val_loss: 3.0481\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9414 - val_loss: 3.0263\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9326 - val_loss: 3.0372\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9321 - val_loss: 3.0198\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9251 - val_loss: 3.0290\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9248 - val_loss: 3.0146\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9191 - val_loss: 3.0229\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9194 - val_loss: 3.0104\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.9140 - val_loss: 3.0180\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9150 - val_loss: 3.0065\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9093 - val_loss: 3.0135\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.9109 - val_loss: 3.0028\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9048 - val_loss: 3.0093\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9071 - val_loss: 2.9995\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9009 - val_loss: 3.0056\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9038 - val_loss: 2.9966\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8976 - val_loss: 3.0025\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 2.9009 - val_loss: 2.9942\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8950 - val_loss: 3.0001\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8987 - val_loss: 2.9923\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8932 - val_loss: 2.9984\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8971 - val_loss: 2.9911\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8923 - val_loss: 2.9977\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8964 - val_loss: 2.9908\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8929 - val_loss: 2.9984\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8971 - val_loss: 2.9914\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8944 - val_loss: 3.0000\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8985 - val_loss: 2.9926\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8965 - val_loss: 3.0020\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9002 - val_loss: 2.9934\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8981 - val_loss: 3.0034\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9013 - val_loss: 2.9937\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8990 - val_loss: 3.0042\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9019 - val_loss: 2.9936\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8993 - val_loss: 3.0043\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9019 - val_loss: 2.9930\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8992 - val_loss: 3.0039\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.9014 - val_loss: 2.9921\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8987 - val_loss: 3.0031\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9005 - val_loss: 2.9910\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8979 - val_loss: 3.0020\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8995 - val_loss: 2.9897\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8969 - val_loss: 3.0007\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8983 - val_loss: 2.9883\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8957 - val_loss: 2.9992\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8969 - val_loss: 2.9868\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.8944 - val_loss: 2.9975\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8953 - val_loss: 2.9850\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8928 - val_loss: 2.9955\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8935 - val_loss: 2.9831\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8909 - val_loss: 2.9934\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8916 - val_loss: 2.9811\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8889 - val_loss: 2.9910\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8894 - val_loss: 2.9789\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8867 - val_loss: 2.9885\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8872 - val_loss: 2.9767\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8845 - val_loss: 2.9861\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8849 - val_loss: 2.9746\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8824 - val_loss: 2.9838\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8828 - val_loss: 2.9726\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8804 - val_loss: 2.9816\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8808 - val_loss: 2.9708\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8787 - val_loss: 2.9797\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8790 - val_loss: 2.9691\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8772 - val_loss: 2.9780\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8774 - val_loss: 2.9677\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8758 - val_loss: 2.9765\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8760 - val_loss: 2.9664\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8747 - val_loss: 2.9751\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8747 - val_loss: 2.9652\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8737 - val_loss: 2.9739\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8736 - val_loss: 2.9640\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8727 - val_loss: 2.9727\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8725 - val_loss: 2.9629\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8718 - val_loss: 2.9716\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8714 - val_loss: 2.9617\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8709 - val_loss: 2.9705\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.8704 - val_loss: 2.9605\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8699 - val_loss: 2.9694\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8693 - val_loss: 2.9593\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8690 - val_loss: 2.9682\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8682 - val_loss: 2.9581\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8679 - val_loss: 2.9670\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8671 - val_loss: 2.9568\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8669 - val_loss: 2.9658\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.8660 - val_loss: 2.9555\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8658 - val_loss: 2.9646\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8648 - val_loss: 2.9542\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8647 - val_loss: 2.9634\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8637 - val_loss: 2.9529\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8636 - val_loss: 2.9621\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8625 - val_loss: 2.9515\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8624 - val_loss: 2.9609\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8613 - val_loss: 2.9501\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8612 - val_loss: 2.9596\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8601 - val_loss: 2.9487\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8601 - val_loss: 2.9584\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8589 - val_loss: 2.9473\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8589 - val_loss: 2.9571\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8577 - val_loss: 2.9459\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8577 - val_loss: 2.9559\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8566 - val_loss: 2.9445\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8566 - val_loss: 2.9547\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8554 - val_loss: 2.9432\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8554 - val_loss: 2.9536\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8543 - val_loss: 2.9419\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8544 - val_loss: 2.9525\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8533 - val_loss: 2.9407\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.8534 - val_loss: 2.9515\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8523 - val_loss: 2.9395\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8524 - val_loss: 2.9506\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8515 - val_loss: 2.9384\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8516 - val_loss: 2.9498\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8507 - val_loss: 2.9374\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8509 - val_loss: 2.9491\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8501 - val_loss: 2.9366\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8503 - val_loss: 2.9485\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8495 - val_loss: 2.9358\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8498 - val_loss: 2.9480\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8491 - val_loss: 2.9351\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.8494 - val_loss: 2.9476\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8486 - val_loss: 2.9344\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8490 - val_loss: 2.9472\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8483 - val_loss: 2.9338\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8487 - val_loss: 2.9467\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8479 - val_loss: 2.9331\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8484 - val_loss: 2.9463\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8475 - val_loss: 2.9324\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8480 - val_loss: 2.9458\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8471 - val_loss: 2.9317\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8476 - val_loss: 2.9453\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8466 - val_loss: 2.9309\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8470 - val_loss: 2.9447\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8460 - val_loss: 2.9300\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8464 - val_loss: 2.9439\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8452 - val_loss: 2.9290\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8456 - val_loss: 2.9430\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8443 - val_loss: 2.9279\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8446 - val_loss: 2.9418\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8432 - val_loss: 2.9265\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8434 - val_loss: 2.9404\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8419 - val_loss: 2.9249\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8418 - val_loss: 2.9387\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8401 - val_loss: 2.9233\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8403 - val_loss: 2.9371\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8385 - val_loss: 2.9218\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8391 - val_loss: 2.9357\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8371 - val_loss: 2.9206\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.8382 - val_loss: 2.9346\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8360 - val_loss: 2.9195\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8375 - val_loss: 2.9337\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8351 - val_loss: 2.9187\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8370 - val_loss: 2.9330\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8345 - val_loss: 2.9195\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8379 - val_loss: 2.9341\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8355 - val_loss: 2.9201\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8390 - val_loss: 2.9351\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8365 - val_loss: 2.9206\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8397 - val_loss: 2.9356\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8370 - val_loss: 2.9206\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8399 - val_loss: 2.9356\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8370 - val_loss: 2.9201\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8395 - val_loss: 2.9350\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8364 - val_loss: 2.9190\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8386 - val_loss: 2.9338\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8352 - val_loss: 2.9174\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8369 - val_loss: 2.9320\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8333 - val_loss: 2.9152\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8347 - val_loss: 2.9295\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8309 - val_loss: 2.9125\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8318 - val_loss: 2.9258\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.8272 - val_loss: 2.9086\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8277 - val_loss: 2.9203\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8219 - val_loss: 2.9032\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8217 - val_loss: 2.9141\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8161 - val_loss: 2.8976\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8153 - val_loss: 2.9080\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8103 - val_loss: 2.8924\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8094 - val_loss: 2.9023\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8049 - val_loss: 2.8879\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8044 - val_loss: 2.8975\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8003 - val_loss: 2.8829\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7989 - val_loss: 2.8920\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7949 - val_loss: 2.8772\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7916 - val_loss: 2.8846\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7877 - val_loss: 2.8721\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7858 - val_loss: 2.8784\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7818 - val_loss: 2.8680\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7813 - val_loss: 2.8739\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7774 - val_loss: 2.8646\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.7776 - val_loss: 2.8704\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7740 - val_loss: 2.8609\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7737 - val_loss: 2.8668\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7704 - val_loss: 2.8582\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7706 - val_loss: 2.8642\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7677 - val_loss: 2.8562\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7683 - val_loss: 2.8624\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7658 - val_loss: 2.8548\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 56.1456 - val_loss: 54.5588\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 54.6180 - val_loss: 52.9345\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 52.9935 - val_loss: 51.1016\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 51.1614 - val_loss: 48.9568\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 49.0173 - val_loss: 46.4467\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 46.5054 - val_loss: 43.5924\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 43.6446 - val_loss: 40.4676\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 40.5068 - val_loss: 37.1710\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 37.1867 - val_loss: 33.8080\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 33.7805 - val_loss: 30.5120\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 30.4081 - val_loss: 27.4502\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 27.2412 - val_loss: 24.7728\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 24.4885 - val_loss: 22.6436\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 22.3858 - val_loss: 21.1668\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 20.9596 - val_loss: 19.9827\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 19.8656 - val_loss: 19.0104\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 18.9822 - val_loss: 18.2088\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 18.2479 - val_loss: 17.5376\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 17.6210 - val_loss: 16.9550\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 17.0672 - val_loss: 16.4170\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 16.5496 - val_loss: 15.8765\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 16.0250 - val_loss: 15.2849\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 15.4458 - val_loss: 14.5987\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.7677 - val_loss: 13.8000\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 13.9701 - val_loss: 12.9156\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 13.0732 - val_loss: 12.0011\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 12.1080 - val_loss: 11.1435\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 11.1805 - val_loss: 10.4126\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 10.4144 - val_loss: 9.7960\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 9.8163 - val_loss: 9.3380\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.3733 - val_loss: 9.0037\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 9.0575 - val_loss: 8.7756\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.8375 - val_loss: 8.6227\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 8.6860 - val_loss: 8.5208\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.5817 - val_loss: 8.4520\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 8.5092 - val_loss: 8.4037\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.4575 - val_loss: 8.3662\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 8.4172 - val_loss: 8.3320\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 8.3820 - val_loss: 8.2987\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.3485 - val_loss: 8.2652\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 8.3158 - val_loss: 8.2314\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.2830 - val_loss: 8.1966\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 8.2495 - val_loss: 8.1606\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 8.2146 - val_loss: 8.1228\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 8.1779 - val_loss: 8.0830\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.1396 - val_loss: 8.0422\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0996 - val_loss: 7.9979\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.0583 - val_loss: 7.9531\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.0156 - val_loss: 7.9056\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 7.9719 - val_loss: 7.8581\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 7.9275 - val_loss: 7.8090\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7.8826 - val_loss: 7.7601\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.8375 - val_loss: 7.7104\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 7.7921 - val_loss: 7.6609\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.7466 - val_loss: 7.6110\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.7007 - val_loss: 7.5614\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.6546 - val_loss: 7.5112\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.6080 - val_loss: 7.4612\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.5611 - val_loss: 7.4105\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.5137 - val_loss: 7.3600\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.4657 - val_loss: 7.3076\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.4172 - val_loss: 7.2567\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.3681 - val_loss: 7.2015\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.3188 - val_loss: 7.1522\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.2690 - val_loss: 7.0938\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.2196 - val_loss: 7.0491\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 7.1701 - val_loss: 6.9882\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.1231 - val_loss: 6.9519\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7.0759 - val_loss: 6.8857\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.0289 - val_loss: 6.8594\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.9855 - val_loss: 6.7846\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 6.9361 - val_loss: 6.7711\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.8991 - val_loss: 6.6894\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 6.8507 - val_loss: 6.6944\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.8235 - val_loss: 6.5982\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.7705 - val_loss: 6.6290\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 6.7588 - val_loss: 6.5075\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 6.6914 - val_loss: 6.5622\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 6.6932 - val_loss: 6.4084\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 6.6024 - val_loss: 6.4622\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.5960 - val_loss: 6.2874\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.4885 - val_loss: 6.3292\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.4679 - val_loss: 6.1504\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.3570 - val_loss: 6.1801\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 6.3255 - val_loss: 6.0097\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.2216 - val_loss: 6.0231\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 6.1767 - val_loss: 5.8655\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.0825 - val_loss: 5.8668\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 6.0292 - val_loss: 5.7200\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 5.9416 - val_loss: 5.7145\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.8857 - val_loss: 5.5798\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 5.8062 - val_loss: 5.5684\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.7482 - val_loss: 5.4460\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 5.6776 - val_loss: 5.4360\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.6224 - val_loss: 5.3219\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 5.5580 - val_loss: 5.3151\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 5.5063 - val_loss: 5.2073\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 5.4477 - val_loss: 5.2064\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 5.4006 - val_loss: 5.1057\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.3512 - val_loss: 5.1125\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.3079 - val_loss: 5.0207\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.2732 - val_loss: 5.0455\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.2376 - val_loss: 4.9473\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.2070 - val_loss: 4.9960\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.1811 - val_loss: 4.8819\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 5.1491 - val_loss: 4.9389\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.1181 - val_loss: 4.8049\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 5.0792 - val_loss: 4.8662\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.0414 - val_loss: 4.7181\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 4.9991 - val_loss: 4.7787\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.9520 - val_loss: 4.6235\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.9102 - val_loss: 4.6847\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 4.8580 - val_loss: 4.5266\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 4.8185 - val_loss: 4.5914\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.7659 - val_loss: 4.4343\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.7308 - val_loss: 4.5062\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.6819 - val_loss: 4.3484\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.6492 - val_loss: 4.4273\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.6045 - val_loss: 4.2678\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.5725 - val_loss: 4.3519\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.5314 - val_loss: 4.1928\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.5011 - val_loss: 4.2810\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.4634 - val_loss: 4.1232\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.4350 - val_loss: 4.2121\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.3980 - val_loss: 4.0558\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.3704 - val_loss: 4.1460\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.3361 - val_loss: 3.9928\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.3100 - val_loss: 4.0857\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.2798 - val_loss: 3.9358\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.2557 - val_loss: 4.0300\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4.2281 - val_loss: 3.8843\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.2060 - val_loss: 3.9784\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.1802 - val_loss: 3.8372\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 4.1600 - val_loss: 3.9300\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.1354 - val_loss: 3.7941\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.1168 - val_loss: 3.8826\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.0924 - val_loss: 3.7546\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.0757 - val_loss: 3.8388\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 4.0523 - val_loss: 3.7208\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.0379 - val_loss: 3.8009\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.0168 - val_loss: 3.6928\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.0046 - val_loss: 3.7691\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.9864 - val_loss: 3.6717\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.9777 - val_loss: 3.7518\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.9679 - val_loss: 3.6561\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.9564 - val_loss: 3.7289\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.9438 - val_loss: 3.6357\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.9296 - val_loss: 3.7021\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.9162 - val_loss: 3.6133\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.9003 - val_loss: 3.6730\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.8862 - val_loss: 3.5892\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.8691 - val_loss: 3.6424\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.8550 - val_loss: 3.5652\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.8379 - val_loss: 3.6121\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.8242 - val_loss: 3.5432\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.8085 - val_loss: 3.5849\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.7960 - val_loss: 3.5242\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.7819 - val_loss: 3.5609\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.7704 - val_loss: 3.5080\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.7579 - val_loss: 3.5398\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.7472 - val_loss: 3.4940\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.7361 - val_loss: 3.5211\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7258 - val_loss: 3.4813\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.7156 - val_loss: 3.5035\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.7052 - val_loss: 3.4686\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6951 - val_loss: 3.4864\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6840 - val_loss: 3.4543\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.6734 - val_loss: 3.4705\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6635 - val_loss: 3.4396\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.6518 - val_loss: 3.4545\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.6432 - val_loss: 3.4242\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.6302 - val_loss: 3.4379\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.6228 - val_loss: 3.4101\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.6100 - val_loss: 3.4224\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.6033 - val_loss: 3.3967\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.5909 - val_loss: 3.4078\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.5850 - val_loss: 3.3841\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.5729 - val_loss: 3.3943\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5679 - val_loss: 3.3723\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.5562 - val_loss: 3.3817\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.5518 - val_loss: 3.3612\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.5406 - val_loss: 3.3696\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.5363 - val_loss: 3.3513\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.5264 - val_loss: 3.3581\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.5217 - val_loss: 3.3419\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.5131 - val_loss: 3.3467\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.5073 - val_loss: 3.3298\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.4975 - val_loss: 3.3329\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.4912 - val_loss: 3.3141\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.4793 - val_loss: 3.3166\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.4735 - val_loss: 3.2984\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.4611 - val_loss: 3.3009\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.4565 - val_loss: 3.2843\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.4448 - val_loss: 3.2871\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.4418 - val_loss: 3.2730\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4313 - val_loss: 3.2751\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.4289 - val_loss: 3.2628\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.4188 - val_loss: 3.2635\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 3.4165 - val_loss: 3.2519\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.4059 - val_loss: 3.2529\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.4051 - val_loss: 3.2412\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.3932 - val_loss: 3.2421\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.3935 - val_loss: 3.2307\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.3807 - val_loss: 3.2308\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.3816 - val_loss: 3.2203\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 3.3684 - val_loss: 3.2189\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.3692 - val_loss: 3.2098\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.3560 - val_loss: 3.2064\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.3560 - val_loss: 3.1988\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.3432 - val_loss: 3.1933\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.3424 - val_loss: 3.1873\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.3302 - val_loss: 3.1801\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.3288 - val_loss: 3.1756\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.3174 - val_loss: 3.1661\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.3146 - val_loss: 3.1630\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.3040 - val_loss: 3.1514\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2997 - val_loss: 3.1453\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2861 - val_loss: 3.1316\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.2802 - val_loss: 3.1287\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.2695 - val_loss: 3.1141\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2633 - val_loss: 3.1158\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2562 - val_loss: 3.1005\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2504 - val_loss: 3.1010\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2410 - val_loss: 3.0835\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2340 - val_loss: 3.0871\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.2263 - val_loss: 3.0705\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.2212 - val_loss: 3.0782\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2163 - val_loss: 3.0618\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2128 - val_loss: 3.0731\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2099 - val_loss: 3.0566\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.2076 - val_loss: 3.0709\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2060 - val_loss: 3.0540\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.2049 - val_loss: 3.0708\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.2040 - val_loss: 3.0538\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.2045 - val_loss: 3.0725\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.2037 - val_loss: 3.0613\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2118 - val_loss: 3.0812\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.2102 - val_loss: 3.0691\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2194 - val_loss: 3.0964\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.2231 - val_loss: 3.0834\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.2336 - val_loss: 3.1159\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.2399 - val_loss: 3.0974\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2467 - val_loss: 3.1322\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2530 - val_loss: 3.1034\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.2511 - val_loss: 3.1349\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.2535 - val_loss: 3.0985\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.2452 - val_loss: 3.1262\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.2441 - val_loss: 3.0874\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.2337 - val_loss: 3.1126\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.2306 - val_loss: 3.0749\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2215 - val_loss: 3.0975\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2161 - val_loss: 3.0626\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2097 - val_loss: 3.0838\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.2032 - val_loss: 3.0516\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1992 - val_loss: 3.0726\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.1927 - val_loss: 3.0418\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.1898 - val_loss: 3.0634\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.1840 - val_loss: 3.0304\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1784 - val_loss: 3.0522\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1733 - val_loss: 3.0208\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.1690 - val_loss: 3.0436\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.1651 - val_loss: 3.0126\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1608 - val_loss: 3.0321\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1541 - val_loss: 3.0003\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1483 - val_loss: 3.0178\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1405 - val_loss: 2.9874\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.1351 - val_loss: 3.0054\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1286 - val_loss: 2.9760\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1231 - val_loss: 2.9954\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1187 - val_loss: 2.9661\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.1122 - val_loss: 2.9867\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1099 - val_loss: 2.9567\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1015 - val_loss: 2.9782\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.1013 - val_loss: 2.9464\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.0895 - val_loss: 2.9680\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0909 - val_loss: 2.9348\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0767 - val_loss: 2.9564\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0796 - val_loss: 2.9240\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0653 - val_loss: 2.9459\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0695 - val_loss: 2.9154\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0566 - val_loss: 2.9382\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0620 - val_loss: 2.9093\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0506 - val_loss: 2.9334\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0571 - val_loss: 2.9056\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0467 - val_loss: 2.9322\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0555 - val_loss: 2.9043\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0454 - val_loss: 2.9335\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0564 - val_loss: 2.9048\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0460 - val_loss: 2.9364\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0583 - val_loss: 2.9075\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0483 - val_loss: 2.9414\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0618 - val_loss: 2.9145\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0549 - val_loss: 2.9504\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0690 - val_loss: 2.9262\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0663 - val_loss: 2.9657\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0819 - val_loss: 2.9393\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0791 - val_loss: 2.9806\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0940 - val_loss: 2.9508\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.0902 - val_loss: 2.9910\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1020 - val_loss: 2.9569\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.0959 - val_loss: 2.9956\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.1047 - val_loss: 2.9579\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.0963 - val_loss: 2.9954\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1033 - val_loss: 2.9552\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.0928 - val_loss: 2.9914\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0985 - val_loss: 2.9497\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0866 - val_loss: 2.9847\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0915 - val_loss: 2.9423\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0786 - val_loss: 2.9765\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.0834 - val_loss: 2.9342\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 3.0700 - val_loss: 2.9682\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0752 - val_loss: 2.9263\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.0618 - val_loss: 2.9604\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0676 - val_loss: 2.9190\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0542 - val_loss: 2.9534\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0608 - val_loss: 2.9125\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0476 - val_loss: 2.9472\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.0548 - val_loss: 2.9068\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.0417 - val_loss: 2.9418\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0494 - val_loss: 2.9018\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3.0366 - val_loss: 2.9372\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0447 - val_loss: 2.8975\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.0320 - val_loss: 2.9330\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0405 - val_loss: 2.8936\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.0278 - val_loss: 2.9293\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.0365 - val_loss: 2.8900\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.0239 - val_loss: 2.9256\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0327 - val_loss: 2.8865\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.0200 - val_loss: 2.9220\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.0289 - val_loss: 2.8830\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 3.0161 - val_loss: 2.9183\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0249 - val_loss: 2.8794\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0120 - val_loss: 2.9143\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0208 - val_loss: 2.8757\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0078 - val_loss: 2.9101\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.0166 - val_loss: 2.8719\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0036 - val_loss: 2.9059\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 3.0124 - val_loss: 2.8682\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9995 - val_loss: 2.9019\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.0084 - val_loss: 2.8646\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9955 - val_loss: 2.8980\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0045 - val_loss: 2.8612\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9917 - val_loss: 2.8944\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0009 - val_loss: 2.8580\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9881 - val_loss: 2.8910\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9974 - val_loss: 2.8549\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9845 - val_loss: 2.8877\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9940 - val_loss: 2.8517\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9810 - val_loss: 2.8844\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9906 - val_loss: 2.8485\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.9773 - val_loss: 2.8809\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9870 - val_loss: 2.8452\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.9735 - val_loss: 2.8773\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9834 - val_loss: 2.8419\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.9698 - val_loss: 2.8738\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9799 - val_loss: 2.8389\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9664 - val_loss: 2.8706\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9766 - val_loss: 2.8362\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.9634 - val_loss: 2.8680\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9738 - val_loss: 2.8341\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9610 - val_loss: 2.8663\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9718 - val_loss: 2.8328\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9594 - val_loss: 2.8656\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9706 - val_loss: 2.8324\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9587 - val_loss: 2.8660\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9704 - val_loss: 2.8330\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9591 - val_loss: 2.8676\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9711 - val_loss: 2.8348\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9606 - val_loss: 2.8702\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 2.9726 - val_loss: 2.8374\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9629 - val_loss: 2.8732\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9746 - val_loss: 2.8402\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9653 - val_loss: 2.8760\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9763 - val_loss: 2.8424\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9670 - val_loss: 2.8779\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9772 - val_loss: 2.8435\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9676 - val_loss: 2.8786\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9770 - val_loss: 2.8435\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.9670 - val_loss: 2.8781\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9758 - val_loss: 2.8426\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9655 - val_loss: 2.8767\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.9739 - val_loss: 2.8409\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9632 - val_loss: 2.8746\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9713 - val_loss: 2.8386\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9604 - val_loss: 2.8721\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.9684 - val_loss: 2.8360\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9573 - val_loss: 2.8692\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9653 - val_loss: 2.8331\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9540 - val_loss: 2.8661\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9620 - val_loss: 2.8301\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9506 - val_loss: 2.8629\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9587 - val_loss: 2.8270\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9470 - val_loss: 2.8595\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9551 - val_loss: 2.8236\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9432 - val_loss: 2.8559\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9514 - val_loss: 2.8199\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9390 - val_loss: 2.8517\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9473 - val_loss: 2.8147\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9329 - val_loss: 2.8451\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9410 - val_loss: 2.8073\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9247 - val_loss: 2.8350\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9319 - val_loss: 2.7990\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9158 - val_loss: 2.8241\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9221 - val_loss: 2.7909\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9072 - val_loss: 2.8142\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9132 - val_loss: 2.7836\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8995 - val_loss: 2.8048\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.9046 - val_loss: 2.7767\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8922 - val_loss: 2.7941\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8946 - val_loss: 2.7693\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8841 - val_loss: 2.7838\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8851 - val_loss: 2.7621\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8764 - val_loss: 2.7742\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8761 - val_loss: 2.7551\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8683 - val_loss: 2.7652\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8673 - val_loss: 2.7480\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8602 - val_loss: 2.7563\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8586 - val_loss: 2.7411\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8521 - val_loss: 2.7477\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8502 - val_loss: 2.7351\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8451 - val_loss: 2.7407\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8430 - val_loss: 2.7302\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8391 - val_loss: 2.7355\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8374 - val_loss: 2.7266\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8343 - val_loss: 2.7315\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8330 - val_loss: 2.7236\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8304 - val_loss: 2.7284\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8295 - val_loss: 2.7212\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8273 - val_loss: 2.7260\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8266 - val_loss: 2.7194\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8248 - val_loss: 2.7243\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8244 - val_loss: 2.7181\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8230 - val_loss: 2.7233\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8229 - val_loss: 2.7173\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8217 - val_loss: 2.7230\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8220 - val_loss: 2.7173\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8211 - val_loss: 2.7236\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8218 - val_loss: 2.7179\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8212 - val_loss: 2.7253\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8225 - val_loss: 2.7195\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8221 - val_loss: 2.7281\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8242 - val_loss: 2.7217\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8236 - val_loss: 2.7318\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8268 - val_loss: 2.7245\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8259 - val_loss: 2.7379\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8319 - val_loss: 2.7296\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.8310 - val_loss: 2.7486\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8417 - val_loss: 2.7420\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8445 - val_loss: 2.7776\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8677 - val_loss: 2.7727\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8777 - val_loss: 2.8291\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9134 - val_loss: 2.8199\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9250 - val_loss: 2.8794\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9574 - val_loss: 2.8600\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9632 - val_loss: 2.9132\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9862 - val_loss: 2.8863\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9867 - val_loss: 2.9268\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9963 - val_loss: 2.8891\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9882 - val_loss: 2.9202\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9891 - val_loss: 2.8774\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9759 - val_loss: 2.9050\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9747 - val_loss: 2.8558\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9549 - val_loss: 2.8855\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9572 - val_loss: 2.8415\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9410 - val_loss: 2.8717\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.9446 - val_loss: 2.8303\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9300 - val_loss: 2.8605\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9343 - val_loss: 2.8196\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.9191 - val_loss: 2.8503\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9246 - val_loss: 2.8106\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.9101 - val_loss: 2.8422\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9170 - val_loss: 2.8037\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9032 - val_loss: 2.8360\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9111 - val_loss: 2.7984\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8978 - val_loss: 2.8312\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.9064 - val_loss: 2.7942\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8935 - val_loss: 2.8272\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9024 - val_loss: 2.7907\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8899 - val_loss: 2.8234\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8987 - val_loss: 2.7875\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8863 - val_loss: 2.8197\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8949 - val_loss: 2.7843\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8828 - val_loss: 2.8162\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8912 - val_loss: 2.7813\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8795 - val_loss: 2.8131\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8880 - val_loss: 2.7788\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8766 - val_loss: 2.8108\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8855 - val_loss: 2.7769\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8744 - val_loss: 2.8094\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8838 - val_loss: 2.7757\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8728 - val_loss: 2.8086\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8826 - val_loss: 2.7749\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8716 - val_loss: 2.8080\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8814 - val_loss: 2.7742\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8704 - val_loss: 2.8074\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8802 - val_loss: 2.7735\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8692 - val_loss: 2.8065\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8788 - val_loss: 2.7726\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8677 - val_loss: 2.8055\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8772 - val_loss: 2.7715\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8661 - val_loss: 2.8043\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8755 - val_loss: 2.7703\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8643 - val_loss: 2.8029\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8736 - val_loss: 2.7690\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8624 - val_loss: 2.8014\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.8715 - val_loss: 2.7676\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8604 - val_loss: 2.7998\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8695 - val_loss: 2.7662\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8584 - val_loss: 2.7981\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8674 - val_loss: 2.7647\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8563 - val_loss: 2.7965\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8652 - val_loss: 2.7633\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8543 - val_loss: 2.7949\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.8631 - val_loss: 2.7618\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8523 - val_loss: 2.7933\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8611 - val_loss: 2.7604\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8503 - val_loss: 2.7918\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8590 - val_loss: 2.7591\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8484 - val_loss: 2.7903\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8571 - val_loss: 2.7578\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.8465 - val_loss: 2.7890\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8552 - val_loss: 2.7567\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8447 - val_loss: 2.7878\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8535 - val_loss: 2.7556\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8430 - val_loss: 2.7866\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8518 - val_loss: 2.7545\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8414 - val_loss: 2.7856\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8501 - val_loss: 2.7536\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8398 - val_loss: 2.7846\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8486 - val_loss: 2.7527\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8383 - val_loss: 2.7837\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8471 - val_loss: 2.7519\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8369 - val_loss: 2.7829\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8457 - val_loss: 2.7511\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8356 - val_loss: 2.7821\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8444 - val_loss: 2.7504\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8342 - val_loss: 2.7814\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8431 - val_loss: 2.7497\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8329 - val_loss: 2.7807\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8418 - val_loss: 2.7491\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8317 - val_loss: 2.7800\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8405 - val_loss: 2.7484\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8305 - val_loss: 2.7794\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8393 - val_loss: 2.7478\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8292 - val_loss: 2.7787\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8380 - val_loss: 2.7472\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8280 - val_loss: 2.7781\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8368 - val_loss: 2.7465\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8268 - val_loss: 2.7774\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8355 - val_loss: 2.7459\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8255 - val_loss: 2.7767\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8343 - val_loss: 2.7452\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.8243 - val_loss: 2.7759\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8329 - val_loss: 2.7444\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8229 - val_loss: 2.7751\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8316 - val_loss: 2.7437\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8216 - val_loss: 2.7742\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8301 - val_loss: 2.7428\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8201 - val_loss: 2.7732\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8286 - val_loss: 2.7419\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8186 - val_loss: 2.7722\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8271 - val_loss: 2.7409\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8170 - val_loss: 2.7710\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8254 - val_loss: 2.7397\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8153 - val_loss: 2.7697\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8235 - val_loss: 2.7385\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8135 - val_loss: 2.7682\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8215 - val_loss: 2.7370\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8114 - val_loss: 2.7664\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8193 - val_loss: 2.7353\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8090 - val_loss: 2.7641\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8165 - val_loss: 2.7330\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8061 - val_loss: 2.7610\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8131 - val_loss: 2.7297\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8021 - val_loss: 2.7564\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8082 - val_loss: 2.7246\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7963 - val_loss: 2.7493\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8012 - val_loss: 2.7175\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.7885 - val_loss: 2.7398\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7923 - val_loss: 2.7101\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7807 - val_loss: 2.7292\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7822 - val_loss: 2.7020\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7725 - val_loss: 2.7192\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7726 - val_loss: 2.6952\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.7652 - val_loss: 2.7139\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7668 - val_loss: 2.6916\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7609 - val_loss: 2.7108\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7629 - val_loss: 2.6897\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7580 - val_loss: 2.7094\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7607 - val_loss: 2.6889\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7563 - val_loss: 2.7091\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7596 - val_loss: 2.6888\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7554 - val_loss: 2.7094\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7591 - val_loss: 2.6892\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7550 - val_loss: 2.7103\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7592 - val_loss: 2.6899\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7551 - val_loss: 2.7115\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7598 - val_loss: 2.6910\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7555 - val_loss: 2.7130\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7605 - val_loss: 2.6922\n"
     ]
    }
   ],
   "source": [
    "historyVal_LM = []\n",
    "historyTr_LM = []\n",
    "\n",
    "#mc = ModelCheckpoint('best_modelLC2HL.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_LM()\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, batch_size=1036).history\n",
    "    historyVal_LM.append(history['val_loss'])\n",
    "    historyTr_LM.append(history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_LM_mean=np.mean(historyVal_LM, axis=0)\n",
    "historyTr_LM_mean=np.mean(historyTr_LM, axis=0)\n",
    "\n",
    "historyVal_LM_sd=np.std(historyVal_LM, axis=0)\n",
    "historyTr_LM_sd=np.std(historyTr_LM, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_LM_mean)),historyTr_LM_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_LM_mean)), historyTr_LM_mean - historyTr_LM_sd, \n",
    "                 historyTr_LM_mean + historyTr_LM_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_LM_mean)), historyVal_LM_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_LM_mean)), historyVal_LM_mean - historyVal_LM_sd, \n",
    "                 historyVal_LM_mean + historyVal_LM_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,14)\n",
    "plt.xlim(-5,600)\n",
    "plt.yticks(np.arange(0, 14, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low momentum (mom=0.01) result:\n",
      "MEE on the validation 2.92079701423645 with standard deviation 0.13706766883313531\n",
      "MEE on the training 2.7291064262390137 with standard deviation 0.0289291966534779\n"
     ]
    }
   ],
   "source": [
    "print(\"Low momentum (mom=0.01) result:\")\n",
    "print(\"MEE on the validation\",historyVal_LM_mean[-1],\"with standard deviation\",historyVal_LM_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_LM_mean[-1],\"with standard deviation\",historyTr_LM_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ES():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=10, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(25, activation=mlpr.best_params_['act'], \n",
    "                    kernel_regularizer=l2(mlpr.best_params_['alpha']),\n",
    "                    kernel_initializer=RandomUniform(minval=-0.7, maxval=0.7, seed=1)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss=[MEE_k], optimizer= SGD(lr=mlpr.best_params_['lr'], \n",
    "                                                            momentum=mlpr.best_params_['mom']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 56.5841 - val_loss: 55.0912\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 55.1750 - val_loss: 52.8561\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 52.9416 - val_loss: 49.9200\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 50.0089 - val_loss: 46.2256\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 46.3187 - val_loss: 41.7820\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 41.8788 - val_loss: 36.7501\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 36.8519 - val_loss: 31.3842\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 31.5063 - val_loss: 26.1761\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 26.3767 - val_loss: 22.1741\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 22.4745 - val_loss: 19.7276\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 20.0575 - val_loss: 18.0866\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 18.4327 - val_loss: 17.0133\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 17.3656 - val_loss: 16.3222\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 16.6809 - val_loss: 15.8444\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 16.2079 - val_loss: 15.4543\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 15.8153 - val_loss: 15.0795\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 15.4315 - val_loss: 14.6546\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 14.9922 - val_loss: 14.0612\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 14.3762 - val_loss: 13.1236\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 13.4015 - val_loss: 11.7782\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 12.0186 - val_loss: 10.6632\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 10.8637 - val_loss: 9.8443\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 10.0016 - val_loss: 9.1435\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.2730 - val_loss: 8.7170\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 8.8195 - val_loss: 8.5271\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.6283 - val_loss: 8.3805\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 8.4948 - val_loss: 8.2511\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.3698 - val_loss: 8.1336\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.2524 - val_loss: 8.0309\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 8.1481 - val_loss: 7.9313\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 8.0468 - val_loss: 7.8320\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 7.9464 - val_loss: 7.7373\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.8493 - val_loss: 7.6472\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.7553 - val_loss: 7.5599\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 7.6630 - val_loss: 7.4733\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 7.5706 - val_loss: 7.3868\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 7.4776 - val_loss: 7.3001\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7.3841 - val_loss: 7.2128\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 7.2896 - val_loss: 7.1245\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 7.1940 - val_loss: 7.0354\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.0972 - val_loss: 6.9450\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 6.9993 - val_loss: 6.8534\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.9002 - val_loss: 6.7605\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.7999 - val_loss: 6.6665\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 6.6985 - val_loss: 6.5714\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 6.5960 - val_loss: 6.4754\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.4927 - val_loss: 6.3786\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.3885 - val_loss: 6.2812\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 6.2837 - val_loss: 6.1830\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.1782 - val_loss: 6.0843\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.0722 - val_loss: 5.9848\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.9655 - val_loss: 5.8845\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 5.8581 - val_loss: 5.7837\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.7500 - val_loss: 5.6821\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 5.6411 - val_loss: 5.5796\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.5315 - val_loss: 5.4769\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.4214 - val_loss: 5.3739\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 5.3108 - val_loss: 5.2709\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 5.2000 - val_loss: 5.1684\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 5.0893 - val_loss: 5.0665\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 4.9790 - val_loss: 4.9655\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 4.8700 - val_loss: 4.8665\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.7630 - val_loss: 4.7705\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.6594 - val_loss: 4.6781\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.5599 - val_loss: 4.5907\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.4649 - val_loss: 4.5092\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.3760 - val_loss: 4.4357\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 4.2930 - val_loss: 4.3691\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 4.2166 - val_loss: 4.3087\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.1478 - val_loss: 4.2537\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 4.0860 - val_loss: 4.2039\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.0303 - val_loss: 4.1586\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.9798 - val_loss: 4.1174\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.9337 - val_loss: 4.0796\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.8913 - val_loss: 4.0441\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.8522 - val_loss: 4.0116\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.8161 - val_loss: 3.9813\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.7826 - val_loss: 3.9533\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.7514 - val_loss: 3.9269\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.7220 - val_loss: 3.9014\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.6943 - val_loss: 3.8769\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.6683 - val_loss: 3.8533\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.6437 - val_loss: 3.8291\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.6205 - val_loss: 3.8062\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.5984 - val_loss: 3.7858\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.5772 - val_loss: 3.7655\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.5567 - val_loss: 3.7465\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.5370 - val_loss: 3.7274\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5178 - val_loss: 3.7083\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4991 - val_loss: 3.6896\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.4808 - val_loss: 3.6712\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.4629 - val_loss: 3.6531\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.4454 - val_loss: 3.6352\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.4284 - val_loss: 3.6178\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.4119 - val_loss: 3.6009\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.3959 - val_loss: 3.5841\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.3803 - val_loss: 3.5681\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.3650 - val_loss: 3.5525\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3502 - val_loss: 3.5370\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.3357 - val_loss: 3.5224\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.3216 - val_loss: 3.5081\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3079 - val_loss: 3.4941\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2944 - val_loss: 3.4809\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.2813 - val_loss: 3.4685\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.2686 - val_loss: 3.4556\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.2564 - val_loss: 3.4434\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.2445 - val_loss: 3.4318\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.2330 - val_loss: 3.4197\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.2219 - val_loss: 3.4083\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.2111 - val_loss: 3.3972\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.2007 - val_loss: 3.3862\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1906 - val_loss: 3.3758\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1808 - val_loss: 3.3655\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1712 - val_loss: 3.3556\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.1620 - val_loss: 3.3459\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.1529 - val_loss: 3.3364\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1440 - val_loss: 3.3273\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1353 - val_loss: 3.3183\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.1268 - val_loss: 3.3094\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.1184 - val_loss: 3.3005\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1103 - val_loss: 3.2918\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.1025 - val_loss: 3.2833\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0949 - val_loss: 3.2750\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0875 - val_loss: 3.2672\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0804 - val_loss: 3.2587\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.0737 - val_loss: 3.2522\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0670 - val_loss: 3.2435\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.0605 - val_loss: 3.2370\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.0541 - val_loss: 3.2292\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.0479 - val_loss: 3.2229\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.0419 - val_loss: 3.2159\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0361 - val_loss: 3.2095\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.0304 - val_loss: 3.2029\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0249 - val_loss: 3.1967\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.0195 - val_loss: 3.1905\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0142 - val_loss: 3.1844\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0091 - val_loss: 3.1784\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0041 - val_loss: 3.1725\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9991 - val_loss: 3.1667\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9943 - val_loss: 3.1610\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9896 - val_loss: 3.1554\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9850 - val_loss: 3.1500\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9806 - val_loss: 3.1448\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9762 - val_loss: 3.1399\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9720 - val_loss: 3.1348\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9678 - val_loss: 3.1301\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9638 - val_loss: 3.1255\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.9598 - val_loss: 3.1209\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9559 - val_loss: 3.1164\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9521 - val_loss: 3.1121\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9484 - val_loss: 3.1080\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9448 - val_loss: 3.1040\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.9412 - val_loss: 3.1001\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9376 - val_loss: 3.0963\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.9341 - val_loss: 3.0926\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.9307 - val_loss: 3.0888\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9272 - val_loss: 3.0850\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9239 - val_loss: 3.0814\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9206 - val_loss: 3.0776\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9173 - val_loss: 3.0743\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9140 - val_loss: 3.0705\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9108 - val_loss: 3.0674\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9077 - val_loss: 3.0636\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9046 - val_loss: 3.0608\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.9015 - val_loss: 3.0571\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8985 - val_loss: 3.0545\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8955 - val_loss: 3.0510\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8925 - val_loss: 3.0485\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8895 - val_loss: 3.0450\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8866 - val_loss: 3.0425\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8837 - val_loss: 3.0392\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8809 - val_loss: 3.0367\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8781 - val_loss: 3.0333\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.8754 - val_loss: 3.0310\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8728 - val_loss: 3.0275\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8701 - val_loss: 3.0254\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8676 - val_loss: 3.0218\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8651 - val_loss: 3.0203\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8626 - val_loss: 3.0164\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8603 - val_loss: 3.0162\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8585 - val_loss: 3.0116\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8568 - val_loss: 3.0134\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8555 - val_loss: 3.0074\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8542 - val_loss: 3.0108\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8530 - val_loss: 3.0034\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8513 - val_loss: 3.0073\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8497 - val_loss: 2.9986\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8471 - val_loss: 3.0012\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8435 - val_loss: 2.9931\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8405 - val_loss: 2.9954\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8377 - val_loss: 2.9884\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8352 - val_loss: 2.9901\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.8323 - val_loss: 2.9835\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8298 - val_loss: 2.9852\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8274 - val_loss: 2.9791\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8253 - val_loss: 2.9809\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.8232 - val_loss: 2.9748\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.8212 - val_loss: 2.9770\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8192 - val_loss: 2.9706\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8173 - val_loss: 2.9732\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8154 - val_loss: 2.9664\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8135 - val_loss: 2.9694\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8117 - val_loss: 2.9622\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8097 - val_loss: 2.9655\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8079 - val_loss: 2.9579\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8058 - val_loss: 2.9614\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8040 - val_loss: 2.9537\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8018 - val_loss: 2.9573\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8000 - val_loss: 2.9496\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7978 - val_loss: 2.9532\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7961 - val_loss: 2.9456\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7938 - val_loss: 2.9493\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7923 - val_loss: 2.9417\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7900 - val_loss: 2.9455\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7886 - val_loss: 2.9378\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7863 - val_loss: 2.9416\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7850 - val_loss: 2.9339\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7826 - val_loss: 2.9378\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7814 - val_loss: 2.9300\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7790 - val_loss: 2.9341\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7779 - val_loss: 2.9262\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7756 - val_loss: 2.9307\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7748 - val_loss: 2.9225\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7727 - val_loss: 2.9291\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7742 - val_loss: 2.9201\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7735 - val_loss: 2.9283\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7747 - val_loss: 2.9164\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7710 - val_loss: 2.9230\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7701 - val_loss: 2.9116\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7663 - val_loss: 2.9180\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7657 - val_loss: 2.9077\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7628 - val_loss: 2.9146\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7628 - val_loss: 2.9042\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7599 - val_loss: 2.9106\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7592 - val_loss: 2.9010\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7560 - val_loss: 2.9068\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7552 - val_loss: 2.8976\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7527 - val_loss: 2.9032\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7518 - val_loss: 2.8941\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7494 - val_loss: 2.8997\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7486 - val_loss: 2.8908\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7463 - val_loss: 2.8966\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7455 - val_loss: 2.8879\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7435 - val_loss: 2.8940\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7430 - val_loss: 2.8854\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7414 - val_loss: 2.8918\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7412 - val_loss: 2.8829\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7396 - val_loss: 2.8896\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7395 - val_loss: 2.8800\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7375 - val_loss: 2.8865\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7370 - val_loss: 2.8769\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7346 - val_loss: 2.8832\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7338 - val_loss: 2.8741\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7316 - val_loss: 2.8802\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7310 - val_loss: 2.8715\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7289 - val_loss: 2.8776\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7286 - val_loss: 2.8687\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7262 - val_loss: 2.8746\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7257 - val_loss: 2.8658\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7228 - val_loss: 2.8712\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7220 - val_loss: 2.8631\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7186 - val_loss: 2.8665\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7164 - val_loss: 2.8602\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7135 - val_loss: 2.8625\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7117 - val_loss: 2.8580\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7096 - val_loss: 2.8595\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7084 - val_loss: 2.8558\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7063 - val_loss: 2.8567\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7051 - val_loss: 2.8540\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7031 - val_loss: 2.8548\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7019 - val_loss: 2.8525\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7003 - val_loss: 2.8530\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6994 - val_loss: 2.8506\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6979 - val_loss: 2.8509\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6971 - val_loss: 2.8485\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6957 - val_loss: 2.8491\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6951 - val_loss: 2.8468\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6940 - val_loss: 2.8483\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6944 - val_loss: 2.8456\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6949 - val_loss: 2.8499\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6979 - val_loss: 2.8441\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6983 - val_loss: 2.8518\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7017 - val_loss: 2.8428\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7003 - val_loss: 2.8529\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7027 - val_loss: 2.8420\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6985 - val_loss: 2.8494\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6985 - val_loss: 2.8400\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6951 - val_loss: 2.8466\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6963 - val_loss: 2.8372\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6929 - val_loss: 2.8435\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6935 - val_loss: 2.8346\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6898 - val_loss: 2.8403\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6897 - val_loss: 2.8330\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6867 - val_loss: 2.8370\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6861 - val_loss: 2.8304\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6828 - val_loss: 2.8325\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6813 - val_loss: 2.8274\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6786 - val_loss: 2.8292\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6772 - val_loss: 2.8259\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6755 - val_loss: 2.8274\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6746 - val_loss: 2.8253\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6738 - val_loss: 2.8266\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6738 - val_loss: 2.8241\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6731 - val_loss: 2.8258\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6733 - val_loss: 2.8224\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6719 - val_loss: 2.8242\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6715 - val_loss: 2.8211\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6697 - val_loss: 2.8221\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6689 - val_loss: 2.8197\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6674 - val_loss: 2.8202\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6669 - val_loss: 2.8182\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6655 - val_loss: 2.8187\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6652 - val_loss: 2.8168\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6638 - val_loss: 2.8172\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6636 - val_loss: 2.8155\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6621 - val_loss: 2.8157\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6619 - val_loss: 2.8142\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6604 - val_loss: 2.8141\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6603 - val_loss: 2.8130\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6588 - val_loss: 2.8124\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6585 - val_loss: 2.8115\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6569 - val_loss: 2.8098\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6558 - val_loss: 2.8094\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6542 - val_loss: 2.8073\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6529 - val_loss: 2.8079\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6510 - val_loss: 2.8044\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6498 - val_loss: 2.8066\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6485 - val_loss: 2.8026\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6478 - val_loss: 2.8054\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6470 - val_loss: 2.8016\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6466 - val_loss: 2.8042\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6458 - val_loss: 2.8006\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6456 - val_loss: 2.8034\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6452 - val_loss: 2.8002\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6452 - val_loss: 2.8028\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6448 - val_loss: 2.8000\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6451 - val_loss: 2.8021\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6444 - val_loss: 2.8003\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6456 - val_loss: 2.8014\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6444 - val_loss: 2.8014\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6473 - val_loss: 2.8017\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6462 - val_loss: 2.8041\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6507 - val_loss: 2.8015\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6474 - val_loss: 2.8033\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6503 - val_loss: 2.7992\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6451 - val_loss: 2.7994\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6461 - val_loss: 2.7967\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.6409 - val_loss: 2.7946\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6409 - val_loss: 2.7939\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6357 - val_loss: 2.7886\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6341 - val_loss: 2.7914\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.6303 - val_loss: 2.7854\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6281 - val_loss: 2.7905\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6267 - val_loss: 2.7846\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6259 - val_loss: 2.7906\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6254 - val_loss: 2.7828\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6262 - val_loss: 2.7900\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6254 - val_loss: 2.7821\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6263 - val_loss: 2.7885\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6246 - val_loss: 2.7823\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6239 - val_loss: 2.7875\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6234 - val_loss: 2.7845\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6239 - val_loss: 2.7888\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6244 - val_loss: 2.7861\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6297 - val_loss: 2.7923\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6310 - val_loss: 2.7914\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6378 - val_loss: 2.7928\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6342 - val_loss: 2.7912\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6371 - val_loss: 2.7905\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6310 - val_loss: 2.7876\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6325 - val_loss: 2.7884\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.6272 - val_loss: 2.7839\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6288 - val_loss: 2.7863\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6241 - val_loss: 2.7810\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6253 - val_loss: 2.7835\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6205 - val_loss: 2.7793\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6198 - val_loss: 2.7817\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6166 - val_loss: 2.7783\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6166 - val_loss: 2.7819\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6144 - val_loss: 2.7748\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6170 - val_loss: 2.7821\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6144 - val_loss: 2.7709\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6151 - val_loss: 2.7779\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6102 - val_loss: 2.7694\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6076 - val_loss: 2.7750\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6048 - val_loss: 2.7712\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6037 - val_loss: 2.7765\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6032 - val_loss: 2.7692\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6044 - val_loss: 2.7771\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6040 - val_loss: 2.7675\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6063 - val_loss: 2.7770\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6058 - val_loss: 2.7689\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6092 - val_loss: 2.7776\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6095 - val_loss: 2.7768\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6158 - val_loss: 2.7809\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6142 - val_loss: 2.7810\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6215 - val_loss: 2.7859\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6212 - val_loss: 2.7828\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6263 - val_loss: 2.7906\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6252 - val_loss: 2.7813\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6247 - val_loss: 2.7865\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6195 - val_loss: 2.7774\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6186 - val_loss: 2.7806\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6137 - val_loss: 2.7739\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6147 - val_loss: 2.7784\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6099 - val_loss: 2.7713\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6110 - val_loss: 2.7751\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6056 - val_loss: 2.7680\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6062 - val_loss: 2.7716\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6013 - val_loss: 2.7662\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6023 - val_loss: 2.7700\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5985 - val_loss: 2.7642\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5992 - val_loss: 2.7677\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.5954 - val_loss: 2.7625\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5949 - val_loss: 2.7645\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5911 - val_loss: 2.7622\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5899 - val_loss: 2.7630\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5874 - val_loss: 2.7600\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5865 - val_loss: 2.7628\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5847 - val_loss: 2.7566\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5848 - val_loss: 2.7626\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5839 - val_loss: 2.7554\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5848 - val_loss: 2.7615\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5831 - val_loss: 2.7571\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5832 - val_loss: 2.7609\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5825 - val_loss: 2.7606\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5838 - val_loss: 2.7627\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5851 - val_loss: 2.7626\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5904 - val_loss: 2.7660\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5912 - val_loss: 2.7659\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6006 - val_loss: 2.7760\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6044 - val_loss: 2.7758\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6132 - val_loss: 2.7829\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6134 - val_loss: 2.7762\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6122 - val_loss: 2.7760\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6055 - val_loss: 2.7696\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6039 - val_loss: 2.7710\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5981 - val_loss: 2.7620\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5966 - val_loss: 2.7677\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5921 - val_loss: 2.7591\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5922 - val_loss: 2.7658\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5888 - val_loss: 2.7587\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5906 - val_loss: 2.7657\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5882 - val_loss: 2.7590\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5908 - val_loss: 2.7656\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5882 - val_loss: 2.7587\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5903 - val_loss: 2.7646\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5871 - val_loss: 2.7577\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5888 - val_loss: 2.7631\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5851 - val_loss: 2.7566\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5868 - val_loss: 2.7614\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5829 - val_loss: 2.7551\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.5846 - val_loss: 2.7599\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5808 - val_loss: 2.7536\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5827 - val_loss: 2.7588\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5790 - val_loss: 2.7525\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5811 - val_loss: 2.7579\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5776 - val_loss: 2.7517\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5798 - val_loss: 2.7571\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5764 - val_loss: 2.7511\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5789 - val_loss: 2.7566\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5756 - val_loss: 2.7507\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.5782 - val_loss: 2.7561\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5749 - val_loss: 2.7505\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5776 - val_loss: 2.7558\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5743 - val_loss: 2.7502\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5770 - val_loss: 2.7553\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5735 - val_loss: 2.7497\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5762 - val_loss: 2.7547\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5726 - val_loss: 2.7491\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.5751 - val_loss: 2.7541\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5714 - val_loss: 2.7483\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5739 - val_loss: 2.7533\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5701 - val_loss: 2.7475\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5725 - val_loss: 2.7525\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5688 - val_loss: 2.7466\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5712 - val_loss: 2.7518\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5675 - val_loss: 2.7458\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5699 - val_loss: 2.7510\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5663 - val_loss: 2.7451\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5687 - val_loss: 2.7504\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5651 - val_loss: 2.7445\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5675 - val_loss: 2.7498\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5640 - val_loss: 2.7439\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5664 - val_loss: 2.7492\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5630 - val_loss: 2.7434\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5655 - val_loss: 2.7487\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5621 - val_loss: 2.7429\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5645 - val_loss: 2.7483\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5612 - val_loss: 2.7425\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5637 - val_loss: 2.7480\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5605 - val_loss: 2.7420\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5631 - val_loss: 2.7479\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5601 - val_loss: 2.7419\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5635 - val_loss: 2.7492\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5615 - val_loss: 2.7437\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5663 - val_loss: 2.7510\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5650 - val_loss: 2.7477\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5700 - val_loss: 2.7523\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5687 - val_loss: 2.7508\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5723 - val_loss: 2.7517\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5680 - val_loss: 2.7488\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5697 - val_loss: 2.7498\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5647 - val_loss: 2.7450\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5655 - val_loss: 2.7481\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5612 - val_loss: 2.7421\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5622 - val_loss: 2.7465\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5579 - val_loss: 2.7384\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5577 - val_loss: 2.7441\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5529 - val_loss: 2.7348\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5522 - val_loss: 2.7396\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5463 - val_loss: 2.7323\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5440 - val_loss: 2.7337\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5392 - val_loss: 2.7313\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5375 - val_loss: 2.7320\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5347 - val_loss: 2.7305\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5336 - val_loss: 2.7316\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5324 - val_loss: 2.7298\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5325 - val_loss: 2.7322\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.5319 - val_loss: 2.7296\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5326 - val_loss: 2.7326\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5320 - val_loss: 2.7300\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5333 - val_loss: 2.7343\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5344 - val_loss: 2.7338\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5397 - val_loss: 2.7393\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5414 - val_loss: 2.7394\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5501 - val_loss: 2.7486\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5538 - val_loss: 2.7487\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5623 - val_loss: 2.7513\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5612 - val_loss: 2.7523\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5649 - val_loss: 2.7485\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5582 - val_loss: 2.7472\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5591 - val_loss: 2.7453\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5522 - val_loss: 2.7421\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5536 - val_loss: 2.7437\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5491 - val_loss: 2.7389\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5499 - val_loss: 2.7420\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5457 - val_loss: 2.7363\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5459 - val_loss: 2.7401\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5422 - val_loss: 2.7343\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5425 - val_loss: 2.7391\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5397 - val_loss: 2.7332\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5409 - val_loss: 2.7385\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5384 - val_loss: 2.7326\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5399 - val_loss: 2.7378\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5374 - val_loss: 2.7324\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5389 - val_loss: 2.7372\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5363 - val_loss: 2.7321\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.5380 - val_loss: 2.7364\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5353 - val_loss: 2.7317\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5369 - val_loss: 2.7357\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5342 - val_loss: 2.7313\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5359 - val_loss: 2.7350\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5331 - val_loss: 2.7309\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5350 - val_loss: 2.7345\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5322 - val_loss: 2.7307\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5342 - val_loss: 2.7343\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5316 - val_loss: 2.7306\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5337 - val_loss: 2.7342\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5312 - val_loss: 2.7307\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5335 - val_loss: 2.7346\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5312 - val_loss: 2.7312\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.5338 - val_loss: 2.7354\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5318 - val_loss: 2.7321\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5348 - val_loss: 2.7356\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5322 - val_loss: 2.7328\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5349 - val_loss: 2.7352\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5317 - val_loss: 2.7327\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5336 - val_loss: 2.7345\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5306 - val_loss: 2.7320\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 55.7604 - val_loss: 52.7696\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 54.2662 - val_loss: 50.3850\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 51.8791 - val_loss: 47.2173\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 48.7062 - val_loss: 43.2313\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 44.7119 - val_loss: 38.5245\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 39.9898 - val_loss: 33.3240\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 34.7526 - val_loss: 27.9979\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 29.3469 - val_loss: 23.2531\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 24.4581 - val_loss: 20.2386\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 21.0737 - val_loss: 18.6576\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 19.0445 - val_loss: 17.6056\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 17.7291 - val_loss: 16.8546\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 16.8529 - val_loss: 16.2919\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 16.2300 - val_loss: 15.7920\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 15.6980 - val_loss: 15.2323\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 15.1210 - val_loss: 14.5886\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 14.4692 - val_loss: 13.8940\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 13.7713 - val_loss: 13.1128\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 12.9921 - val_loss: 12.1939\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 12.0851 - val_loss: 11.0289\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 10.9855 - val_loss: 9.8733\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 10.0326 - val_loss: 9.1575\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 9.4224 - val_loss: 8.7748\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 9.0122 - val_loss: 8.5650\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.7899 - val_loss: 8.3575\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.6226 - val_loss: 8.1755\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.4757 - val_loss: 8.0479\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 8.3484 - val_loss: 7.9476\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.2297 - val_loss: 7.8512\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.1181 - val_loss: 7.7508\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.0128 - val_loss: 7.6551\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.9155 - val_loss: 7.5679\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.8233 - val_loss: 7.4842\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 7.7322 - val_loss: 7.3984\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 7.6406 - val_loss: 7.3098\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.5479 - val_loss: 7.2206\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 7.4537 - val_loss: 7.1309\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.3576 - val_loss: 7.0391\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.2593 - val_loss: 6.9444\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.1585 - val_loss: 6.8470\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.0553 - val_loss: 6.7472\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.9498 - val_loss: 6.6453\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.8423 - val_loss: 6.5413\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 6.7324 - val_loss: 6.4353\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 6.6203 - val_loss: 6.3275\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 6.5058 - val_loss: 6.2182\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 6.3891 - val_loss: 6.1082\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 6.2705 - val_loss: 5.9975\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.1501 - val_loss: 5.8853\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 6.0278 - val_loss: 5.7721\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.9036 - val_loss: 5.6582\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 5.7776 - val_loss: 5.5438\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.6499 - val_loss: 5.4294\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.5205 - val_loss: 5.3144\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 5.3900 - val_loss: 5.1991\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 5.2590 - val_loss: 5.0846\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 5.1290 - val_loss: 4.9724\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 5.0022 - val_loss: 4.8629\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.8808 - val_loss: 4.7564\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 4.7654 - val_loss: 4.6561\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.6571 - val_loss: 4.5657\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4.5566 - val_loss: 4.4823\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.4636 - val_loss: 4.4070\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.3767 - val_loss: 4.3357\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2963 - val_loss: 4.2714\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.2229 - val_loss: 4.2120\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.1566 - val_loss: 4.1580\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.0967 - val_loss: 4.1086\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.0426 - val_loss: 4.0643\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.9936 - val_loss: 4.0245\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.9486 - val_loss: 3.9895\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.9073 - val_loss: 3.9571\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.8703 - val_loss: 3.9279\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.8372 - val_loss: 3.8996\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.8075 - val_loss: 3.8735\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.7800 - val_loss: 3.8502\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.7539 - val_loss: 3.8266\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.7291 - val_loss: 3.8022\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.7055 - val_loss: 3.7779\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.6832 - val_loss: 3.7547\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.6620 - val_loss: 3.7324\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.6419 - val_loss: 3.7106\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.6227 - val_loss: 3.6889\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.6042 - val_loss: 3.6682\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.5861 - val_loss: 3.6482\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.5685 - val_loss: 3.6286\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5514 - val_loss: 3.6093\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.5346 - val_loss: 3.5903\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.5181 - val_loss: 3.5717\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.5021 - val_loss: 3.5535\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.4864 - val_loss: 3.5358\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.4711 - val_loss: 3.5185\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4562 - val_loss: 3.5015\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4417 - val_loss: 3.4849\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.4275 - val_loss: 3.4691\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.4136 - val_loss: 3.4533\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.4000 - val_loss: 3.4377\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.3867 - val_loss: 3.4224\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3737 - val_loss: 3.4076\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3610 - val_loss: 3.3931\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3488 - val_loss: 3.3791\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.3368 - val_loss: 3.3657\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.3252 - val_loss: 3.3521\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.3142 - val_loss: 3.3390\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 3.3035 - val_loss: 3.3267\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.2931 - val_loss: 3.3149\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2829 - val_loss: 3.3033\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.2729 - val_loss: 3.2919\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2632 - val_loss: 3.2808\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2537 - val_loss: 3.2701\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2444 - val_loss: 3.2597\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.2353 - val_loss: 3.2495\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.2264 - val_loss: 3.2395\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.2176 - val_loss: 3.2298\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.2091 - val_loss: 3.2206\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.2006 - val_loss: 3.2115\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1924 - val_loss: 3.2025\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1843 - val_loss: 3.1938\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1764 - val_loss: 3.1854\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1687 - val_loss: 3.1773\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.1611 - val_loss: 3.1693\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.1538 - val_loss: 3.1617\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.1466 - val_loss: 3.1543\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.1395 - val_loss: 3.1472\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1327 - val_loss: 3.1400\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.1261 - val_loss: 3.1335\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1197 - val_loss: 3.1276\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.1134 - val_loss: 3.1215\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1071 - val_loss: 3.1154\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1010 - val_loss: 3.1094\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0950 - val_loss: 3.1038\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.0891 - val_loss: 3.0981\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0832 - val_loss: 3.0926\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.0774 - val_loss: 3.0868\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0718 - val_loss: 3.0815\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.0662 - val_loss: 3.0760\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.0607 - val_loss: 3.0711\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.0553 - val_loss: 3.0657\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0500 - val_loss: 3.0612\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0449 - val_loss: 3.0563\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0398 - val_loss: 3.0523\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0349 - val_loss: 3.0470\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0301 - val_loss: 3.0445\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0255 - val_loss: 3.0381\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0214 - val_loss: 3.0383\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0173 - val_loss: 3.0300\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0135 - val_loss: 3.0319\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.0091 - val_loss: 3.0226\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.0049 - val_loss: 3.0255\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0010 - val_loss: 3.0155\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9972 - val_loss: 3.0182\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9934 - val_loss: 3.0091\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9898 - val_loss: 3.0127\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9860 - val_loss: 3.0027\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9819 - val_loss: 3.0058\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9778 - val_loss: 2.9962\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9738 - val_loss: 2.9996\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9700 - val_loss: 2.9900\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9662 - val_loss: 2.9936\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9626 - val_loss: 2.9842\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9588 - val_loss: 2.9877\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9553 - val_loss: 2.9784\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9516 - val_loss: 2.9820\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9482 - val_loss: 2.9728\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9446 - val_loss: 2.9764\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.9412 - val_loss: 2.9675\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9378 - val_loss: 2.9710\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9344 - val_loss: 2.9624\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9311 - val_loss: 2.9658\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9278 - val_loss: 2.9576\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9247 - val_loss: 2.9607\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9214 - val_loss: 2.9527\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.9183 - val_loss: 2.9557\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9151 - val_loss: 2.9479\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9121 - val_loss: 2.9511\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9091 - val_loss: 2.9434\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9061 - val_loss: 2.9468\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.9033 - val_loss: 2.9390\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9005 - val_loss: 2.9428\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8978 - val_loss: 2.9358\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.8955 - val_loss: 2.9389\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.8930 - val_loss: 2.9321\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8907 - val_loss: 2.9362\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.8881 - val_loss: 2.9284\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8856 - val_loss: 2.9320\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8829 - val_loss: 2.9249\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.8808 - val_loss: 2.9283\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8784 - val_loss: 2.9213\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8765 - val_loss: 2.9255\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8741 - val_loss: 2.9177\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8720 - val_loss: 2.9218\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8694 - val_loss: 2.9140\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8672 - val_loss: 2.9180\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8647 - val_loss: 2.9104\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8625 - val_loss: 2.9143\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8601 - val_loss: 2.9070\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8580 - val_loss: 2.9107\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8557 - val_loss: 2.9036\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8535 - val_loss: 2.9071\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8513 - val_loss: 2.9006\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8491 - val_loss: 2.9035\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 2.8470 - val_loss: 2.8983\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8446 - val_loss: 2.8994\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8424 - val_loss: 2.8953\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8396 - val_loss: 2.8952\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8374 - val_loss: 2.8922\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8351 - val_loss: 2.8923\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8333 - val_loss: 2.8895\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8313 - val_loss: 2.8898\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.8297 - val_loss: 2.8867\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8278 - val_loss: 2.8875\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8261 - val_loss: 2.8838\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.8243 - val_loss: 2.8850\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.8226 - val_loss: 2.8809\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8208 - val_loss: 2.8825\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8190 - val_loss: 2.8782\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8173 - val_loss: 2.8798\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8155 - val_loss: 2.8754\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8137 - val_loss: 2.8772\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8120 - val_loss: 2.8728\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.8102 - val_loss: 2.8745\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8084 - val_loss: 2.8701\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8067 - val_loss: 2.8720\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8049 - val_loss: 2.8675\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8032 - val_loss: 2.8695\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8015 - val_loss: 2.8649\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7998 - val_loss: 2.8671\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7980 - val_loss: 2.8624\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7964 - val_loss: 2.8646\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7946 - val_loss: 2.8598\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7929 - val_loss: 2.8621\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7912 - val_loss: 2.8572\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7894 - val_loss: 2.8596\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7877 - val_loss: 2.8546\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7859 - val_loss: 2.8573\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7842 - val_loss: 2.8521\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7824 - val_loss: 2.8543\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 2.7806 - val_loss: 2.8499\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7788 - val_loss: 2.8513\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7770 - val_loss: 2.8481\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7753 - val_loss: 2.8486\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7736 - val_loss: 2.8469\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7721 - val_loss: 2.8464\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7705 - val_loss: 2.8454\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7690 - val_loss: 2.8443\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7674 - val_loss: 2.8436\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.7659 - val_loss: 2.8424\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7644 - val_loss: 2.8417\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7629 - val_loss: 2.8405\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7614 - val_loss: 2.8399\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7599 - val_loss: 2.8386\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7584 - val_loss: 2.8381\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7569 - val_loss: 2.8367\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7555 - val_loss: 2.8364\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7541 - val_loss: 2.8348\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7527 - val_loss: 2.8347\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7513 - val_loss: 2.8329\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7499 - val_loss: 2.8327\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7486 - val_loss: 2.8309\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7472 - val_loss: 2.8307\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7460 - val_loss: 2.8289\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.7446 - val_loss: 2.8287\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7434 - val_loss: 2.8267\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7419 - val_loss: 2.8267\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7406 - val_loss: 2.8247\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7391 - val_loss: 2.8246\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7378 - val_loss: 2.8229\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.7363 - val_loss: 2.8226\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7350 - val_loss: 2.8212\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7336 - val_loss: 2.8208\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7323 - val_loss: 2.8195\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7309 - val_loss: 2.8189\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7297 - val_loss: 2.8177\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7283 - val_loss: 2.8171\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7271 - val_loss: 2.8159\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7257 - val_loss: 2.8153\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7245 - val_loss: 2.8141\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7232 - val_loss: 2.8136\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7220 - val_loss: 2.8125\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.7207 - val_loss: 2.8119\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7196 - val_loss: 2.8111\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7182 - val_loss: 2.8101\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7172 - val_loss: 2.8097\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7158 - val_loss: 2.8082\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7147 - val_loss: 2.8081\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7133 - val_loss: 2.8063\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7121 - val_loss: 2.8064\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7108 - val_loss: 2.8046\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.7097 - val_loss: 2.8047\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7084 - val_loss: 2.8029\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7072 - val_loss: 2.8029\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.7060 - val_loss: 2.8013\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7048 - val_loss: 2.8011\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7036 - val_loss: 2.7997\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7024 - val_loss: 2.7992\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7012 - val_loss: 2.7983\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7000 - val_loss: 2.7974\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6988 - val_loss: 2.7969\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6977 - val_loss: 2.7957\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6965 - val_loss: 2.7956\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6955 - val_loss: 2.7941\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6942 - val_loss: 2.7942\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6932 - val_loss: 2.7925\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6919 - val_loss: 2.7927\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6909 - val_loss: 2.7910\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6896 - val_loss: 2.7913\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6889 - val_loss: 2.7898\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6876 - val_loss: 2.7897\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6869 - val_loss: 2.7885\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6857 - val_loss: 2.7882\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6849 - val_loss: 2.7870\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6837 - val_loss: 2.7867\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6827 - val_loss: 2.7854\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6814 - val_loss: 2.7853\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6804 - val_loss: 2.7837\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6790 - val_loss: 2.7836\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6780 - val_loss: 2.7822\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6766 - val_loss: 2.7818\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6757 - val_loss: 2.7807\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6743 - val_loss: 2.7800\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.6734 - val_loss: 2.7795\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6720 - val_loss: 2.7783\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.6712 - val_loss: 2.7788\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6699 - val_loss: 2.7766\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6692 - val_loss: 2.7781\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6681 - val_loss: 2.7752\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6674 - val_loss: 2.7777\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6667 - val_loss: 2.7735\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6663 - val_loss: 2.7782\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6664 - val_loss: 2.7723\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6669 - val_loss: 2.7806\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6679 - val_loss: 2.7721\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6683 - val_loss: 2.7828\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6695 - val_loss: 2.7718\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6683 - val_loss: 2.7821\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6685 - val_loss: 2.7707\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6661 - val_loss: 2.7800\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6660 - val_loss: 2.7693\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6636 - val_loss: 2.7782\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6637 - val_loss: 2.7680\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6615 - val_loss: 2.7769\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6617 - val_loss: 2.7669\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6596 - val_loss: 2.7759\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6599 - val_loss: 2.7658\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6578 - val_loss: 2.7750\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6582 - val_loss: 2.7646\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6561 - val_loss: 2.7741\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6566 - val_loss: 2.7635\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6544 - val_loss: 2.7732\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6549 - val_loss: 2.7624\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6528 - val_loss: 2.7723\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6533 - val_loss: 2.7614\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6512 - val_loss: 2.7714\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6518 - val_loss: 2.7604\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6497 - val_loss: 2.7705\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6503 - val_loss: 2.7595\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.6482 - val_loss: 2.7698\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6489 - val_loss: 2.7587\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6469 - val_loss: 2.7692\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6476 - val_loss: 2.7579\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6455 - val_loss: 2.7686\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6464 - val_loss: 2.7571\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6442 - val_loss: 2.7679\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6451 - val_loss: 2.7562\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6427 - val_loss: 2.7669\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6435 - val_loss: 2.7551\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6410 - val_loss: 2.7657\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6417 - val_loss: 2.7540\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6391 - val_loss: 2.7643\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6396 - val_loss: 2.7526\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.6369 - val_loss: 2.7631\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6371 - val_loss: 2.7509\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6344 - val_loss: 2.7624\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6349 - val_loss: 2.7494\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6331 - val_loss: 2.7623\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6341 - val_loss: 2.7489\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6331 - val_loss: 2.7635\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6347 - val_loss: 2.7486\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6328 - val_loss: 2.7636\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6340 - val_loss: 2.7479\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6311 - val_loss: 2.7621\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6318 - val_loss: 2.7467\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6287 - val_loss: 2.7599\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6291 - val_loss: 2.7452\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 2.6265 - val_loss: 2.7582\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 2.6271 - val_loss: 2.7440\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6248 - val_loss: 2.7569\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6255 - val_loss: 2.7429\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6230 - val_loss: 2.7553\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6238 - val_loss: 2.7424\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6211 - val_loss: 2.7547\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6225 - val_loss: 2.7419\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6191 - val_loss: 2.7524\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6194 - val_loss: 2.7404\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6152 - val_loss: 2.7478\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6142 - val_loss: 2.7378\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6109 - val_loss: 2.7443\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6104 - val_loss: 2.7359\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6084 - val_loss: 2.7431\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6089 - val_loss: 2.7353\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6077 - val_loss: 2.7441\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6095 - val_loss: 2.7358\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6082 - val_loss: 2.7459\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6104 - val_loss: 2.7362\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6078 - val_loss: 2.7459\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6093 - val_loss: 2.7363\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6062 - val_loss: 2.7444\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6069 - val_loss: 2.7352\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6037 - val_loss: 2.7423\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6042 - val_loss: 2.7336\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6013 - val_loss: 2.7406\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6020 - val_loss: 2.7322\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5995 - val_loss: 2.7399\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6005 - val_loss: 2.7311\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5982 - val_loss: 2.7398\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5996 - val_loss: 2.7304\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5973 - val_loss: 2.7401\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5990 - val_loss: 2.7302\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5972 - val_loss: 2.7413\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5995 - val_loss: 2.7311\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5976 - val_loss: 2.7426\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6001 - val_loss: 2.7327\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5976 - val_loss: 2.7436\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6005 - val_loss: 2.7350\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5977 - val_loss: 2.7447\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6017 - val_loss: 2.7397\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6003 - val_loss: 2.7469\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6054 - val_loss: 2.7511\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6101 - val_loss: 2.7552\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6172 - val_loss: 2.7598\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6215 - val_loss: 2.7622\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6286 - val_loss: 2.7627\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6245 - val_loss: 2.7574\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6238 - val_loss: 2.7560\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6153 - val_loss: 2.7474\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6118 - val_loss: 2.7504\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6075 - val_loss: 2.7412\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6031 - val_loss: 2.7453\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5991 - val_loss: 2.7357\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5955 - val_loss: 2.7380\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5889 - val_loss: 2.7292\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5881 - val_loss: 2.7393\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5870 - val_loss: 2.7280\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5884 - val_loss: 2.7432\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5903 - val_loss: 2.7281\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5925 - val_loss: 2.7516\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5997 - val_loss: 2.7314\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6013 - val_loss: 2.7625\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6118 - val_loss: 2.7358\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6083 - val_loss: 2.7662\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6149 - val_loss: 2.7337\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6036 - val_loss: 2.7603\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6084 - val_loss: 2.7296\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5979 - val_loss: 2.7568\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6043 - val_loss: 2.7275\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5955 - val_loss: 2.7546\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6017 - val_loss: 2.7253\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5927 - val_loss: 2.7511\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5974 - val_loss: 2.7228\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5885 - val_loss: 2.7465\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5916 - val_loss: 2.7212\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5850 - val_loss: 2.7429\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5865 - val_loss: 2.7203\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5816 - val_loss: 2.7390\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5814 - val_loss: 2.7188\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5777 - val_loss: 2.7374\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5787 - val_loss: 2.7203\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5781 - val_loss: 2.7408\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5818 - val_loss: 2.7255\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5835 - val_loss: 2.7471\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.5873 - val_loss: 2.7302\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5877 - val_loss: 2.7473\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5876 - val_loss: 2.7295\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5848 - val_loss: 2.7415\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5815 - val_loss: 2.7266\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5797 - val_loss: 2.7357\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5757 - val_loss: 2.7236\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5746 - val_loss: 2.7312\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5701 - val_loss: 2.7207\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5699 - val_loss: 2.7289\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5666 - val_loss: 2.7180\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5660 - val_loss: 2.7265\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5635 - val_loss: 2.7159\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5642 - val_loss: 2.7282\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5644 - val_loss: 2.7165\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5669 - val_loss: 2.7346\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5702 - val_loss: 2.7172\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5725 - val_loss: 2.7429\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5797 - val_loss: 2.7184\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5784 - val_loss: 2.7483\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5857 - val_loss: 2.7185\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5797 - val_loss: 2.7503\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5873 - val_loss: 2.7183\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5772 - val_loss: 2.7451\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5810 - val_loss: 2.7150\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5711 - val_loss: 2.7405\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5752 - val_loss: 2.7129\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5683 - val_loss: 2.7387\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5731 - val_loss: 2.7118\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5671 - val_loss: 2.7371\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5707 - val_loss: 2.7108\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5652 - val_loss: 2.7352\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5679 - val_loss: 2.7102\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5637 - val_loss: 2.7351\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5670 - val_loss: 2.7100\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5633 - val_loss: 2.7356\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5669 - val_loss: 2.7100\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5630 - val_loss: 2.7356\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5662 - val_loss: 2.7098\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.5622 - val_loss: 2.7340\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5637 - val_loss: 2.7095\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5605 - val_loss: 2.7333\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5622 - val_loss: 2.7110\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5604 - val_loss: 2.7346\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5624 - val_loss: 2.7143\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5608 - val_loss: 2.7333\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5609 - val_loss: 2.7169\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5605 - val_loss: 2.7340\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5616 - val_loss: 2.7230\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5663 - val_loss: 2.7328\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5628 - val_loss: 2.7226\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5645 - val_loss: 2.7260\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5573 - val_loss: 2.7201\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5590 - val_loss: 2.7206\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5523 - val_loss: 2.7174\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5537 - val_loss: 2.7180\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5488 - val_loss: 2.7164\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5510 - val_loss: 2.7170\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.5472 - val_loss: 2.7170\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5515 - val_loss: 2.7171\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5477 - val_loss: 2.7170\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5511 - val_loss: 2.7157\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5460 - val_loss: 2.7148\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5472 - val_loss: 2.7132\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5421 - val_loss: 2.7127\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5437 - val_loss: 2.7120\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.539 - 0s 70ms/step - loss: 2.5399 - val_loss: 2.7125\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5426 - val_loss: 2.7119\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5394 - val_loss: 2.7130\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5433 - val_loss: 2.7113\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5395 - val_loss: 2.7121\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5415 - val_loss: 2.7087\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5365 - val_loss: 2.7097\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5375 - val_loss: 2.7057\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5327 - val_loss: 2.7081\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5341 - val_loss: 2.7039\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5303 - val_loss: 2.7076\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5324 - val_loss: 2.7033\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5292 - val_loss: 2.7076\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5316 - val_loss: 2.7032\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5287 - val_loss: 2.7078\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.5313 - val_loss: 2.7034\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5284 - val_loss: 2.7078\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5308 - val_loss: 2.7033\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5277 - val_loss: 2.7073\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5297 - val_loss: 2.7028\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5263 - val_loss: 2.7064\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5281 - val_loss: 2.7020\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5245 - val_loss: 2.7053\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5264 - val_loss: 2.7014\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5229 - val_loss: 2.7045\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5249 - val_loss: 2.7012\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5216 - val_loss: 2.7041\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5241 - val_loss: 2.7015\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5209 - val_loss: 2.7042\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5240 - val_loss: 2.7026\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5209 - val_loss: 2.7050\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5252 - val_loss: 2.7068\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5234 - val_loss: 2.7062\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5294 - val_loss: 2.7151\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5289 - val_loss: 2.7079\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5351 - val_loss: 2.7246\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5369 - val_loss: 2.7082\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5422 - val_loss: 2.7401\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5521 - val_loss: 2.7125\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5550 - val_loss: 2.7551\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5671 - val_loss: 2.7145\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5616 - val_loss: 2.7556\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5667 - val_loss: 2.7104\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5529 - val_loss: 2.7474\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5560 - val_loss: 2.7087\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5499 - val_loss: 2.7477\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5554 - val_loss: 2.7074\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5470 - val_loss: 2.7420\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5491 - val_loss: 2.7038\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5404 - val_loss: 2.7356\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5417 - val_loss: 2.7018\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.5356 - val_loss: 2.7324\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5376 - val_loss: 2.7011\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5332 - val_loss: 2.7309\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5353 - val_loss: 2.7007\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5316 - val_loss: 2.7294\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5329 - val_loss: 2.7000\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5295 - val_loss: 2.7272\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5298 - val_loss: 2.6990\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5269 - val_loss: 2.7251\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.5268 - val_loss: 2.6982\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5247 - val_loss: 2.7237\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5248 - val_loss: 2.6978\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5233 - val_loss: 2.7231\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 57.4961 - val_loss: 56.2640\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 56.0556 - val_loss: 54.0587\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 53.8500 - val_loss: 51.2561\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 51.0484 - val_loss: 47.7581\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 47.5529 - val_loss: 43.4184\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 43.2149 - val_loss: 38.2322\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 38.0265 - val_loss: 32.4959\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 32.2786 - val_loss: 26.9320\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 26.6451 - val_loss: 22.5946\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 22.1831 - val_loss: 20.0193\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 19.6192 - val_loss: 18.3181\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 18.0056 - val_loss: 17.2254\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 16.9508 - val_loss: 16.5234\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 16.2453 - val_loss: 15.9814\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 15.6909 - val_loss: 15.3779\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 15.0778 - val_loss: 14.6408\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 14.3440 - val_loss: 13.8563\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.5777 - val_loss: 12.9669\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 12.7110 - val_loss: 12.1657\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.9092 - val_loss: 11.4020\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 11.1253 - val_loss: 10.6293\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 10.3263 - val_loss: 9.9335\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.6178 - val_loss: 9.4383\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.1240 - val_loss: 9.1495\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.8448 - val_loss: 8.9852\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.6746 - val_loss: 8.8789\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.5601 - val_loss: 8.7969\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 8.4762 - val_loss: 8.7253\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.4056 - val_loss: 8.6527\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.3374 - val_loss: 8.5795\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 8.2693 - val_loss: 8.5082\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2032 - val_loss: 8.4389\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.1397 - val_loss: 8.3726\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 8.0785 - val_loss: 8.3080\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.0182 - val_loss: 8.2431\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 7.9572 - val_loss: 8.1763\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.8941 - val_loss: 8.1066\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 7.8281 - val_loss: 8.0339\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.7594 - val_loss: 7.9583\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 7.6877 - val_loss: 7.8797\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.6132 - val_loss: 7.7979\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 7.5357 - val_loss: 7.7130\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 7.4556 - val_loss: 7.6254\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 7.3732 - val_loss: 7.5360\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.2891 - val_loss: 7.4452\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.2041 - val_loss: 7.3532\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.1184 - val_loss: 7.2601\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.0322 - val_loss: 7.1656\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 6.9453 - val_loss: 7.0692\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.8574 - val_loss: 6.9711\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 6.7684 - val_loss: 6.8711\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.6780 - val_loss: 6.7692\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.5864 - val_loss: 6.6650\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 6.4938 - val_loss: 6.5585\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 6.4000 - val_loss: 6.4507\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.3052 - val_loss: 6.3418\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 6.2095 - val_loss: 6.2315\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 6.1130 - val_loss: 6.1196\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 6.0158 - val_loss: 6.0067\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 5.9183 - val_loss: 5.8935\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 5.8206 - val_loss: 5.7805\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 5.7231 - val_loss: 5.6677\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 5.6258 - val_loss: 5.5553\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.5291 - val_loss: 5.4438\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.4332 - val_loss: 5.3335\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 5.3382 - val_loss: 5.2247\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 5.2445 - val_loss: 5.1177\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 5.1523 - val_loss: 5.0129\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.0617 - val_loss: 4.9110\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.9731 - val_loss: 4.8128\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.8870 - val_loss: 4.7171\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.8035 - val_loss: 4.6263\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.7228 - val_loss: 4.5397\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.6450 - val_loss: 4.4573\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.5705 - val_loss: 4.3792\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.4994 - val_loss: 4.3042\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.4324 - val_loss: 4.2350\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 4.3700 - val_loss: 4.1709\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 4.3114 - val_loss: 4.1100\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.2565 - val_loss: 4.0572\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.2049 - val_loss: 4.0017\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 4.1563 - val_loss: 3.9559\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 4.1101 - val_loss: 3.9080\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 4.0664 - val_loss: 3.8653\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 4.0248 - val_loss: 3.8216\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.9852 - val_loss: 3.7824\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.9478 - val_loss: 3.7475\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.9123 - val_loss: 3.7124\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.8783 - val_loss: 3.6779\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.8458 - val_loss: 3.6447\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.8145 - val_loss: 3.6141\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.7845 - val_loss: 3.5861\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.7556 - val_loss: 3.5573\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.7282 - val_loss: 3.5329\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.7023 - val_loss: 3.5089\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3.6777 - val_loss: 3.4856\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.6540 - val_loss: 3.4614\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.6312 - val_loss: 3.4409\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.6090 - val_loss: 3.4175\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.5876 - val_loss: 3.3999\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.5672 - val_loss: 3.3778\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.5476 - val_loss: 3.3612\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5288 - val_loss: 3.3432\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5112 - val_loss: 3.3281\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4945 - val_loss: 3.3069\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.4784 - val_loss: 3.2934\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4628 - val_loss: 3.2756\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.4478 - val_loss: 3.2612\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.4333 - val_loss: 3.2462\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4194 - val_loss: 3.2334\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4060 - val_loss: 3.2203\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.3931 - val_loss: 3.2069\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.3806 - val_loss: 3.1947\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3683 - val_loss: 3.1826\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.3564 - val_loss: 3.1709\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.3447 - val_loss: 3.1594\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.3333 - val_loss: 3.1484\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.3222 - val_loss: 3.1378\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3113 - val_loss: 3.1278\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3007 - val_loss: 3.1187\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.2904 - val_loss: 3.1099\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2805 - val_loss: 3.1013\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.2708 - val_loss: 3.0936\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2614 - val_loss: 3.0854\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2523 - val_loss: 3.0775\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2434 - val_loss: 3.0701\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.2347 - val_loss: 3.0628\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2261 - val_loss: 3.0558\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.2178 - val_loss: 3.0490\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.2097 - val_loss: 3.0426\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.2017 - val_loss: 3.0365\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1939 - val_loss: 3.0304\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1862 - val_loss: 3.0246\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.1786 - val_loss: 3.0188\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.1712 - val_loss: 3.0131\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.1638 - val_loss: 3.0076\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1565 - val_loss: 3.0021\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1494 - val_loss: 2.9968\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1423 - val_loss: 2.9916\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1353 - val_loss: 2.9865\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1286 - val_loss: 2.9819\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.1220 - val_loss: 2.9771\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.1156 - val_loss: 2.9715\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 3.1093 - val_loss: 2.9673\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.1033 - val_loss: 2.9628\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.0974 - val_loss: 2.9582\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0918 - val_loss: 2.9539\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.0862 - val_loss: 2.9500\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.0808 - val_loss: 2.9461\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.0755 - val_loss: 2.9422\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.0704 - val_loss: 2.9386\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0653 - val_loss: 2.9350\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0602 - val_loss: 2.9315\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.0553 - val_loss: 2.9282\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0505 - val_loss: 2.9248\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0457 - val_loss: 2.9216\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0410 - val_loss: 2.9184\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0364 - val_loss: 2.9153\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0318 - val_loss: 2.9123\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0273 - val_loss: 2.9094\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.0229 - val_loss: 2.9065\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 3.0187 - val_loss: 2.9036\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.0145 - val_loss: 2.9006\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 3.0104 - val_loss: 2.8977\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.0064 - val_loss: 2.8948\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0025 - val_loss: 2.8920\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9986 - val_loss: 2.8891\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9948 - val_loss: 2.8862\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9911 - val_loss: 2.8833\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9874 - val_loss: 2.8804\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9838 - val_loss: 2.8775\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.9803 - val_loss: 2.8746\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9767 - val_loss: 2.8717\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.9733 - val_loss: 2.8688\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9698 - val_loss: 2.8660\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9664 - val_loss: 2.8632\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9630 - val_loss: 2.8605\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9596 - val_loss: 2.8578\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9563 - val_loss: 2.8550\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9530 - val_loss: 2.8519\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9497 - val_loss: 2.8487\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9464 - val_loss: 2.8450\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9433 - val_loss: 2.8456\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.9402 - val_loss: 2.8377\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9371 - val_loss: 2.8412\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9340 - val_loss: 2.8327\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9310 - val_loss: 2.8360\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9280 - val_loss: 2.8276\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.9250 - val_loss: 2.8318\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9222 - val_loss: 2.8224\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9193 - val_loss: 2.8279\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9170 - val_loss: 2.8180\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9144 - val_loss: 2.8249\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.9133 - val_loss: 2.8144\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9113 - val_loss: 2.8247\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9110 - val_loss: 2.8093\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9079 - val_loss: 2.8234\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.9069 - val_loss: 2.8058\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.9026 - val_loss: 2.8186\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.9009 - val_loss: 2.8026\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8966 - val_loss: 2.8135\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8950 - val_loss: 2.7990\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8909 - val_loss: 2.8093\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8896 - val_loss: 2.7953\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8857 - val_loss: 2.8054\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8843 - val_loss: 2.7916\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8805 - val_loss: 2.8008\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8786 - val_loss: 2.7877\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8750 - val_loss: 2.7958\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8732 - val_loss: 2.7846\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8699 - val_loss: 2.7919\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8685 - val_loss: 2.7815\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8654 - val_loss: 2.7892\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8641 - val_loss: 2.7782\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8610 - val_loss: 2.7863\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8598 - val_loss: 2.7755\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8567 - val_loss: 2.7832\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.8555 - val_loss: 2.7728\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8522 - val_loss: 2.7800\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8508 - val_loss: 2.7699\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8475 - val_loss: 2.7766\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8461 - val_loss: 2.7671\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.8429 - val_loss: 2.7735\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8417 - val_loss: 2.7646\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8386 - val_loss: 2.7708\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8374 - val_loss: 2.7622\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8343 - val_loss: 2.7681\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8332 - val_loss: 2.7597\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.8299 - val_loss: 2.7655\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.8289 - val_loss: 2.7571\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8255 - val_loss: 2.7630\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8245 - val_loss: 2.7546\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.8212 - val_loss: 2.7607\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8202 - val_loss: 2.7523\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8170 - val_loss: 2.7584\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8160 - val_loss: 2.7503\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8130 - val_loss: 2.7562\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8122 - val_loss: 2.7484\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.8093 - val_loss: 2.7541\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.8088 - val_loss: 2.7466\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8058 - val_loss: 2.7522\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8057 - val_loss: 2.7449\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8026 - val_loss: 2.7505\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8027 - val_loss: 2.7430\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7995 - val_loss: 2.7489\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7998 - val_loss: 2.7409\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7964 - val_loss: 2.7472\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7965 - val_loss: 2.7388\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7930 - val_loss: 2.7453\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7931 - val_loss: 2.7368\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7896 - val_loss: 2.7432\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.7896 - val_loss: 2.7348\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7862 - val_loss: 2.7411\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7859 - val_loss: 2.7328\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7828 - val_loss: 2.7390\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7822 - val_loss: 2.7308\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7794 - val_loss: 2.7371\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7783 - val_loss: 2.7287\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7759 - val_loss: 2.7350\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7747 - val_loss: 2.7271\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7727 - val_loss: 2.7328\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7715 - val_loss: 2.7256\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7697 - val_loss: 2.7308\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7686 - val_loss: 2.7237\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7668 - val_loss: 2.7294\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7657 - val_loss: 2.7217\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7639 - val_loss: 2.7281\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7627 - val_loss: 2.7197\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7610 - val_loss: 2.7265\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7598 - val_loss: 2.7177\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7581 - val_loss: 2.7248\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7568 - val_loss: 2.7157\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7552 - val_loss: 2.7226\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7538 - val_loss: 2.7141\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7523 - val_loss: 2.7203\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7510 - val_loss: 2.7129\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7496 - val_loss: 2.7180\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.7484 - val_loss: 2.7116\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7470 - val_loss: 2.7160\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7458 - val_loss: 2.7102\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7445 - val_loss: 2.7142\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7433 - val_loss: 2.7088\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7420 - val_loss: 2.7125\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7408 - val_loss: 2.7074\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7396 - val_loss: 2.7109\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7384 - val_loss: 2.7060\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7372 - val_loss: 2.7110\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7361 - val_loss: 2.7031\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7350 - val_loss: 2.7120\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.7340 - val_loss: 2.7004\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7329 - val_loss: 2.7136\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7322 - val_loss: 2.6962\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.7314 - val_loss: 2.7149\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7308 - val_loss: 2.6948\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7298 - val_loss: 2.7138\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.7290 - val_loss: 2.6939\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.7277 - val_loss: 2.7128\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7269 - val_loss: 2.6922\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7255 - val_loss: 2.7119\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7246 - val_loss: 2.6902\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7233 - val_loss: 2.7110\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7224 - val_loss: 2.6886\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7211 - val_loss: 2.7100\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7203 - val_loss: 2.6869\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7190 - val_loss: 2.7094\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7183 - val_loss: 2.6851\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7172 - val_loss: 2.7099\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7167 - val_loss: 2.6834\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7158 - val_loss: 2.7125\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7159 - val_loss: 2.6819\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7150 - val_loss: 2.7141\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7149 - val_loss: 2.6810\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7137 - val_loss: 2.7135\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7133 - val_loss: 2.6800\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7118 - val_loss: 2.7120\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7111 - val_loss: 2.6788\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7094 - val_loss: 2.7104\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7086 - val_loss: 2.6775\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7069 - val_loss: 2.7088\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7060 - val_loss: 2.6757\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7043 - val_loss: 2.7073\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7035 - val_loss: 2.6735\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7018 - val_loss: 2.7065\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.7012 - val_loss: 2.6717\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6998 - val_loss: 2.7061\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6992 - val_loss: 2.6704\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6979 - val_loss: 2.7057\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6974 - val_loss: 2.6694\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6961 - val_loss: 2.7049\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6956 - val_loss: 2.6684\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6942 - val_loss: 2.7040\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6937 - val_loss: 2.6673\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6923 - val_loss: 2.7032\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6918 - val_loss: 2.6661\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6905 - val_loss: 2.7025\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6899 - val_loss: 2.6649\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6886 - val_loss: 2.7020\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6881 - val_loss: 2.6636\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6868 - val_loss: 2.7015\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6864 - val_loss: 2.6624\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6851 - val_loss: 2.7013\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6847 - val_loss: 2.6612\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6834 - val_loss: 2.7012\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6831 - val_loss: 2.6598\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6818 - val_loss: 2.7009\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6814 - val_loss: 2.6585\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6801 - val_loss: 2.7002\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6796 - val_loss: 2.6573\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6783 - val_loss: 2.6995\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6777 - val_loss: 2.6562\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6765 - val_loss: 2.6987\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6759 - val_loss: 2.6550\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6747 - val_loss: 2.6980\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6741 - val_loss: 2.6538\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6729 - val_loss: 2.6973\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6723 - val_loss: 2.6525\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6712 - val_loss: 2.6968\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6706 - val_loss: 2.6512\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6696 - val_loss: 2.6964\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6689 - val_loss: 2.6499\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6680 - val_loss: 2.6961\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6674 - val_loss: 2.6485\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.6665 - val_loss: 2.6959\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6659 - val_loss: 2.6473\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6650 - val_loss: 2.6956\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6643 - val_loss: 2.6462\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6634 - val_loss: 2.6951\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6627 - val_loss: 2.6453\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6618 - val_loss: 2.6946\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6611 - val_loss: 2.6443\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6604 - val_loss: 2.6945\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6597 - val_loss: 2.6436\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6590 - val_loss: 2.6931\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6580 - val_loss: 2.6437\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6573 - val_loss: 2.6906\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6565 - val_loss: 2.6448\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6558 - val_loss: 2.6884\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6570 - val_loss: 2.6532\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6597 - val_loss: 2.6973\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6704 - val_loss: 2.6608\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6705 - val_loss: 2.7042\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6776 - val_loss: 2.6553\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6686 - val_loss: 2.6973\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6695 - val_loss: 2.6487\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6612 - val_loss: 2.6915\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6625 - val_loss: 2.6454\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6558 - val_loss: 2.6886\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6574 - val_loss: 2.6438\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6519 - val_loss: 2.6867\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6531 - val_loss: 2.6436\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6498 - val_loss: 2.6872\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.6529 - val_loss: 2.6443\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6499 - val_loss: 2.6879\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6537 - val_loss: 2.6444\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6500 - val_loss: 2.6881\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.6539 - val_loss: 2.6438\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6495 - val_loss: 2.6877\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6530 - val_loss: 2.6424\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6478 - val_loss: 2.6863\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6503 - val_loss: 2.6398\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6444 - val_loss: 2.6835\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6448 - val_loss: 2.6379\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 2.6408 - val_loss: 2.6821\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6402 - val_loss: 2.6379\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6387 - val_loss: 2.6824\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6413 - val_loss: 2.6388\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6391 - val_loss: 2.6830\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6430 - val_loss: 2.6397\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6397 - val_loss: 2.6827\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6440 - val_loss: 2.6406\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6408 - val_loss: 2.6839\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6457 - val_loss: 2.6404\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6421 - val_loss: 2.6855\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6462 - val_loss: 2.6384\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6413 - val_loss: 2.6845\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6438 - val_loss: 2.6363\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6386 - val_loss: 2.6820\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6406 - val_loss: 2.6333\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6347 - val_loss: 2.6799\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6314 - val_loss: 2.6273\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6278 - val_loss: 2.6765\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6234 - val_loss: 2.6248\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6223 - val_loss: 2.6732\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6198 - val_loss: 2.6248\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6200 - val_loss: 2.6731\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6185 - val_loss: 2.6249\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6195 - val_loss: 2.6741\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6196 - val_loss: 2.6297\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6224 - val_loss: 2.6792\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6311 - val_loss: 2.6421\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6336 - val_loss: 2.6867\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6429 - val_loss: 2.6383\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6356 - val_loss: 2.6841\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6369 - val_loss: 2.6297\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6291 - val_loss: 2.6784\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6290 - val_loss: 2.6265\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6228 - val_loss: 2.6740\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6217 - val_loss: 2.6230\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6167 - val_loss: 2.6688\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6123 - val_loss: 2.6213\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6108 - val_loss: 2.6643\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6093 - val_loss: 2.6241\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6100 - val_loss: 2.6655\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6153 - val_loss: 2.6291\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6139 - val_loss: 2.6695\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6198 - val_loss: 2.6308\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6181 - val_loss: 2.6774\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6246 - val_loss: 2.6300\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6223 - val_loss: 2.6803\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.6258 - val_loss: 2.6280\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6207 - val_loss: 2.6769\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6222 - val_loss: 2.6256\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6167 - val_loss: 2.6733\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6184 - val_loss: 2.6242\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6136 - val_loss: 2.6715\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6155 - val_loss: 2.6228\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6112 - val_loss: 2.6688\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6110 - val_loss: 2.6183\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6052 - val_loss: 2.6626\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6011 - val_loss: 2.6160\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5985 - val_loss: 2.6582\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5961 - val_loss: 2.6172\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5960 - val_loss: 2.6570\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5979 - val_loss: 2.6209\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5976 - val_loss: 2.6590\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6018 - val_loss: 2.6247\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6004 - val_loss: 2.6653\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6065 - val_loss: 2.6253\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6061 - val_loss: 2.6759\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6127 - val_loss: 2.6257\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6109 - val_loss: 2.6789\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6140 - val_loss: 2.6234\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.6088 - val_loss: 2.6739\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.6091 - val_loss: 2.6205\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.6038 - val_loss: 2.6700\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.6043 - val_loss: 2.6181\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6001 - val_loss: 2.6678\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5998 - val_loss: 2.6150\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5957 - val_loss: 2.6633\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5935 - val_loss: 2.6114\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5899 - val_loss: 2.6554\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5841 - val_loss: 2.6091\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5829 - val_loss: 2.6537\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5811 - val_loss: 2.6113\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5829 - val_loss: 2.6539\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5844 - val_loss: 2.6166\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5857 - val_loss: 2.6575\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5908 - val_loss: 2.6256\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5921 - val_loss: 2.6674\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6039 - val_loss: 2.6293\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6012 - val_loss: 2.6776\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.6091 - val_loss: 2.6256\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6044 - val_loss: 2.6793\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6053 - val_loss: 2.6178\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5972 - val_loss: 2.6703\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5941 - val_loss: 2.6112\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5881 - val_loss: 2.6641\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5845 - val_loss: 2.6064\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5810 - val_loss: 2.6576\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5749 - val_loss: 2.6031\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5750 - val_loss: 2.6568\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5721 - val_loss: 2.6044\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5748 - val_loss: 2.6589\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5738 - val_loss: 2.6077\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5763 - val_loss: 2.6582\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5765 - val_loss: 2.6152\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5795 - val_loss: 2.6652\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5884 - val_loss: 2.6262\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5890 - val_loss: 2.6721\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5984 - val_loss: 2.6299\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5968 - val_loss: 2.6816\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6073 - val_loss: 2.6355\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6057 - val_loss: 2.6886\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6131 - val_loss: 2.6375\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6096 - val_loss: 2.6910\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6146 - val_loss: 2.6361\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6076 - val_loss: 2.6865\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.6099 - val_loss: 2.6326\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6014 - val_loss: 2.6776\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6028 - val_loss: 2.6287\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5951 - val_loss: 2.6718\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5974 - val_loss: 2.6251\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5905 - val_loss: 2.6678\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5905 - val_loss: 2.6151\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5813 - val_loss: 2.6618\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5790 - val_loss: 2.6031\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5711 - val_loss: 2.6538\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5623 - val_loss: 2.5940\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5605 - val_loss: 2.6550\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5565 - val_loss: 2.5947\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5583 - val_loss: 2.6585\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5561 - val_loss: 2.5935\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5589 - val_loss: 2.6596\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5558 - val_loss: 2.5936\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5573 - val_loss: 2.6588\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5541 - val_loss: 2.5933\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5558 - val_loss: 2.6589\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5531 - val_loss: 2.5932\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5551 - val_loss: 2.6598\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5526 - val_loss: 2.5926\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5556 - val_loss: 2.6615\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5529 - val_loss: 2.5924\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5572 - val_loss: 2.6649\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5542 - val_loss: 2.5927\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5578 - val_loss: 2.6656\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5541 - val_loss: 2.5931\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5571 - val_loss: 2.6646\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5534 - val_loss: 2.5944\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5554 - val_loss: 2.6622\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5534 - val_loss: 2.6011\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5568 - val_loss: 2.6650\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5635 - val_loss: 2.6217\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5730 - val_loss: 2.6766\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5897 - val_loss: 2.6377\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5891 - val_loss: 2.6798\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5992 - val_loss: 2.6331\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5879 - val_loss: 2.6780\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5904 - val_loss: 2.6254\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5839 - val_loss: 2.6810\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5879 - val_loss: 2.6255\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5866 - val_loss: 2.6997\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5984 - val_loss: 2.6308\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.5973 - val_loss: 2.7094\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6060 - val_loss: 2.6294\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5970 - val_loss: 2.7019\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 2.5991 - val_loss: 2.6250\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5899 - val_loss: 2.6940\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5928 - val_loss: 2.6239\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5869 - val_loss: 2.6903\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5902 - val_loss: 2.6230\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.5846 - val_loss: 2.6851\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.5864 - val_loss: 2.6203\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5798 - val_loss: 2.6713\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5759 - val_loss: 2.6134\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5650 - val_loss: 2.6478\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5558 - val_loss: 2.5956\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5388 - val_loss: 2.6162\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5264 - val_loss: 2.5960\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5194 - val_loss: 2.5991\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5166 - val_loss: 2.6018\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5159 - val_loss: 2.5984\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5152 - val_loss: 2.6031\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5146 - val_loss: 2.6004\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5143 - val_loss: 2.6015\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5150 - val_loss: 2.6084\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.5184 - val_loss: 2.6049\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5239 - val_loss: 2.6143\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5245 - val_loss: 2.6085\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5293 - val_loss: 2.6112\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5229 - val_loss: 2.6048\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5226 - val_loss: 2.6046\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5156 - val_loss: 2.5985\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 56.6359 - val_loss: 56.2001\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 55.1908 - val_loss: 53.9836\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 52.9762 - val_loss: 51.1669\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 50.1613 - val_loss: 47.6938\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 46.6897 - val_loss: 43.5234\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 42.5187 - val_loss: 38.7167\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 37.7075 - val_loss: 33.3948\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 32.3802 - val_loss: 27.8967\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 26.9192 - val_loss: 23.0406\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 22.2709 - val_loss: 19.8325\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 19.5518 - val_loss: 17.8639\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 17.9215 - val_loss: 16.6139\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 16.8361 - val_loss: 15.7265\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 16.0169 - val_loss: 15.0092\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.3221 - val_loss: 14.3849\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 14.7024 - val_loss: 13.7815\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 14.0939 - val_loss: 13.1248\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 13.4204 - val_loss: 12.3934\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 12.6506 - val_loss: 11.5689\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.7482 - val_loss: 10.7336\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.7736 - val_loss: 10.1409\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 10.0010 - val_loss: 9.6868\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 9.3759 - val_loss: 9.3650\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.9225 - val_loss: 9.1669\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.6909 - val_loss: 8.9891\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.5487 - val_loss: 8.8447\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 8.4494 - val_loss: 8.7472\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.3787 - val_loss: 8.6814\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.3138 - val_loss: 8.6140\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2459 - val_loss: 8.5359\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.1746 - val_loss: 8.4564\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 8.1021 - val_loss: 8.3823\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 8.0276 - val_loss: 8.3088\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.9505 - val_loss: 8.2299\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.8709 - val_loss: 8.1457\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 7.7876 - val_loss: 8.0586\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 7.7007 - val_loss: 7.9687\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.6104 - val_loss: 7.8743\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 7.5168 - val_loss: 7.7751\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 7.4202 - val_loss: 7.6719\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 7.3204 - val_loss: 7.5650\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 7.2174 - val_loss: 7.4538\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 7.1109 - val_loss: 7.3384\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.0011 - val_loss: 7.2192\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 6.8886 - val_loss: 7.0980\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.7738 - val_loss: 6.9747\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.6573 - val_loss: 6.8501\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.5397 - val_loss: 6.7252\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 6.4216 - val_loss: 6.6009\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.3033 - val_loss: 6.4780\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 6.1851 - val_loss: 6.3567\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 6.0672 - val_loss: 6.2365\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.9498 - val_loss: 6.1172\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.8331 - val_loss: 6.0007\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.7171 - val_loss: 5.8850\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 5.6019 - val_loss: 5.7713\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 5.4877 - val_loss: 5.6604\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.3745 - val_loss: 5.5510\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.2621 - val_loss: 5.4430\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.1509 - val_loss: 5.3372\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.0414 - val_loss: 5.2320\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.9341 - val_loss: 5.1306\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.8302 - val_loss: 5.0328\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.7296 - val_loss: 4.9392\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 4.6331 - val_loss: 4.8511\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 4.5417 - val_loss: 4.7686\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.4557 - val_loss: 4.6904\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.3760 - val_loss: 4.6159\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.3029 - val_loss: 4.5477\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.2363 - val_loss: 4.4830\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 4.1748 - val_loss: 4.4228\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4.1175 - val_loss: 4.3665\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.0641 - val_loss: 4.3125\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 4.0148 - val_loss: 4.2606\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.9689 - val_loss: 4.2125\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.9261 - val_loss: 4.1679\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.8861 - val_loss: 4.1250\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 3.8486 - val_loss: 4.0846\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.8134 - val_loss: 4.0474\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.7807 - val_loss: 4.0122\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 3.7501 - val_loss: 3.9788\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.7211 - val_loss: 3.9466\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 3.6942 - val_loss: 3.9147\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.6692 - val_loss: 3.8859\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.6455 - val_loss: 3.8598\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.6232 - val_loss: 3.8358\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.6019 - val_loss: 3.8115\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.5813 - val_loss: 3.7883\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.5615 - val_loss: 3.7664\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.5427 - val_loss: 3.7449\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3.5246 - val_loss: 3.7248\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.5074 - val_loss: 3.7060\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.4908 - val_loss: 3.6881\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.4749 - val_loss: 3.6711\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.4596 - val_loss: 3.6553\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4447 - val_loss: 3.6401\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.4304 - val_loss: 3.6248\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.4166 - val_loss: 3.6097\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.4031 - val_loss: 3.5953\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.3901 - val_loss: 3.5815\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.3774 - val_loss: 3.5679\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3651 - val_loss: 3.5546\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.3529 - val_loss: 3.5418\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 3.3411 - val_loss: 3.5294\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3295 - val_loss: 3.5169\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3.3182 - val_loss: 3.5045\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.3071 - val_loss: 3.4929\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2963 - val_loss: 3.4817\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.2856 - val_loss: 3.4704\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.2752 - val_loss: 3.4592\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3.2650 - val_loss: 3.4493\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 3.2551 - val_loss: 3.4397\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.2453 - val_loss: 3.4296\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3.2357 - val_loss: 3.4194\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 3.2264 - val_loss: 3.4100\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 3.2173 - val_loss: 3.4012\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.2083 - val_loss: 3.3924\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 3.1994 - val_loss: 3.3837\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 3.1907 - val_loss: 3.3753\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1821 - val_loss: 3.3671\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.1736 - val_loss: 3.3593\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1652 - val_loss: 3.3516\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.1569 - val_loss: 3.3443\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.1488 - val_loss: 3.3368\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.1409 - val_loss: 3.3288\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.1332 - val_loss: 3.3218\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1256 - val_loss: 3.3148\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.1181 - val_loss: 3.3077\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 3.1107 - val_loss: 3.3010\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.1035 - val_loss: 3.2947\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.0964 - val_loss: 3.2885\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 3.0895 - val_loss: 3.2827\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.0828 - val_loss: 3.2770\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3.0763 - val_loss: 3.2715\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 3.0699 - val_loss: 3.2663\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0636 - val_loss: 3.2612\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.0575 - val_loss: 3.2562\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3.0514 - val_loss: 3.2512\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0455 - val_loss: 3.2464\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0397 - val_loss: 3.2419\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0340 - val_loss: 3.2375\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.0283 - val_loss: 3.2332\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3.0228 - val_loss: 3.2291\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.0173 - val_loss: 3.2250\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 3.0119 - val_loss: 3.2210\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 3.0067 - val_loss: 3.2176\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 3.0015 - val_loss: 3.2135\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9964 - val_loss: 3.2103\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9914 - val_loss: 3.2065\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.9865 - val_loss: 3.2029\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.9816 - val_loss: 3.1995\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9768 - val_loss: 3.1959\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9721 - val_loss: 3.1924\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.9674 - val_loss: 3.1891\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9628 - val_loss: 3.1859\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9583 - val_loss: 3.1828\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.9540 - val_loss: 3.1797\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9497 - val_loss: 3.1771\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9454 - val_loss: 3.1745\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9413 - val_loss: 3.1719\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9372 - val_loss: 3.1694\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9332 - val_loss: 3.1668\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.9292 - val_loss: 3.1643\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.9253 - val_loss: 3.1619\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9214 - val_loss: 3.1594\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.9177 - val_loss: 3.1570\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.9139 - val_loss: 3.1546\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9103 - val_loss: 3.1522\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.9066 - val_loss: 3.1498\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.9031 - val_loss: 3.1475\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8996 - val_loss: 3.1452\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.8961 - val_loss: 3.1430\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8927 - val_loss: 3.1408\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8893 - val_loss: 3.1387\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.8860 - val_loss: 3.1366\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8827 - val_loss: 3.1344\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.8794 - val_loss: 3.1323\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8762 - val_loss: 3.1302\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8731 - val_loss: 3.1281\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8700 - val_loss: 3.1259\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8669 - val_loss: 3.1238\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.8638 - val_loss: 3.1217\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.8608 - val_loss: 3.1196\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 2.8578 - val_loss: 3.1174\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8549 - val_loss: 3.1152\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8520 - val_loss: 3.1132\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8491 - val_loss: 3.1111\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8463 - val_loss: 3.1091\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8435 - val_loss: 3.1068\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8408 - val_loss: 3.1048\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8381 - val_loss: 3.1024\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.8354 - val_loss: 3.1012\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.8328 - val_loss: 3.0978\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.8303 - val_loss: 3.0988\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8278 - val_loss: 3.0919\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8255 - val_loss: 3.0982\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.8233 - val_loss: 3.0861\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.8211 - val_loss: 3.0969\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8190 - val_loss: 3.0818\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8166 - val_loss: 3.0936\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.8145 - val_loss: 3.0777\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8121 - val_loss: 3.0904\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.8101 - val_loss: 3.0739\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.8077 - val_loss: 3.0867\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8057 - val_loss: 3.0703\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8034 - val_loss: 3.0833\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8014 - val_loss: 3.0668\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7992 - val_loss: 3.0800\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7972 - val_loss: 3.0635\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7951 - val_loss: 3.0770\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7931 - val_loss: 3.0602\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7910 - val_loss: 3.0749\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7894 - val_loss: 3.0569\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.7874 - val_loss: 3.0734\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7860 - val_loss: 3.0533\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.7839 - val_loss: 3.0722\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7830 - val_loss: 3.0497\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.7808 - val_loss: 3.0710\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7801 - val_loss: 3.0464\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7777 - val_loss: 3.0695\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7770 - val_loss: 3.0435\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.7744 - val_loss: 3.0672\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7738 - val_loss: 3.0408\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7710 - val_loss: 3.0644\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.7703 - val_loss: 3.0382\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7675 - val_loss: 3.0616\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7668 - val_loss: 3.0358\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7640 - val_loss: 3.0591\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7634 - val_loss: 3.0335\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7607 - val_loss: 3.0585\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7609 - val_loss: 3.0312\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7586 - val_loss: 3.0595\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.7594 - val_loss: 3.0293\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 2.7572 - val_loss: 3.0608\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.7581 - val_loss: 3.0276\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7557 - val_loss: 3.0609\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7563 - val_loss: 3.0257\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7541 - val_loss: 3.0604\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7544 - val_loss: 3.0238\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7520 - val_loss: 3.0590\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7520 - val_loss: 3.0217\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7492 - val_loss: 3.0564\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7489 - val_loss: 3.0197\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7459 - val_loss: 3.0534\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.7455 - val_loss: 3.0175\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.7426 - val_loss: 3.0505\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.7423 - val_loss: 3.0154\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.7394 - val_loss: 3.0477\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7391 - val_loss: 3.0133\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.7363 - val_loss: 3.0447\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7359 - val_loss: 3.0112\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.7330 - val_loss: 3.0415\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7326 - val_loss: 3.0089\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.7296 - val_loss: 3.0382\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7292 - val_loss: 3.0068\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7265 - val_loss: 3.0355\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.7262 - val_loss: 3.0048\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7236 - val_loss: 3.0330\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7232 - val_loss: 3.0030\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7206 - val_loss: 3.0306\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7203 - val_loss: 3.0012\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.7176 - val_loss: 3.0279\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7171 - val_loss: 2.9993\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.7144 - val_loss: 3.0246\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7136 - val_loss: 2.9973\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7107 - val_loss: 3.0194\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.7088 - val_loss: 2.9953\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.7053 - val_loss: 3.0128\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7034 - val_loss: 2.9935\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.7003 - val_loss: 3.0074\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6989 - val_loss: 2.9918\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6967 - val_loss: 3.0039\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6957 - val_loss: 2.9901\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6940 - val_loss: 3.0027\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6934 - val_loss: 2.9884\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6920 - val_loss: 3.0030\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6918 - val_loss: 2.9870\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6908 - val_loss: 3.0042\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6909 - val_loss: 2.9855\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6901 - val_loss: 3.0054\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6902 - val_loss: 2.9840\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6892 - val_loss: 3.0060\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.6894 - val_loss: 2.9827\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6880 - val_loss: 3.0060\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6881 - val_loss: 2.9813\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6865 - val_loss: 3.0056\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6869 - val_loss: 2.9800\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6849 - val_loss: 3.0051\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6855 - val_loss: 2.9786\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6832 - val_loss: 3.0040\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6835 - val_loss: 2.9772\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6810 - val_loss: 3.0017\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6808 - val_loss: 2.9756\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6782 - val_loss: 2.9982\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6771 - val_loss: 2.9737\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6747 - val_loss: 2.9950\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6737 - val_loss: 2.9720\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6715 - val_loss: 2.9925\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6709 - val_loss: 2.9706\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6691 - val_loss: 2.9910\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6687 - val_loss: 2.9693\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6673 - val_loss: 2.9905\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6672 - val_loss: 2.9682\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6660 - val_loss: 2.9906\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6662 - val_loss: 2.9673\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6650 - val_loss: 2.9911\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6655 - val_loss: 2.9663\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6639 - val_loss: 2.9909\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6643 - val_loss: 2.9651\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6623 - val_loss: 2.9895\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2.6623 - val_loss: 2.9637\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.6600 - val_loss: 2.9871\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 2.6595 - val_loss: 2.9622\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 2.6573 - val_loss: 2.9846\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6565 - val_loss: 2.9607\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6547 - val_loss: 2.9828\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6541 - val_loss: 2.9595\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.6526 - val_loss: 2.9816\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6522 - val_loss: 2.9585\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6509 - val_loss: 2.9809\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6506 - val_loss: 2.9575\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6493 - val_loss: 2.9803\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.6491 - val_loss: 2.9565\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6477 - val_loss: 2.9794\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6474 - val_loss: 2.9554\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6459 - val_loss: 2.9781\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6454 - val_loss: 2.9543\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6439 - val_loss: 2.9766\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6432 - val_loss: 2.9531\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6418 - val_loss: 2.9750\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.6410 - val_loss: 2.9519\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.6396 - val_loss: 2.9736\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.6389 - val_loss: 2.9509\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.6376 - val_loss: 2.9732\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6375 - val_loss: 2.9503\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6364 - val_loss: 2.9735\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.6366 - val_loss: 2.9496\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6355 - val_loss: 2.9738\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6358 - val_loss: 2.9488\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6345 - val_loss: 2.9732\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6345 - val_loss: 2.9477\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6328 - val_loss: 2.9717\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6324 - val_loss: 2.9464\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6305 - val_loss: 2.9694\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6296 - val_loss: 2.9449\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.6276 - val_loss: 2.9666\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6265 - val_loss: 2.9432\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.6242 - val_loss: 2.9632\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6231 - val_loss: 2.9416\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.6204 - val_loss: 2.9595\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6195 - val_loss: 2.9407\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6176 - val_loss: 2.9580\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6173 - val_loss: 2.9400\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6159 - val_loss: 2.9579\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.6161 - val_loss: 2.9393\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6150 - val_loss: 2.9589\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6157 - val_loss: 2.9388\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.6147 - val_loss: 2.9601\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6155 - val_loss: 2.9390\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.6154 - val_loss: 2.9629\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.6165 - val_loss: 2.9395\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6170 - val_loss: 2.9666\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6185 - val_loss: 2.9407\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.6190 - val_loss: 2.9699\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.6205 - val_loss: 2.9418\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6205 - val_loss: 2.9707\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.6203 - val_loss: 2.9410\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.6191 - val_loss: 2.9681\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6175 - val_loss: 2.9381\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6152 - val_loss: 2.9637\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.6134 - val_loss: 2.9345\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.6095 - val_loss: 2.9563\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.6067 - val_loss: 2.9317\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6032 - val_loss: 2.9494\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.6004 - val_loss: 2.9299\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5969 - val_loss: 2.9434\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5952 - val_loss: 2.9302\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5928 - val_loss: 2.9404\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5922 - val_loss: 2.9285\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5909 - val_loss: 2.9404\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.5912 - val_loss: 2.9277\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5902 - val_loss: 2.9421\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5909 - val_loss: 2.9271\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5899 - val_loss: 2.9436\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5907 - val_loss: 2.9268\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5898 - val_loss: 2.9453\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5907 - val_loss: 2.9273\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5909 - val_loss: 2.9491\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.5924 - val_loss: 2.9294\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5944 - val_loss: 2.9575\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5979 - val_loss: 2.9335\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6017 - val_loss: 2.9669\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6051 - val_loss: 2.9383\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.6078 - val_loss: 2.9682\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6055 - val_loss: 2.9347\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6033 - val_loss: 2.9612\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5992 - val_loss: 2.9294\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5962 - val_loss: 2.9549\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5936 - val_loss: 2.9246\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5894 - val_loss: 2.9471\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5865 - val_loss: 2.9218\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5829 - val_loss: 2.9403\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5801 - val_loss: 2.9204\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5769 - val_loss: 2.9352\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5752 - val_loss: 2.9205\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5723 - val_loss: 2.9313\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5712 - val_loss: 2.9185\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.5695 - val_loss: 2.9301\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.5693 - val_loss: 2.9182\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5686 - val_loss: 2.9324\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5695 - val_loss: 2.9180\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5694 - val_loss: 2.9364\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.5712 - val_loss: 2.9198\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5734 - val_loss: 2.9453\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5766 - val_loss: 2.9238\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 2.5797 - val_loss: 2.9564\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5844 - val_loss: 2.9304\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.5893 - val_loss: 2.9634\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.5899 - val_loss: 2.9305\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5897 - val_loss: 2.9585\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5853 - val_loss: 2.9247\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5824 - val_loss: 2.9510\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.5788 - val_loss: 2.9197\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.5749 - val_loss: 2.9429\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5715 - val_loss: 2.9162\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5675 - val_loss: 2.9359\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 2.5647 - val_loss: 2.9140\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5613 - val_loss: 2.9316\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.5601 - val_loss: 2.9137\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5575 - val_loss: 2.9298\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.5572 - val_loss: 2.9122\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5555 - val_loss: 2.9298\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5561 - val_loss: 2.9126\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 2.5551 - val_loss: 2.9316\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5562 - val_loss: 2.9137\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5569 - val_loss: 2.9371\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5591 - val_loss: 2.9174\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5611 - val_loss: 2.9459\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5647 - val_loss: 2.9213\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5678 - val_loss: 2.9542\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5710 - val_loss: 2.9261\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5758 - val_loss: 2.9577\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5741 - val_loss: 2.9241\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5738 - val_loss: 2.9521\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5691 - val_loss: 2.9182\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5655 - val_loss: 2.9429\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5609 - val_loss: 2.9137\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2.5563 - val_loss: 2.9340\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5525 - val_loss: 2.9102\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5483 - val_loss: 2.9281\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5466 - val_loss: 2.9092\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5439 - val_loss: 2.9269\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5438 - val_loss: 2.9095\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5419 - val_loss: 2.9269\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5423 - val_loss: 2.9092\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5417 - val_loss: 2.9299\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5434 - val_loss: 2.9120\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5456 - val_loss: 2.9388\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5495 - val_loss: 2.9174\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5529 - val_loss: 2.9486\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5571 - val_loss: 2.9229\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.5631 - val_loss: 2.9547\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5623 - val_loss: 2.9231\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5642 - val_loss: 2.9509\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5593 - val_loss: 2.9170\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5561 - val_loss: 2.9411\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.5506 - val_loss: 2.9121\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5470 - val_loss: 2.9334\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5431 - val_loss: 2.9086\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5390 - val_loss: 2.9278\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5369 - val_loss: 2.9066\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5339 - val_loss: 2.9256\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5337 - val_loss: 2.9069\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5323 - val_loss: 2.9275\n",
      "Epoch 475/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5336 - val_loss: 2.9089\n",
      "Epoch 476/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5341 - val_loss: 2.9325\n",
      "Epoch 477/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5366 - val_loss: 2.9125\n",
      "Epoch 478/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5396 - val_loss: 2.9407\n",
      "Epoch 479/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5429 - val_loss: 2.9179\n",
      "Epoch 480/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5478 - val_loss: 2.9484\n",
      "Epoch 481/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5492 - val_loss: 2.9198\n",
      "Epoch 482/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5518 - val_loss: 2.9476\n",
      "Epoch 483/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5486 - val_loss: 2.9162\n",
      "Epoch 484/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5472 - val_loss: 2.9414\n",
      "Epoch 485/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5431 - val_loss: 2.9125\n",
      "Epoch 486/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5409 - val_loss: 2.9348\n",
      "Epoch 487/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5363 - val_loss: 2.9093\n",
      "Epoch 488/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5348 - val_loss: 2.9306\n",
      "Epoch 489/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5318 - val_loss: 2.9072\n",
      "Epoch 490/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5300 - val_loss: 2.9275\n",
      "Epoch 491/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5282 - val_loss: 2.9059\n",
      "Epoch 492/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5263 - val_loss: 2.9265\n",
      "Epoch 493/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5257 - val_loss: 2.9063\n",
      "Epoch 494/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5251 - val_loss: 2.9280\n",
      "Epoch 495/600\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 2.5257 - val_loss: 2.9081\n",
      "Epoch 496/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5270 - val_loss: 2.9319\n",
      "Epoch 497/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5281 - val_loss: 2.9113\n",
      "Epoch 498/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5311 - val_loss: 2.9389\n",
      "Epoch 499/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5334 - val_loss: 2.9155\n",
      "Epoch 500/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5370 - val_loss: 2.9428\n",
      "Epoch 501/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5363 - val_loss: 2.9150\n",
      "Epoch 502/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5368 - val_loss: 2.9401\n",
      "Epoch 503/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5339 - val_loss: 2.9122\n",
      "Epoch 504/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5325 - val_loss: 2.9355\n",
      "Epoch 505/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5292 - val_loss: 2.9099\n",
      "Epoch 506/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5283 - val_loss: 2.9321\n",
      "Epoch 507/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5251 - val_loss: 2.9081\n",
      "Epoch 508/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5246 - val_loss: 2.9289\n",
      "Epoch 509/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5216 - val_loss: 2.9065\n",
      "Epoch 510/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5208 - val_loss: 2.9265\n",
      "Epoch 511/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5185 - val_loss: 2.9055\n",
      "Epoch 512/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5180 - val_loss: 2.9260\n",
      "Epoch 513/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5167 - val_loss: 2.9059\n",
      "Epoch 514/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5172 - val_loss: 2.9275\n",
      "Epoch 515/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5168 - val_loss: 2.9076\n",
      "Epoch 516/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5184 - val_loss: 2.9317\n",
      "Epoch 517/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5193 - val_loss: 2.9105\n",
      "Epoch 518/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5221 - val_loss: 2.9366\n",
      "Epoch 519/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.5230 - val_loss: 2.9126\n",
      "Epoch 520/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5250 - val_loss: 2.9380\n",
      "Epoch 521/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5239 - val_loss: 2.9118\n",
      "Epoch 522/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5239 - val_loss: 2.9357\n",
      "Epoch 523/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.5215 - val_loss: 2.9099\n",
      "Epoch 524/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.5204 - val_loss: 2.9325\n",
      "Epoch 525/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.5178 - val_loss: 2.9081\n",
      "Epoch 526/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5169 - val_loss: 2.9301\n",
      "Epoch 527/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5148 - val_loss: 2.9069\n",
      "Epoch 528/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5142 - val_loss: 2.9278\n",
      "Epoch 529/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5118 - val_loss: 2.9057\n",
      "Epoch 530/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5114 - val_loss: 2.9251\n",
      "Epoch 531/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5087 - val_loss: 2.9045\n",
      "Epoch 532/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5082 - val_loss: 2.9232\n",
      "Epoch 533/600\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 2.5060 - val_loss: 2.9038\n",
      "Epoch 534/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5059 - val_loss: 2.9232\n",
      "Epoch 535/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5047 - val_loss: 2.9045\n",
      "Epoch 536/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5056 - val_loss: 2.9256\n",
      "Epoch 537/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.5055 - val_loss: 2.9065\n",
      "Epoch 538/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.5074 - val_loss: 2.9309\n",
      "Epoch 539/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.5094 - val_loss: 2.9100\n",
      "Epoch 540/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5124 - val_loss: 2.9362\n",
      "Epoch 541/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5138 - val_loss: 2.9120\n",
      "Epoch 542/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 2.5155 - val_loss: 2.9369\n",
      "Epoch 543/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5145 - val_loss: 2.9107\n",
      "Epoch 544/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5133 - val_loss: 2.9330\n",
      "Epoch 545/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5104 - val_loss: 2.9078\n",
      "Epoch 546/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5085 - val_loss: 2.9293\n",
      "Epoch 547/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5061 - val_loss: 2.9061\n",
      "Epoch 548/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5046 - val_loss: 2.9273\n",
      "Epoch 549/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5033 - val_loss: 2.9051\n",
      "Epoch 550/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5024 - val_loss: 2.9264\n",
      "Epoch 551/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5017 - val_loss: 2.9049\n",
      "Epoch 552/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5010 - val_loss: 2.9265\n",
      "Epoch 553/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5008 - val_loss: 2.9051\n",
      "Epoch 554/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5005 - val_loss: 2.9273\n",
      "Epoch 555/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5006 - val_loss: 2.9058\n",
      "Epoch 556/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5006 - val_loss: 2.9283\n",
      "Epoch 557/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.5008 - val_loss: 2.9066\n",
      "Epoch 558/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5009 - val_loss: 2.9290\n",
      "Epoch 559/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5008 - val_loss: 2.9069\n",
      "Epoch 560/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5007 - val_loss: 2.9288\n",
      "Epoch 561/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5002 - val_loss: 2.9065\n",
      "Epoch 562/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.4994 - val_loss: 2.9279\n",
      "Epoch 563/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.4987 - val_loss: 2.9057\n",
      "Epoch 564/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.4973 - val_loss: 2.9265\n",
      "Epoch 565/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.4964 - val_loss: 2.9048\n",
      "Epoch 566/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.4949 - val_loss: 2.9253\n",
      "Epoch 567/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.4944 - val_loss: 2.9043\n",
      "Epoch 568/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.4932 - val_loss: 2.9249\n",
      "Epoch 569/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.4932 - val_loss: 2.9044\n",
      "Epoch 570/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.4923 - val_loss: 2.9252\n",
      "Epoch 571/600\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4927 - val_loss: 2.9049\n",
      "Epoch 572/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.4925 - val_loss: 2.9264\n",
      "Epoch 573/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4933 - val_loss: 2.9058\n",
      "Epoch 574/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4933 - val_loss: 2.9279\n",
      "Epoch 575/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.4945 - val_loss: 2.9064\n",
      "Epoch 576/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.4935 - val_loss: 2.9277\n",
      "Epoch 577/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4937 - val_loss: 2.9057\n",
      "Epoch 578/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.4919 - val_loss: 2.9265\n",
      "Epoch 579/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.4918 - val_loss: 2.9049\n",
      "Epoch 580/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.4900 - val_loss: 2.9250\n",
      "Epoch 581/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.4897 - val_loss: 2.9040\n",
      "Epoch 582/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.4882 - val_loss: 2.9241\n",
      "Epoch 583/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4882 - val_loss: 2.9037\n",
      "Epoch 584/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.4868 - val_loss: 2.9234\n",
      "Epoch 585/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.4870 - val_loss: 2.9032\n",
      "Epoch 586/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4852 - val_loss: 2.9220\n",
      "Epoch 587/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4849 - val_loss: 2.9022\n",
      "Epoch 588/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4823 - val_loss: 2.9194\n",
      "Epoch 589/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4813 - val_loss: 2.9007\n",
      "Epoch 590/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.4792 - val_loss: 2.9177\n",
      "Epoch 591/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.4786 - val_loss: 2.9003\n",
      "Epoch 592/600\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 2.4774 - val_loss: 2.9160\n",
      "Epoch 593/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.4767 - val_loss: 2.8999\n",
      "Epoch 594/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.4763 - val_loss: 2.9155\n",
      "Epoch 595/600\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.4759 - val_loss: 2.9002\n",
      "Epoch 596/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.4761 - val_loss: 2.9178\n",
      "Epoch 597/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.4775 - val_loss: 2.9022\n",
      "Epoch 598/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.4782 - val_loss: 2.9249\n",
      "Epoch 599/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.4833 - val_loss: 2.9071\n",
      "Epoch 600/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.4848 - val_loss: 2.9339\n",
      "Epoch 1/600\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 56.3782 - val_loss: 55.1934\n",
      "Epoch 2/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 54.8375 - val_loss: 52.7848\n",
      "Epoch 3/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 52.4274 - val_loss: 49.7058\n",
      "Epoch 4/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 49.3471 - val_loss: 45.9492\n",
      "Epoch 5/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 45.5919 - val_loss: 41.4238\n",
      "Epoch 6/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 41.0753 - val_loss: 36.1217\n",
      "Epoch 7/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 35.7951 - val_loss: 30.4155\n",
      "Epoch 8/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 30.1309 - val_loss: 25.2019\n",
      "Epoch 9/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 24.9413 - val_loss: 21.5801\n",
      "Epoch 10/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 21.3214 - val_loss: 19.5728\n",
      "Epoch 11/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 19.2735 - val_loss: 18.3710\n",
      "Epoch 12/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 18.0027 - val_loss: 17.6317\n",
      "Epoch 13/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.2204 - val_loss: 17.1732\n",
      "Epoch 14/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 16.7492 - val_loss: 16.8717\n",
      "Epoch 15/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 16.4545 - val_loss: 16.6262\n",
      "Epoch 16/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 16.2243 - val_loss: 16.3426\n",
      "Epoch 17/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 15.9589 - val_loss: 15.9044\n",
      "Epoch 18/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 15.5404 - val_loss: 15.1173\n",
      "Epoch 19/600\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 14.7796 - val_loss: 13.7511\n",
      "Epoch 20/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 13.4790 - val_loss: 12.2454\n",
      "Epoch 21/600\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 12.1120 - val_loss: 11.1210\n",
      "Epoch 22/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 11.1893 - val_loss: 10.0821\n",
      "Epoch 23/600\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 10.2428 - val_loss: 9.2289\n",
      "Epoch 24/600\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 9.4401 - val_loss: 8.6778\n",
      "Epoch 25/600\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 8.9273 - val_loss: 8.3507\n",
      "Epoch 26/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 8.6239 - val_loss: 8.2078\n",
      "Epoch 27/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 8.4747 - val_loss: 8.0950\n",
      "Epoch 28/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.3520 - val_loss: 7.9821\n",
      "Epoch 29/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 8.2286 - val_loss: 7.8838\n",
      "Epoch 30/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 8.1225 - val_loss: 7.7917\n",
      "Epoch 31/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 8.0233 - val_loss: 7.7047\n",
      "Epoch 32/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 7.9281 - val_loss: 7.6182\n",
      "Epoch 33/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 7.8332 - val_loss: 7.5324\n",
      "Epoch 34/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 7.7394 - val_loss: 7.4455\n",
      "Epoch 35/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 7.6452 - val_loss: 7.3562\n",
      "Epoch 36/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.5483 - val_loss: 7.2635\n",
      "Epoch 37/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.4470 - val_loss: 7.1666\n",
      "Epoch 38/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 7.3406 - val_loss: 7.0652\n",
      "Epoch 39/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.2295 - val_loss: 6.9600\n",
      "Epoch 40/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.1149 - val_loss: 6.8524\n",
      "Epoch 41/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.9980 - val_loss: 6.7432\n",
      "Epoch 42/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.8800 - val_loss: 6.6330\n",
      "Epoch 43/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 6.7616 - val_loss: 6.5220\n",
      "Epoch 44/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 6.6431 - val_loss: 6.4102\n",
      "Epoch 45/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 6.5244 - val_loss: 6.2972\n",
      "Epoch 46/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 6.4050 - val_loss: 6.1826\n",
      "Epoch 47/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 6.2845 - val_loss: 6.0664\n",
      "Epoch 48/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.1625 - val_loss: 5.9488\n",
      "Epoch 49/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.0390 - val_loss: 5.8305\n",
      "Epoch 50/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 5.9144 - val_loss: 5.7122\n",
      "Epoch 51/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.7894 - val_loss: 5.5946\n",
      "Epoch 52/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 5.6646 - val_loss: 5.4788\n",
      "Epoch 53/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.5411 - val_loss: 5.3651\n",
      "Epoch 54/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 5.4192 - val_loss: 5.2540\n",
      "Epoch 55/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 5.2995 - val_loss: 5.1456\n",
      "Epoch 56/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 5.1823 - val_loss: 5.0400\n",
      "Epoch 57/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 5.0682 - val_loss: 4.9377\n",
      "Epoch 58/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 4.9579 - val_loss: 4.8391\n",
      "Epoch 59/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.8524 - val_loss: 4.7448\n",
      "Epoch 60/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 4.7520 - val_loss: 4.6553\n",
      "Epoch 61/600\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 4.6569 - val_loss: 4.5709\n",
      "Epoch 62/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 4.5675 - val_loss: 4.4924\n",
      "Epoch 63/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 4.4847 - val_loss: 4.4198\n",
      "Epoch 64/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 4.4087 - val_loss: 4.3531\n",
      "Epoch 65/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 4.3397 - val_loss: 4.2922\n",
      "Epoch 66/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.2765 - val_loss: 4.2358\n",
      "Epoch 67/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.2182 - val_loss: 4.1838\n",
      "Epoch 68/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.1639 - val_loss: 4.1351\n",
      "Epoch 69/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.1128 - val_loss: 4.0887\n",
      "Epoch 70/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.0645 - val_loss: 4.0445\n",
      "Epoch 71/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 4.0185 - val_loss: 4.0027\n",
      "Epoch 72/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.9754 - val_loss: 3.9639\n",
      "Epoch 73/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.9351 - val_loss: 3.9273\n",
      "Epoch 74/600\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 3.8969 - val_loss: 3.8925\n",
      "Epoch 75/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.8605 - val_loss: 3.8596\n",
      "Epoch 76/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.8259 - val_loss: 3.8283\n",
      "Epoch 77/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.7926 - val_loss: 3.7982\n",
      "Epoch 78/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.7608 - val_loss: 3.7696\n",
      "Epoch 79/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.7305 - val_loss: 3.7427\n",
      "Epoch 80/600\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 3.7016 - val_loss: 3.7178\n",
      "Epoch 81/600\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3.6740 - val_loss: 3.6963\n",
      "Epoch 82/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.6474 - val_loss: 3.6760\n",
      "Epoch 83/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.6215 - val_loss: 3.6562\n",
      "Epoch 84/600\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 3.5965 - val_loss: 3.6369\n",
      "Epoch 85/600\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 3.5723 - val_loss: 3.6179\n",
      "Epoch 86/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.5487 - val_loss: 3.5997\n",
      "Epoch 87/600\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 3.5256 - val_loss: 3.5833\n",
      "Epoch 88/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 3.5030 - val_loss: 3.5669\n",
      "Epoch 89/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.4809 - val_loss: 3.5521\n",
      "Epoch 90/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.4594 - val_loss: 3.5373\n",
      "Epoch 91/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 3.4388 - val_loss: 3.5238\n",
      "Epoch 92/600\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 3.4190 - val_loss: 3.5104\n",
      "Epoch 93/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.3998 - val_loss: 3.4979\n",
      "Epoch 94/600\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 3.3812 - val_loss: 3.4860\n",
      "Epoch 95/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.3632 - val_loss: 3.4746\n",
      "Epoch 96/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.3459 - val_loss: 3.4639\n",
      "Epoch 97/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.3292 - val_loss: 3.4535\n",
      "Epoch 98/600\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 3.3132 - val_loss: 3.4435\n",
      "Epoch 99/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.2978 - val_loss: 3.4337\n",
      "Epoch 100/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.2832 - val_loss: 3.4245\n",
      "Epoch 101/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 3.2691 - val_loss: 3.4156\n",
      "Epoch 102/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 3.2557 - val_loss: 3.4072\n",
      "Epoch 103/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.2429 - val_loss: 3.3995\n",
      "Epoch 104/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.2308 - val_loss: 3.3923\n",
      "Epoch 105/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 3.2192 - val_loss: 3.3856\n",
      "Epoch 106/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.2082 - val_loss: 3.3792\n",
      "Epoch 107/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.1977 - val_loss: 3.3731\n",
      "Epoch 108/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1876 - val_loss: 3.3674\n",
      "Epoch 109/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 3.1780 - val_loss: 3.3620\n",
      "Epoch 110/600\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 3.1687 - val_loss: 3.3570\n",
      "Epoch 111/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1597 - val_loss: 3.3521\n",
      "Epoch 112/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1510 - val_loss: 3.3474\n",
      "Epoch 113/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1427 - val_loss: 3.3428\n",
      "Epoch 114/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 3.1347 - val_loss: 3.3384\n",
      "Epoch 115/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1269 - val_loss: 3.3340\n",
      "Epoch 116/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.1194 - val_loss: 3.3297\n",
      "Epoch 117/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.1121 - val_loss: 3.3256\n",
      "Epoch 118/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 3.1049 - val_loss: 3.3217\n",
      "Epoch 119/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0979 - val_loss: 3.3179\n",
      "Epoch 120/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0910 - val_loss: 3.3143\n",
      "Epoch 121/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0843 - val_loss: 3.3109\n",
      "Epoch 122/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0777 - val_loss: 3.3076\n",
      "Epoch 123/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.0713 - val_loss: 3.3044\n",
      "Epoch 124/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 3.0651 - val_loss: 3.3014\n",
      "Epoch 125/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0589 - val_loss: 3.2985\n",
      "Epoch 126/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0530 - val_loss: 3.2957\n",
      "Epoch 127/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 3.0474 - val_loss: 3.2934\n",
      "Epoch 128/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.0420 - val_loss: 3.2911\n",
      "Epoch 129/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.0369 - val_loss: 3.2891\n",
      "Epoch 130/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.0319 - val_loss: 3.2870\n",
      "Epoch 131/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 3.0272 - val_loss: 3.2852\n",
      "Epoch 132/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 3.0226 - val_loss: 3.2834\n",
      "Epoch 133/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 3.0182 - val_loss: 3.2817\n",
      "Epoch 134/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 3.0139 - val_loss: 3.2799\n",
      "Epoch 135/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.0097 - val_loss: 3.2779\n",
      "Epoch 136/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 3.0057 - val_loss: 3.2759\n",
      "Epoch 137/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 3.0017 - val_loss: 3.2735\n",
      "Epoch 138/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9978 - val_loss: 3.2712\n",
      "Epoch 139/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.9940 - val_loss: 3.2688\n",
      "Epoch 140/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9903 - val_loss: 3.2666\n",
      "Epoch 141/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.9867 - val_loss: 3.2643\n",
      "Epoch 142/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.9830 - val_loss: 3.2621\n",
      "Epoch 143/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.9795 - val_loss: 3.2598\n",
      "Epoch 144/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9760 - val_loss: 3.2576\n",
      "Epoch 145/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9725 - val_loss: 3.2553\n",
      "Epoch 146/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9691 - val_loss: 3.2530\n",
      "Epoch 147/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9657 - val_loss: 3.2507\n",
      "Epoch 148/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9623 - val_loss: 3.2484\n",
      "Epoch 149/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.9590 - val_loss: 3.2461\n",
      "Epoch 150/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.9557 - val_loss: 3.2438\n",
      "Epoch 151/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9524 - val_loss: 3.2415\n",
      "Epoch 152/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.9491 - val_loss: 3.2392\n",
      "Epoch 153/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.9459 - val_loss: 3.2369\n",
      "Epoch 154/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9426 - val_loss: 3.2347\n",
      "Epoch 155/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.9394 - val_loss: 3.2325\n",
      "Epoch 156/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9362 - val_loss: 3.2303\n",
      "Epoch 157/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.9330 - val_loss: 3.2281\n",
      "Epoch 158/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9299 - val_loss: 3.2259\n",
      "Epoch 159/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9267 - val_loss: 3.2237\n",
      "Epoch 160/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9236 - val_loss: 3.2215\n",
      "Epoch 161/600\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.9204 - val_loss: 3.2193\n",
      "Epoch 162/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9173 - val_loss: 3.2171\n",
      "Epoch 163/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.9142 - val_loss: 3.2149\n",
      "Epoch 164/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.9111 - val_loss: 3.2126\n",
      "Epoch 165/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.9080 - val_loss: 3.2104\n",
      "Epoch 166/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.9050 - val_loss: 3.2081\n",
      "Epoch 167/600\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 2.9019 - val_loss: 3.2058\n",
      "Epoch 168/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8989 - val_loss: 3.2035\n",
      "Epoch 169/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8958 - val_loss: 3.2013\n",
      "Epoch 170/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8928 - val_loss: 3.1990\n",
      "Epoch 171/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.8898 - val_loss: 3.1968\n",
      "Epoch 172/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8869 - val_loss: 3.1947\n",
      "Epoch 173/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.8839 - val_loss: 3.1927\n",
      "Epoch 174/600\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.8810 - val_loss: 3.1908\n",
      "Epoch 175/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.8782 - val_loss: 3.1900\n",
      "Epoch 176/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8754 - val_loss: 3.1860\n",
      "Epoch 177/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8727 - val_loss: 3.1868\n",
      "Epoch 178/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.8699 - val_loss: 3.1827\n",
      "Epoch 179/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8672 - val_loss: 3.1821\n",
      "Epoch 180/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8644 - val_loss: 3.1801\n",
      "Epoch 181/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8617 - val_loss: 3.1777\n",
      "Epoch 182/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8591 - val_loss: 3.1762\n",
      "Epoch 183/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.8564 - val_loss: 3.1741\n",
      "Epoch 184/600\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8538 - val_loss: 3.1720\n",
      "Epoch 185/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8511 - val_loss: 3.1701\n",
      "Epoch 186/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8485 - val_loss: 3.1682\n",
      "Epoch 187/600\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.8459 - val_loss: 3.1662\n",
      "Epoch 188/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8434 - val_loss: 3.1643\n",
      "Epoch 189/600\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8408 - val_loss: 3.1624\n",
      "Epoch 190/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8382 - val_loss: 3.1606\n",
      "Epoch 191/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.8357 - val_loss: 3.1587\n",
      "Epoch 192/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8332 - val_loss: 3.1568\n",
      "Epoch 193/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8306 - val_loss: 3.1550\n",
      "Epoch 194/600\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 2.8281 - val_loss: 3.1531\n",
      "Epoch 195/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8256 - val_loss: 3.1512\n",
      "Epoch 196/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.8232 - val_loss: 3.1494\n",
      "Epoch 197/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8207 - val_loss: 3.1476\n",
      "Epoch 198/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.8183 - val_loss: 3.1457\n",
      "Epoch 199/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8159 - val_loss: 3.1439\n",
      "Epoch 200/600\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 2.8135 - val_loss: 3.1421\n",
      "Epoch 201/600\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.8111 - val_loss: 3.1403\n",
      "Epoch 202/600\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 2.8087 - val_loss: 3.1385\n",
      "Epoch 203/600\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 2.8064 - val_loss: 3.1367\n",
      "Epoch 204/600\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 2.8041 - val_loss: 3.1349\n",
      "Epoch 205/600\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.8018 - val_loss: 3.1339\n",
      "Epoch 206/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7996 - val_loss: 3.1309\n",
      "Epoch 207/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.7974 - val_loss: 3.1307\n",
      "Epoch 208/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.7951 - val_loss: 3.1270\n",
      "Epoch 209/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7929 - val_loss: 3.1269\n",
      "Epoch 210/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7906 - val_loss: 3.1234\n",
      "Epoch 211/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.7883 - val_loss: 3.1230\n",
      "Epoch 212/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7861 - val_loss: 3.1198\n",
      "Epoch 213/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7839 - val_loss: 3.1193\n",
      "Epoch 214/600\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 2.7817 - val_loss: 3.1160\n",
      "Epoch 215/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.7795 - val_loss: 3.1155\n",
      "Epoch 216/600\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.7773 - val_loss: 3.1122\n",
      "Epoch 217/600\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 2.7751 - val_loss: 3.1118\n",
      "Epoch 218/600\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 2.7730 - val_loss: 3.1086\n",
      "Epoch 219/600\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 2.7708 - val_loss: 3.1081\n",
      "Epoch 220/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.7687 - val_loss: 3.1051\n",
      "Epoch 221/600\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 2.7665 - val_loss: 3.1044\n",
      "Epoch 222/600\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 2.7643 - val_loss: 3.1019\n",
      "Epoch 223/600\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 2.7622 - val_loss: 3.1009\n",
      "Epoch 224/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7600 - val_loss: 3.0988\n",
      "Epoch 225/600\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 2.7579 - val_loss: 3.0976\n",
      "Epoch 226/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7558 - val_loss: 3.0958\n",
      "Epoch 227/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7537 - val_loss: 3.0944\n",
      "Epoch 228/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.7516 - val_loss: 3.0927\n",
      "Epoch 229/600\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 2.7495 - val_loss: 3.0912\n",
      "Epoch 230/600\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 2.7475 - val_loss: 3.0896\n",
      "Epoch 231/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.7454 - val_loss: 3.0882\n",
      "Epoch 232/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.7433 - val_loss: 3.0866\n",
      "Epoch 233/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.7413 - val_loss: 3.0853\n",
      "Epoch 234/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7393 - val_loss: 3.0836\n",
      "Epoch 235/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7373 - val_loss: 3.0828\n",
      "Epoch 236/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7353 - val_loss: 3.0799\n",
      "Epoch 237/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7334 - val_loss: 3.0811\n",
      "Epoch 238/600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.7316 - val_loss: 3.0754\n",
      "Epoch 239/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7300 - val_loss: 3.0791\n",
      "Epoch 240/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.7280 - val_loss: 3.0720\n",
      "Epoch 241/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7265 - val_loss: 3.0765\n",
      "Epoch 242/600\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2.7244 - val_loss: 3.0695\n",
      "Epoch 243/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.7228 - val_loss: 3.0737\n",
      "Epoch 244/600\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 2.7207 - val_loss: 3.0669\n",
      "Epoch 245/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.7190 - val_loss: 3.0708\n",
      "Epoch 246/600\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 2.7168 - val_loss: 3.0641\n",
      "Epoch 247/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.7151 - val_loss: 3.0680\n",
      "Epoch 248/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7130 - val_loss: 3.0617\n",
      "Epoch 249/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7113 - val_loss: 3.0653\n",
      "Epoch 250/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7093 - val_loss: 3.0597\n",
      "Epoch 251/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7076 - val_loss: 3.0627\n",
      "Epoch 252/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7057 - val_loss: 3.0576\n",
      "Epoch 253/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.7040 - val_loss: 3.0604\n",
      "Epoch 254/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.7022 - val_loss: 3.0555\n",
      "Epoch 255/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7006 - val_loss: 3.0583\n",
      "Epoch 256/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6988 - val_loss: 3.0532\n",
      "Epoch 257/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6973 - val_loss: 3.0567\n",
      "Epoch 258/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6956 - val_loss: 3.0509\n",
      "Epoch 259/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6944 - val_loss: 3.0556\n",
      "Epoch 260/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6927 - val_loss: 3.0486\n",
      "Epoch 261/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6920 - val_loss: 3.0571\n",
      "Epoch 262/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6913 - val_loss: 3.0468\n",
      "Epoch 263/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.6919 - val_loss: 3.0629\n",
      "Epoch 264/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6932 - val_loss: 3.0477\n",
      "Epoch 265/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.6979 - val_loss: 3.0786\n",
      "Epoch 266/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.7048 - val_loss: 3.0541\n",
      "Epoch 267/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.7068 - val_loss: 3.0851\n",
      "Epoch 268/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.7105 - val_loss: 3.0548\n",
      "Epoch 269/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7058 - val_loss: 3.0826\n",
      "Epoch 270/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.7064 - val_loss: 3.0515\n",
      "Epoch 271/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.7009 - val_loss: 3.0786\n",
      "Epoch 272/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7006 - val_loss: 3.0493\n",
      "Epoch 273/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.6964 - val_loss: 3.0770\n",
      "Epoch 274/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6968 - val_loss: 3.0485\n",
      "Epoch 275/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6935 - val_loss: 3.0760\n",
      "Epoch 276/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6936 - val_loss: 3.0480\n",
      "Epoch 277/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6910 - val_loss: 3.0751\n",
      "Epoch 278/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6908 - val_loss: 3.0475\n",
      "Epoch 279/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6887 - val_loss: 3.0743\n",
      "Epoch 280/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6880 - val_loss: 3.0469\n",
      "Epoch 281/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6865 - val_loss: 3.0733\n",
      "Epoch 282/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6852 - val_loss: 3.0463\n",
      "Epoch 283/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6843 - val_loss: 3.0724\n",
      "Epoch 284/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6825 - val_loss: 3.0457\n",
      "Epoch 285/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6820 - val_loss: 3.0716\n",
      "Epoch 286/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6800 - val_loss: 3.0451\n",
      "Epoch 287/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6799 - val_loss: 3.0707\n",
      "Epoch 288/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6774 - val_loss: 3.0445\n",
      "Epoch 289/600\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6777 - val_loss: 3.0697\n",
      "Epoch 290/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6748 - val_loss: 3.0437\n",
      "Epoch 291/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6754 - val_loss: 3.0686\n",
      "Epoch 292/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.6721 - val_loss: 3.0428\n",
      "Epoch 293/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6730 - val_loss: 3.0674\n",
      "Epoch 294/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6693 - val_loss: 3.0417\n",
      "Epoch 295/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6704 - val_loss: 3.0659\n",
      "Epoch 296/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6662 - val_loss: 3.0404\n",
      "Epoch 297/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6675 - val_loss: 3.0625\n",
      "Epoch 298/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6614 - val_loss: 3.0368\n",
      "Epoch 299/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6617 - val_loss: 3.0592\n",
      "Epoch 300/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6566 - val_loss: 3.0320\n",
      "Epoch 301/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6540 - val_loss: 3.0554\n",
      "Epoch 302/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6510 - val_loss: 3.0298\n",
      "Epoch 303/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6508 - val_loss: 3.0547\n",
      "Epoch 304/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6490 - val_loss: 3.0296\n",
      "Epoch 305/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6498 - val_loss: 3.0548\n",
      "Epoch 306/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6478 - val_loss: 3.0298\n",
      "Epoch 307/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6489 - val_loss: 3.0551\n",
      "Epoch 308/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.6466 - val_loss: 3.0300\n",
      "Epoch 309/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6476 - val_loss: 3.0551\n",
      "Epoch 310/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6449 - val_loss: 3.0298\n",
      "Epoch 311/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6459 - val_loss: 3.0547\n",
      "Epoch 312/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6428 - val_loss: 3.0292\n",
      "Epoch 313/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6436 - val_loss: 3.0540\n",
      "Epoch 314/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.6405 - val_loss: 3.0284\n",
      "Epoch 315/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6412 - val_loss: 3.0534\n",
      "Epoch 316/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6384 - val_loss: 3.0278\n",
      "Epoch 317/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6391 - val_loss: 3.0529\n",
      "Epoch 318/600\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 2.6365 - val_loss: 3.0274\n",
      "Epoch 319/600\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6373 - val_loss: 3.0526\n",
      "Epoch 320/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.6347 - val_loss: 3.0270\n",
      "Epoch 321/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6354 - val_loss: 3.0522\n",
      "Epoch 322/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.6327 - val_loss: 3.0265\n",
      "Epoch 323/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.6333 - val_loss: 3.0517\n",
      "Epoch 324/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6307 - val_loss: 3.0259\n",
      "Epoch 325/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.6311 - val_loss: 3.0512\n",
      "Epoch 326/600\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 2.6286 - val_loss: 3.0252\n",
      "Epoch 327/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6287 - val_loss: 3.0506\n",
      "Epoch 328/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.6265 - val_loss: 3.0246\n",
      "Epoch 329/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.6265 - val_loss: 3.0501\n",
      "Epoch 330/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.6245 - val_loss: 3.0242\n",
      "Epoch 331/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.6245 - val_loss: 3.0499\n",
      "Epoch 332/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6227 - val_loss: 3.0241\n",
      "Epoch 333/600\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.6230 - val_loss: 3.0499\n",
      "Epoch 334/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.6212 - val_loss: 3.0243\n",
      "Epoch 335/600\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.6217 - val_loss: 3.0501\n",
      "Epoch 336/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6199 - val_loss: 3.0245\n",
      "Epoch 337/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6206 - val_loss: 3.0503\n",
      "Epoch 338/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.6186 - val_loss: 3.0248\n",
      "Epoch 339/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6195 - val_loss: 3.0505\n",
      "Epoch 340/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.6175 - val_loss: 3.0252\n",
      "Epoch 341/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.6187 - val_loss: 3.0510\n",
      "Epoch 342/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6166 - val_loss: 3.0263\n",
      "Epoch 343/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.6191 - val_loss: 3.0543\n",
      "Epoch 344/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6187 - val_loss: 3.0308\n",
      "Epoch 345/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.6236 - val_loss: 3.0590\n",
      "Epoch 346/600\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.6225 - val_loss: 3.0336\n",
      "Epoch 347/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6254 - val_loss: 3.0599\n",
      "Epoch 348/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6221 - val_loss: 3.0333\n",
      "Epoch 349/600\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6230 - val_loss: 3.0580\n",
      "Epoch 350/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.6184 - val_loss: 3.0308\n",
      "Epoch 351/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6182 - val_loss: 3.0552\n",
      "Epoch 352/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.6138 - val_loss: 3.0278\n",
      "Epoch 353/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.6134 - val_loss: 3.0527\n",
      "Epoch 354/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.6098 - val_loss: 3.0258\n",
      "Epoch 355/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.6096 - val_loss: 3.0506\n",
      "Epoch 356/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6064 - val_loss: 3.0229\n",
      "Epoch 357/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.6042 - val_loss: 3.0452\n",
      "Epoch 358/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5997 - val_loss: 3.0186\n",
      "Epoch 359/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5960 - val_loss: 3.0391\n",
      "Epoch 360/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5925 - val_loss: 3.0152\n",
      "Epoch 361/600\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.5892 - val_loss: 3.0359\n",
      "Epoch 362/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5882 - val_loss: 3.0143\n",
      "Epoch 363/600\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5862 - val_loss: 3.0352\n",
      "Epoch 364/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5862 - val_loss: 3.0147\n",
      "Epoch 365/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5852 - val_loss: 3.0378\n",
      "Epoch 366/600\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5870 - val_loss: 3.0166\n",
      "Epoch 367/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5871 - val_loss: 3.0421\n",
      "Epoch 368/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5897 - val_loss: 3.0202\n",
      "Epoch 369/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5916 - val_loss: 3.0494\n",
      "Epoch 370/600\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5960 - val_loss: 3.0286\n",
      "Epoch 371/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6028 - val_loss: 3.0610\n",
      "Epoch 372/600\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6084 - val_loss: 3.0377\n",
      "Epoch 373/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6126 - val_loss: 3.0624\n",
      "Epoch 374/600\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6089 - val_loss: 3.0341\n",
      "Epoch 375/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6063 - val_loss: 3.0580\n",
      "Epoch 376/600\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6022 - val_loss: 3.0294\n",
      "Epoch 377/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5989 - val_loss: 3.0531\n",
      "Epoch 378/600\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5953 - val_loss: 3.0250\n",
      "Epoch 379/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5927 - val_loss: 3.0490\n",
      "Epoch 380/600\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.5901 - val_loss: 3.0223\n",
      "Epoch 381/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5885 - val_loss: 3.0468\n",
      "Epoch 382/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5870 - val_loss: 3.0212\n",
      "Epoch 383/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5858 - val_loss: 3.0462\n",
      "Epoch 384/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5853 - val_loss: 3.0210\n",
      "Epoch 385/600\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.5839 - val_loss: 3.0461\n",
      "Epoch 386/600\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2.5839 - val_loss: 3.0213\n",
      "Epoch 387/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5827 - val_loss: 3.0464\n",
      "Epoch 388/600\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5829 - val_loss: 3.0218\n",
      "Epoch 389/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5820 - val_loss: 3.0469\n",
      "Epoch 390/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5822 - val_loss: 3.0224\n",
      "Epoch 391/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5814 - val_loss: 3.0473\n",
      "Epoch 392/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5814 - val_loss: 3.0227\n",
      "Epoch 393/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5803 - val_loss: 3.0472\n",
      "Epoch 394/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5802 - val_loss: 3.0225\n",
      "Epoch 395/600\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.5786 - val_loss: 3.0467\n",
      "Epoch 396/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5783 - val_loss: 3.0219\n",
      "Epoch 397/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5762 - val_loss: 3.0456\n",
      "Epoch 398/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5758 - val_loss: 3.0206\n",
      "Epoch 399/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5727 - val_loss: 3.0411\n",
      "Epoch 400/600\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.5701 - val_loss: 3.0167\n",
      "Epoch 401/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5660 - val_loss: 3.0351\n",
      "Epoch 402/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5630 - val_loss: 3.0128\n",
      "Epoch 403/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5596 - val_loss: 3.0319\n",
      "Epoch 404/600\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5589 - val_loss: 3.0113\n",
      "Epoch 405/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5571 - val_loss: 3.0317\n",
      "Epoch 406/600\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.5575 - val_loss: 3.0116\n",
      "Epoch 407/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5570 - val_loss: 3.0333\n",
      "Epoch 408/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5583 - val_loss: 3.0134\n",
      "Epoch 409/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5586 - val_loss: 3.0388\n",
      "Epoch 410/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5630 - val_loss: 3.0197\n",
      "Epoch 411/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5650 - val_loss: 3.0472\n",
      "Epoch 412/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5702 - val_loss: 3.0294\n",
      "Epoch 413/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5762 - val_loss: 3.0629\n",
      "Epoch 414/600\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.5870 - val_loss: 3.0417\n",
      "Epoch 415/600\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.5899 - val_loss: 3.0642\n",
      "Epoch 416/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5875 - val_loss: 3.0337\n",
      "Epoch 417/600\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.5790 - val_loss: 3.0514\n",
      "Epoch 418/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5716 - val_loss: 3.0215\n",
      "Epoch 419/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5619 - val_loss: 3.0405\n",
      "Epoch 420/600\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5584 - val_loss: 3.0166\n",
      "Epoch 421/600\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.5552 - val_loss: 3.0347\n",
      "Epoch 422/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5515 - val_loss: 3.0102\n",
      "Epoch 423/600\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5469 - val_loss: 3.0297\n",
      "Epoch 424/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5463 - val_loss: 3.0088\n",
      "Epoch 425/600\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.5448 - val_loss: 3.0304\n",
      "Epoch 426/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5463 - val_loss: 3.0101\n",
      "Epoch 427/600\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.5458 - val_loss: 3.0334\n",
      "Epoch 428/600\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.5482 - val_loss: 3.0126\n",
      "Epoch 429/600\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.5472 - val_loss: 3.0370\n",
      "Epoch 430/600\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5511 - val_loss: 3.0213\n",
      "Epoch 431/600\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.5559 - val_loss: 3.0515\n",
      "Epoch 432/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5656 - val_loss: 3.0333\n",
      "Epoch 433/600\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.5693 - val_loss: 3.0618\n",
      "Epoch 434/600\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.5753 - val_loss: 3.0379\n",
      "Epoch 435/600\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.5733 - val_loss: 3.0583\n",
      "Epoch 436/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5700 - val_loss: 3.0277\n",
      "Epoch 437/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5602 - val_loss: 3.0481\n",
      "Epoch 438/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5577 - val_loss: 3.0198\n",
      "Epoch 439/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5491 - val_loss: 3.0382\n",
      "Epoch 440/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5460 - val_loss: 3.0151\n",
      "Epoch 441/600\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.5429 - val_loss: 3.0338\n",
      "Epoch 442/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5412 - val_loss: 3.0128\n",
      "Epoch 443/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5403 - val_loss: 3.0334\n",
      "Epoch 444/600\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.5409 - val_loss: 3.0150\n",
      "Epoch 445/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5426 - val_loss: 3.0418\n",
      "Epoch 446/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5492 - val_loss: 3.0213\n",
      "Epoch 447/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5489 - val_loss: 3.0516\n",
      "Epoch 448/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5585 - val_loss: 3.0287\n",
      "Epoch 449/600\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.5558 - val_loss: 3.0532\n",
      "Epoch 450/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5587 - val_loss: 3.0266\n",
      "Epoch 451/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5512 - val_loss: 3.0487\n",
      "Epoch 452/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5522 - val_loss: 3.0221\n",
      "Epoch 453/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5441 - val_loss: 3.0445\n",
      "Epoch 454/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5464 - val_loss: 3.0195\n",
      "Epoch 455/600\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.5402 - val_loss: 3.0402\n",
      "Epoch 456/600\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.5411 - val_loss: 3.0165\n",
      "Epoch 457/600\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.5362 - val_loss: 3.0363\n",
      "Epoch 458/600\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.5364 - val_loss: 3.0146\n",
      "Epoch 459/600\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.5335 - val_loss: 3.0355\n",
      "Epoch 460/600\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.5352 - val_loss: 3.0151\n",
      "Epoch 461/600\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.5334 - val_loss: 3.0380\n",
      "Epoch 462/600\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.5371 - val_loss: 3.0171\n",
      "Epoch 463/600\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 2.5349 - val_loss: 3.0426\n",
      "Epoch 464/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5409 - val_loss: 3.0210\n",
      "Epoch 465/600\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.5385 - val_loss: 3.0475\n",
      "Epoch 466/600\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 2.5453 - val_loss: 3.0239\n",
      "Epoch 467/600\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5409 - val_loss: 3.0476\n",
      "Epoch 468/600\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.5446 - val_loss: 3.0227\n",
      "Epoch 469/600\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.5378 - val_loss: 3.0442\n",
      "Epoch 470/600\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.5396 - val_loss: 3.0198\n",
      "Epoch 471/600\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.5328 - val_loss: 3.0413\n",
      "Epoch 472/600\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.5355 - val_loss: 3.0181\n",
      "Epoch 473/600\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 2.5301 - val_loss: 3.0365\n",
      "Epoch 474/600\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.5295 - val_loss: 3.0148\n"
     ]
    }
   ],
   "source": [
    "historyVal_ES = []\n",
    "historyTr_ES = []\n",
    "\n",
    "el=EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "\n",
    "for traing_index, test_index in kf.split(X_dev):\n",
    "    x_tr = X_dev[traing_index]\n",
    "    y_tr = y_dev[traing_index]\n",
    "    x_val = X_dev[test_index]\n",
    "    y_val = y_dev[test_index]\n",
    "    model=create_model_ES()\n",
    "    #model.add_loss(MEE_k)\n",
    "    history=model.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=600, \n",
    "                      batch_size=1036, callbacks=[el]).history\n",
    "    historyVal_ES.append(history['val_loss'])\n",
    "    historyTr_ES.append(history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_k=600\n",
    "for i in range(0,5):\n",
    "    if len(historyVal_ES[i])<min_k:\n",
    "        min_k=len(historyVal_ES[i])\n",
    "for i in range(0,5):\n",
    "    del historyVal_ES[i][min_k:]\n",
    "    del historyTr_ES[i][min_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 56.3667WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 55.9473\n",
      "Epoch 2/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 52.6964WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 52.0790\n",
      "Epoch 3/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 46.3914WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 45.5769\n",
      "Epoch 4/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 37.1260WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 35.9214\n",
      "Epoch 5/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 25.8886WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.9257\n",
      "Epoch 6/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 19.3024WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.9949\n",
      "Epoch 7/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 16.8060WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.5741\n",
      "Epoch 8/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 15.3996WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 15.1720\n",
      "Epoch 9/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 13.8926WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 13.5834\n",
      "Epoch 10/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 11.7152WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 11.6090\n",
      "Epoch 11/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9.9272WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 9.7904\n",
      "Epoch 12/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.8182WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.8330\n",
      "Epoch 13/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.5372WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.5113\n",
      "Epoch 14/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.3413WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.3060\n",
      "Epoch 15/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 8.0591WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 8.1006\n",
      "Epoch 16/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.9984WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 7.9090\n",
      "Epoch 17/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.5767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 7.7230\n",
      "Epoch 18/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.5080WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.5380\n",
      "Epoch 19/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.3430WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.3430\n",
      "Epoch 20/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.2684WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1616\n",
      "Epoch 21/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.9911WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.9723\n",
      "Epoch 22/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.8803WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 6.8294\n",
      "Epoch 23/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.7283WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 6.6338\n",
      "Epoch 24/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.5129WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.4414\n",
      "Epoch 25/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.2447WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.2695\n",
      "Epoch 26/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 6.0381WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.1006\n",
      "Epoch 27/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.9913WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 5.8932\n",
      "Epoch 28/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.7133WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.7169\n",
      "Epoch 29/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.5264WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.4688\n",
      "Epoch 30/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.2943WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2940\n",
      "Epoch 31/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.0695WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.0501\n",
      "Epoch 32/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.8777WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8614\n",
      "Epoch 33/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.7177WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6711\n",
      "Epoch 34/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.4715WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.4601\n",
      "Epoch 35/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.2794WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 4.2985\n",
      "Epoch 36/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.2069WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1713\n",
      "Epoch 37/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.2308WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1769\n",
      "Epoch 38/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.9078WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.9387\n",
      "Epoch 39/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8970WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.8819\n",
      "Epoch 40/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8025WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.8365\n",
      "Epoch 41/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7872WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.7436\n",
      "Epoch 42/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7933WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6841\n",
      "Epoch 43/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.6987WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6512\n",
      "Epoch 44/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7556WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.7090\n",
      "Epoch 45/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5063WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.5354\n",
      "Epoch 46/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5225WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4952\n",
      "Epoch 47/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5210WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4795\n",
      "Epoch 48/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4330WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4415\n",
      "Epoch 49/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4805WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4128\n",
      "Epoch 50/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3936WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.4029\n",
      "Epoch 51/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3398WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.3320\n",
      "Epoch 52/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3491WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3057\n",
      "Epoch 53/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2899WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2975\n",
      "Epoch 54/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3665WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3442\n",
      "Epoch 55/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2692WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2618\n",
      "Epoch 56/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2440WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.2298\n",
      "Epoch 57/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2063WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2205\n",
      "Epoch 58/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4279WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4053\n",
      "Epoch 59/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2306WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2526\n",
      "Epoch 60/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1856WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2139\n",
      "Epoch 61/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2549WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2293\n",
      "Epoch 62/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2073WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1824\n",
      "Epoch 63/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1693WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1389\n",
      "Epoch 64/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1253\n",
      "Epoch 65/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1118WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1057\n",
      "Epoch 66/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0563WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1041\n",
      "Epoch 67/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0817WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0718\n",
      "Epoch 68/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0477WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0544\n",
      "Epoch 69/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0554WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0566\n",
      "Epoch 70/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0436WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0372\n",
      "Epoch 71/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0600WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 3.0270\n",
      "Epoch 72/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0520WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0440\n",
      "Epoch 73/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0646WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0661\n",
      "Epoch 74/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0912WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0513\n",
      "Epoch 75/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3372WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2264\n",
      "Epoch 76/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0678WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0607\n",
      "Epoch 77/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0041\n",
      "Epoch 78/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1666WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1668\n",
      "Epoch 79/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9858WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9921\n",
      "Epoch 80/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9769WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9761\n",
      "Epoch 81/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.2276WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1147\n",
      "Epoch 82/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9129WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9534\n",
      "Epoch 83/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9701WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9535\n",
      "Epoch 84/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9628WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0119\n",
      "Epoch 85/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0169WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0005\n",
      "Epoch 86/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0188WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0026\n",
      "Epoch 87/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9695WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9375\n",
      "Epoch 88/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8140WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9268\n",
      "Epoch 89/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0634WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0199\n",
      "Epoch 90/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9018WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9648\n",
      "Epoch 91/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0784WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0371\n",
      "Epoch 92/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9088WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9086\n",
      "Epoch 93/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9191WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9082\n",
      "Epoch 94/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9119WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9094\n",
      "Epoch 95/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9658\n",
      "Epoch 96/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9308WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9434\n",
      "Epoch 97/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0244WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0276\n",
      "Epoch 98/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9036WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9194\n",
      "Epoch 99/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9081\n",
      "Epoch 100/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9268WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9071\n",
      "Epoch 101/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8696WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9466\n",
      "Epoch 102/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0175WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9695\n",
      "Epoch 103/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0623WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0181\n",
      "Epoch 104/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8877WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8539\n",
      "Epoch 105/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8505WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8757\n",
      "Epoch 106/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8965WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.9090\n",
      "Epoch 107/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9133WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8680\n",
      "Epoch 108/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8850WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8843\n",
      "Epoch 109/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9021WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8568\n",
      "Epoch 110/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9012WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8927\n",
      "Epoch 111/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9618WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9530\n",
      "Epoch 112/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7827WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8260\n",
      "Epoch 113/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8499WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8495\n",
      "Epoch 114/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9372WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8736\n",
      "Epoch 115/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8075WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8295\n",
      "Epoch 116/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8298WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8155\n",
      "Epoch 117/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8051\n",
      "Epoch 118/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9944WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9381\n",
      "Epoch 119/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8753WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8484\n",
      "Epoch 120/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9003WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9130\n",
      "Epoch 121/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7318WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8037\n",
      "Epoch 122/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8494WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8505\n",
      "Epoch 123/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7975WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8017\n",
      "Epoch 124/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0072WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9840\n",
      "Epoch 125/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7675WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7906\n",
      "Epoch 126/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8803WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8302\n",
      "Epoch 127/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8098WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8594\n",
      "Epoch 128/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8528WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8544\n",
      "Epoch 129/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7254WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7744\n",
      "Epoch 130/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7808WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8403\n",
      "Epoch 131/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9651WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.9235\n",
      "Epoch 132/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7444WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7643\n",
      "Epoch 133/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7132WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8052\n",
      "Epoch 134/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7968WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8140\n",
      "Epoch 135/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7248WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7565\n",
      "Epoch 136/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7303WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7503\n",
      "Epoch 137/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8638WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8153\n",
      "Epoch 138/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7689WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7626\n",
      "Epoch 139/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7805WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7755\n",
      "Epoch 140/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8470WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8278\n",
      "Epoch 141/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8331WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8271\n",
      "Epoch 142/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7204WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7367\n",
      "Epoch 143/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8806WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8635\n",
      "Epoch 144/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7890WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7441\n",
      "Epoch 145/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7808WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7441\n",
      "Epoch 146/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7355\n",
      "Epoch 147/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7876WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7969\n",
      "Epoch 148/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7934WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.7834\n",
      "Epoch 149/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7086WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7320\n",
      "Epoch 150/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7385WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7299\n",
      "Epoch 151/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7167WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7311\n",
      "Epoch 152/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8042WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7602\n",
      "Epoch 153/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7664WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8042\n",
      "Epoch 154/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7625WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8176\n",
      "Epoch 155/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.9488WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9207\n",
      "Epoch 156/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7835WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7175\n",
      "Epoch 157/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7217WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7185\n",
      "Epoch 158/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7690WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8182\n",
      "Epoch 159/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6895WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7184\n",
      "Epoch 160/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8454WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.8582\n",
      "Epoch 161/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8192WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8520\n",
      "Epoch 162/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8442WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8300\n",
      "Epoch 163/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7295WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7489\n",
      "Epoch 164/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7952WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7335\n",
      "Epoch 165/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7279\n",
      "Epoch 166/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6886WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7035\n",
      "Epoch 167/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6733WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6986\n",
      "Epoch 168/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7329WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7507\n",
      "Epoch 169/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7585WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7111\n",
      "Epoch 170/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6721WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7265\n",
      "Epoch 171/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6825WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6992\n",
      "Epoch 172/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7464WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6944\n",
      "Epoch 173/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7878WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7741\n",
      "Epoch 174/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6670WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7062\n",
      "Epoch 175/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6467WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7069\n",
      "Epoch 176/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7172\n",
      "Epoch 177/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8729WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8556\n",
      "Epoch 178/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7174WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6946\n",
      "Epoch 179/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6228WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6896\n",
      "Epoch 180/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7031WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7059\n",
      "Epoch 181/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7994WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7691\n",
      "Epoch 182/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6943WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6988\n",
      "Epoch 183/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6981WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7196\n",
      "Epoch 184/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6845WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6908\n",
      "Epoch 185/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7527WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7934\n",
      "Epoch 186/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6655WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6786\n",
      "Epoch 187/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6851WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7026\n",
      "Epoch 188/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7089WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6740\n",
      "Epoch 189/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6153WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6744\n",
      "Epoch 190/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7061WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7126\n",
      "Epoch 191/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6516WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6594\n",
      "Epoch 192/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6647WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.6780\n",
      "Epoch 193/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7518WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7518\n",
      "Epoch 194/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7611WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7365\n",
      "Epoch 195/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6899WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6812\n",
      "Epoch 196/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6605WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6720\n",
      "Epoch 197/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7709WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7336\n",
      "Epoch 198/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6551WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6793\n",
      "Epoch 199/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6369WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6933\n",
      "Epoch 200/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7417WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6799\n",
      "Epoch 201/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8373WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7656\n",
      "Epoch 202/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6865WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7080\n",
      "Epoch 203/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7448WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7288\n",
      "Epoch 204/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6772WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6818\n",
      "Epoch 205/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6896WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6581\n",
      "Epoch 206/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7333WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7370\n",
      "Epoch 207/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7032WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6833\n",
      "Epoch 208/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6613WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6472\n",
      "Epoch 209/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6360WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6798\n",
      "Epoch 210/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7491WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7165\n",
      "Epoch 211/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6385WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6380\n",
      "Epoch 212/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6725WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6767\n",
      "Epoch 213/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6646WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6448\n",
      "Epoch 214/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5988WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6393\n",
      "Epoch 215/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6476WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6796\n",
      "Epoch 216/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6079WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6409\n",
      "Epoch 217/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6773WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6610\n",
      "Epoch 218/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6237WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6295\n",
      "Epoch 219/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6289WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6416\n",
      "Epoch 220/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6656WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6390\n",
      "Epoch 221/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6858WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6651\n",
      "Epoch 222/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5870WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6443\n",
      "Epoch 223/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6663WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6632\n",
      "Epoch 224/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6840WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6983\n",
      "Epoch 225/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5946WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6194\n",
      "Epoch 226/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6415WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6321\n",
      "Epoch 227/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6675WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6531\n",
      "Epoch 228/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6330WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6588\n",
      "Epoch 229/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7075WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6802\n",
      "Epoch 230/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6436WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6879\n",
      "Epoch 231/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6312WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6319\n",
      "Epoch 232/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6324WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6239\n",
      "Epoch 233/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6227WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6200\n",
      "Epoch 234/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6639WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6534\n",
      "Epoch 235/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7032WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6973\n",
      "Epoch 236/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5836WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6068\n",
      "Epoch 237/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6242WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6347\n",
      "Epoch 238/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6112\n",
      "Epoch 239/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6617WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6650\n",
      "Epoch 240/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7233WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6583\n",
      "Epoch 241/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6230WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6097\n",
      "Epoch 242/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6957\n",
      "Epoch 243/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5845WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6242\n",
      "Epoch 244/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5468WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5922\n",
      "Epoch 245/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5992WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5928\n",
      "Epoch 246/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6318WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6184\n",
      "Epoch 247/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8314WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7989\n",
      "Epoch 248/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5432WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6173\n",
      "Epoch 249/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8028WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7625\n",
      "Epoch 250/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6912\n",
      "Epoch 251/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5821WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5934\n",
      "Epoch 252/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5846WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6005\n",
      "Epoch 253/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6425WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6024\n",
      "Epoch 254/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5890WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6160\n",
      "Epoch 255/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5782WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6050\n",
      "Epoch 256/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6697WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6656\n",
      "Epoch 257/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6040WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6066\n",
      "Epoch 258/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6671WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6595\n",
      "Epoch 259/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7393WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7410\n",
      "Epoch 260/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5734WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5937\n",
      "Epoch 261/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6403WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6158\n",
      "Epoch 262/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6688WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6655\n",
      "Epoch 263/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5904WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6399\n",
      "Epoch 264/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6085WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6046\n",
      "Epoch 265/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6134WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6116\n",
      "Epoch 266/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6560WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6247\n",
      "Epoch 267/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5571WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5819\n",
      "Epoch 268/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6972WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6809\n",
      "Epoch 269/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6019WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5873\n",
      "Epoch 270/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5786WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5700\n",
      "Epoch 271/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6611WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5884\n",
      "Epoch 272/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6172WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6172\n",
      "Epoch 273/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6377\n",
      "Epoch 274/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5521WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5772\n",
      "Epoch 275/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6021WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6243\n",
      "Epoch 276/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5898WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5831\n",
      "Epoch 277/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6151WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6191\n",
      "Epoch 278/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5780\n",
      "Epoch 279/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6717WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7004\n",
      "Epoch 280/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6539WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5812\n",
      "Epoch 281/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5726WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5738\n",
      "Epoch 282/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5938\n",
      "Epoch 283/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6063WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6075\n",
      "Epoch 284/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7427WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7048\n",
      "Epoch 285/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5830\n",
      "Epoch 286/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5712\n",
      "Epoch 287/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6416WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6628\n",
      "Epoch 288/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7003\n",
      "Epoch 289/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5003WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5581\n",
      "Epoch 290/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5364WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5744\n",
      "Epoch 291/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5424WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6040\n",
      "Epoch 292/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5950\n",
      "Epoch 293/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5198WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5674\n",
      "Epoch 294/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6322WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6295\n",
      "Epoch 295/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5953WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6076\n",
      "Epoch 296/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6282WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6031\n",
      "Epoch 297/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5905WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5741\n",
      "Epoch 298/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5779WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5462\n",
      "Epoch 299/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6011WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5655\n",
      "Epoch 300/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5967WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5694\n",
      "Epoch 301/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7036\n",
      "Epoch 302/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6050WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5927\n",
      "Epoch 303/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6237WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6248\n",
      "Epoch 304/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5907WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5659\n",
      "Epoch 305/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6086WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5737\n",
      "Epoch 306/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6169WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6027\n",
      "Epoch 307/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6307WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5896\n",
      "Epoch 308/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5919WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5617\n",
      "Epoch 309/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5538WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5806\n",
      "Epoch 310/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7501WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7037\n",
      "Epoch 311/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5702WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5492\n",
      "Epoch 312/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5244WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5570\n",
      "Epoch 313/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6649WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6303\n",
      "Epoch 314/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5832WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5677\n",
      "Epoch 315/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5988WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5783\n",
      "Epoch 316/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5497WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5500\n",
      "Epoch 317/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4912WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5330\n",
      "Epoch 318/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5437WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5668\n",
      "Epoch 319/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5966WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5751\n",
      "Epoch 320/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6698WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6820\n",
      "Epoch 321/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5314WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5441\n",
      "Epoch 322/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5835WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5250\n",
      "Epoch 323/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5716WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5456\n",
      "Epoch 324/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5074WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5303\n",
      "Epoch 325/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5797WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5296\n",
      "Epoch 326/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6357WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5692\n",
      "Epoch 327/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5190WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5671\n",
      "Epoch 328/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5976WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6252\n",
      "Epoch 329/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5889WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5486\n",
      "Epoch 330/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5813WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5613\n",
      "Epoch 331/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5344WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5779\n",
      "Epoch 332/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4888WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5301\n",
      "Epoch 333/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5630WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5766\n",
      "Epoch 334/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5551WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5782\n",
      "Epoch 335/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5343WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5710\n",
      "Epoch 336/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5733WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5545\n",
      "Epoch 337/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5356WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5348\n",
      "Epoch 338/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5079WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5479\n",
      "Epoch 339/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5475WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5502\n",
      "Epoch 340/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6016WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5624\n",
      "Epoch 341/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4990WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5124\n",
      "Epoch 342/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4997WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5308\n",
      "Epoch 343/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4976WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5485\n",
      "Epoch 344/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5962WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5960\n",
      "Epoch 345/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6912WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5882\n",
      "Epoch 346/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5367WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5212\n",
      "Epoch 347/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5217\n",
      "Epoch 348/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5149WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5081\n",
      "Epoch 349/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4818WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5050\n",
      "Epoch 350/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5413WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5471\n",
      "Epoch 351/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5314WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5552\n",
      "Epoch 352/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5462WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5634\n",
      "Epoch 353/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4685WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5117\n",
      "Epoch 354/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5490WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5088\n",
      "Epoch 355/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5454WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5509\n",
      "Epoch 356/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5101WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5201\n",
      "Epoch 357/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5497WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5261\n",
      "Epoch 358/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5745WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5530\n",
      "Epoch 359/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5897WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5541\n",
      "Epoch 360/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5146WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5381\n",
      "Epoch 361/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5520WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5194\n",
      "Epoch 362/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5724WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5556\n",
      "Epoch 363/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5374WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5032\n",
      "Epoch 364/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5124WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5272\n",
      "Epoch 365/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6433WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6207\n",
      "Epoch 366/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5236WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5257\n",
      "Epoch 367/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5963WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5501\n",
      "Epoch 368/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6640WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6410\n",
      "Epoch 369/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4901WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4878\n",
      "Epoch 370/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5635WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4981\n",
      "Epoch 371/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4887WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5243\n",
      "Epoch 372/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5206WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4966\n",
      "Epoch 373/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6081WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5307\n",
      "Epoch 374/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5523WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5664\n",
      "Epoch 375/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5080\n",
      "Epoch 376/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5323WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5311\n",
      "Epoch 377/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.7034WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6718\n",
      "Epoch 378/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4921WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4970\n",
      "Epoch 379/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4997WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4860\n",
      "Epoch 380/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5843WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5742\n",
      "Epoch 381/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5685WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5630\n",
      "Epoch 382/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6244WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6050\n",
      "Epoch 383/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4791WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4962\n",
      "Epoch 384/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4902WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4815\n",
      "Epoch 385/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5419WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5195\n",
      "Epoch 386/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4779WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4840\n",
      "Epoch 387/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5853WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5531\n",
      "Epoch 388/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5218WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5129\n",
      "Epoch 389/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5732WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5593\n",
      "Epoch 390/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5722WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5707\n",
      "Epoch 391/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3906WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4873\n",
      "Epoch 392/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5671WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5283\n",
      "Epoch 393/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6840WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6117\n",
      "Epoch 394/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5196\n",
      "Epoch 395/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4794\n",
      "Epoch 396/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5086WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5103\n",
      "Epoch 397/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4677WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4810\n",
      "Epoch 398/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4503WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5049\n",
      "Epoch 399/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4828WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4940\n",
      "Epoch 400/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4223WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4946\n",
      "Epoch 401/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4941WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.5130\n",
      "Epoch 402/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5265WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5237\n",
      "Epoch 403/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4970WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4787\n",
      "Epoch 404/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4985WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5101\n",
      "Epoch 405/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4906WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5413\n",
      "Epoch 406/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4565WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4922\n",
      "Epoch 407/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6122WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5456\n",
      "Epoch 408/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5503WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5193\n",
      "Epoch 409/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5154WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4986\n",
      "Epoch 410/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5186WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4890\n",
      "Epoch 411/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5495WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5088\n",
      "Epoch 412/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4752WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4749\n",
      "Epoch 413/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5800WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5857\n",
      "Epoch 414/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5538WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6269\n",
      "Epoch 415/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5811WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5246\n",
      "Epoch 416/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6124WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5878\n",
      "Epoch 417/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5054WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5209\n",
      "Epoch 418/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4755WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4726\n",
      "Epoch 419/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4516WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4701\n",
      "Epoch 420/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5888WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5574\n",
      "Epoch 421/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4868WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.4947\n",
      "Epoch 422/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4941WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5042\n",
      "Epoch 423/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5283WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4990\n",
      "Epoch 424/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4923WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5085\n",
      "Epoch 425/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4841WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4677\n",
      "Epoch 426/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5291WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5198\n",
      "Epoch 427/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4764WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4810\n",
      "Epoch 428/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4623WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4588\n",
      "Epoch 429/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4491WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4754\n",
      "Epoch 430/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5257WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5531\n",
      "Epoch 431/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4843WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4917\n",
      "Epoch 432/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4761WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4618\n",
      "Epoch 433/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4428WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4660\n",
      "Epoch 434/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5155WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5245\n",
      "Epoch 435/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4795WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4466\n",
      "Epoch 436/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5136WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5084\n",
      "Epoch 437/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5316\n",
      "Epoch 438/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4363WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4603\n",
      "Epoch 439/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4124WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4428\n",
      "Epoch 440/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4426WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4549\n",
      "Epoch 441/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5151WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4878\n",
      "Epoch 442/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3881WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4656\n",
      "Epoch 443/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4670WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4614\n",
      "Epoch 444/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4427WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4511\n",
      "Epoch 445/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4481WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4455\n",
      "Epoch 446/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5395WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4818\n",
      "Epoch 447/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4961WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4597\n",
      "Epoch 448/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4594WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4610\n",
      "Epoch 449/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4270WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4688\n",
      "Epoch 450/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4277WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4953\n",
      "Epoch 451/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.6713WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6207\n",
      "Epoch 452/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5066WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4679\n",
      "Epoch 453/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5020WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4920\n",
      "Epoch 454/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4162WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4336\n",
      "Epoch 455/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5177WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4655\n",
      "Epoch 456/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3931WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4558\n",
      "Epoch 457/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4171WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4369\n",
      "Epoch 458/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5756WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5283\n",
      "Epoch 459/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4073WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4661\n",
      "Epoch 460/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5909WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5691\n",
      "Epoch 461/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4916WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4586\n",
      "Epoch 462/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.3901WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4467\n",
      "Epoch 463/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4780WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4875\n",
      "Epoch 464/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5894WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5252\n",
      "Epoch 465/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4722WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4538\n",
      "Epoch 466/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5341WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5031\n",
      "Epoch 467/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5872WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5721\n",
      "Epoch 468/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4997WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4841\n",
      "Epoch 469/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5820WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.5340\n",
      "Epoch 470/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5306WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4866\n",
      "Epoch 471/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4824WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4973\n",
      "Epoch 472/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5874WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6041\n",
      "Epoch 473/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.4715WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4874\n",
      "Epoch 474/474\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5667WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [55.94728469848633,\n",
       "  52.07895278930664,\n",
       "  45.57687759399414,\n",
       "  35.92137908935547,\n",
       "  24.925676345825195,\n",
       "  18.994943618774414,\n",
       "  16.574140548706055,\n",
       "  15.171977996826172,\n",
       "  13.583379745483398,\n",
       "  11.609012603759766,\n",
       "  9.79041862487793,\n",
       "  8.833045959472656,\n",
       "  8.511301040649414,\n",
       "  8.306008338928223,\n",
       "  8.100603103637695,\n",
       "  7.909032821655273,\n",
       "  7.723032474517822,\n",
       "  7.538015842437744,\n",
       "  7.3430352210998535,\n",
       "  7.161631107330322,\n",
       "  6.972282409667969,\n",
       "  6.829354763031006,\n",
       "  6.633838653564453,\n",
       "  6.441385269165039,\n",
       "  6.269529342651367,\n",
       "  6.100556373596191,\n",
       "  5.893163681030273,\n",
       "  5.716915607452393,\n",
       "  5.468841075897217,\n",
       "  5.293971538543701,\n",
       "  5.050141334533691,\n",
       "  4.861372470855713,\n",
       "  4.671133041381836,\n",
       "  4.460111618041992,\n",
       "  4.2985405921936035,\n",
       "  4.171308517456055,\n",
       "  4.176867961883545,\n",
       "  3.9387428760528564,\n",
       "  3.8818676471710205,\n",
       "  3.836468458175659,\n",
       "  3.7436470985412598,\n",
       "  3.6841416358947754,\n",
       "  3.651240587234497,\n",
       "  3.709040880203247,\n",
       "  3.535449981689453,\n",
       "  3.4952337741851807,\n",
       "  3.4794740676879883,\n",
       "  3.44148325920105,\n",
       "  3.4128284454345703,\n",
       "  3.402935028076172,\n",
       "  3.3320188522338867,\n",
       "  3.305715322494507,\n",
       "  3.2975070476531982,\n",
       "  3.3441545963287354,\n",
       "  3.2618420124053955,\n",
       "  3.229755401611328,\n",
       "  3.220456838607788,\n",
       "  3.4053428173065186,\n",
       "  3.252643585205078,\n",
       "  3.2138984203338623,\n",
       "  3.2293143272399902,\n",
       "  3.182433843612671,\n",
       "  3.138921022415161,\n",
       "  3.125349998474121,\n",
       "  3.1057028770446777,\n",
       "  3.1040945053100586,\n",
       "  3.0718276500701904,\n",
       "  3.0543620586395264,\n",
       "  3.056553602218628,\n",
       "  3.03721284866333,\n",
       "  3.027043342590332,\n",
       "  3.044041633605957,\n",
       "  3.0661051273345947,\n",
       "  3.0513319969177246,\n",
       "  3.2263638973236084,\n",
       "  3.0607473850250244,\n",
       "  3.0040717124938965,\n",
       "  3.166754722595215,\n",
       "  2.9921441078186035,\n",
       "  2.9760844707489014,\n",
       "  3.1146533489227295,\n",
       "  2.953399181365967,\n",
       "  2.9535107612609863,\n",
       "  3.0118632316589355,\n",
       "  3.000469923019409,\n",
       "  3.0026357173919678,\n",
       "  2.9375457763671875,\n",
       "  2.926790475845337,\n",
       "  3.019907236099243,\n",
       "  2.9648187160491943,\n",
       "  3.037076473236084,\n",
       "  2.908607244491577,\n",
       "  2.908205986022949,\n",
       "  2.9093594551086426,\n",
       "  2.9658334255218506,\n",
       "  2.9433727264404297,\n",
       "  3.0276360511779785,\n",
       "  2.9193944931030273,\n",
       "  2.9080967903137207,\n",
       "  2.9070703983306885,\n",
       "  2.9466073513031006,\n",
       "  2.9694855213165283,\n",
       "  3.01814866065979,\n",
       "  2.853942632675171,\n",
       "  2.875678539276123,\n",
       "  2.9089694023132324,\n",
       "  2.867953300476074,\n",
       "  2.8843297958374023,\n",
       "  2.8567771911621094,\n",
       "  2.8926844596862793,\n",
       "  2.9530348777770996,\n",
       "  2.8259921073913574,\n",
       "  2.8494935035705566,\n",
       "  2.8735833168029785,\n",
       "  2.829496383666992,\n",
       "  2.815509557723999,\n",
       "  2.8050663471221924,\n",
       "  2.9380645751953125,\n",
       "  2.8483667373657227,\n",
       "  2.912956714630127,\n",
       "  2.8036890029907227,\n",
       "  2.850522994995117,\n",
       "  2.801741600036621,\n",
       "  2.9840457439422607,\n",
       "  2.790581703186035,\n",
       "  2.8301749229431152,\n",
       "  2.8593528270721436,\n",
       "  2.8543789386749268,\n",
       "  2.7744216918945312,\n",
       "  2.8403098583221436,\n",
       "  2.9234650135040283,\n",
       "  2.7643191814422607,\n",
       "  2.805222511291504,\n",
       "  2.813966989517212,\n",
       "  2.7565221786499023,\n",
       "  2.75034761428833,\n",
       "  2.8152735233306885,\n",
       "  2.762554407119751,\n",
       "  2.775528907775879,\n",
       "  2.827784299850464,\n",
       "  2.8270862102508545,\n",
       "  2.736654043197632,\n",
       "  2.8634912967681885,\n",
       "  2.7441134452819824,\n",
       "  2.7440922260284424,\n",
       "  2.7354910373687744,\n",
       "  2.7969415187835693,\n",
       "  2.783376932144165,\n",
       "  2.731996774673462,\n",
       "  2.7298948764801025,\n",
       "  2.731111764907837,\n",
       "  2.760207414627075,\n",
       "  2.8042116165161133,\n",
       "  2.8175859451293945,\n",
       "  2.920703411102295,\n",
       "  2.7175166606903076,\n",
       "  2.7184784412384033,\n",
       "  2.81823992729187,\n",
       "  2.718430519104004,\n",
       "  2.858222007751465,\n",
       "  2.8519949913024902,\n",
       "  2.830038547515869,\n",
       "  2.7489230632781982,\n",
       "  2.7334651947021484,\n",
       "  2.727917194366455,\n",
       "  2.7034995555877686,\n",
       "  2.69858717918396,\n",
       "  2.7507383823394775,\n",
       "  2.7111454010009766,\n",
       "  2.7264890670776367,\n",
       "  2.6991524696350098,\n",
       "  2.6943864822387695,\n",
       "  2.774085760116577,\n",
       "  2.706167459487915,\n",
       "  2.7069478034973145,\n",
       "  2.7171785831451416,\n",
       "  2.8556268215179443,\n",
       "  2.6946325302124023,\n",
       "  2.6896467208862305,\n",
       "  2.7058541774749756,\n",
       "  2.769148111343384,\n",
       "  2.698803424835205,\n",
       "  2.7195873260498047,\n",
       "  2.690753698348999,\n",
       "  2.793370246887207,\n",
       "  2.6785974502563477,\n",
       "  2.702618360519409,\n",
       "  2.6739659309387207,\n",
       "  2.674443006515503,\n",
       "  2.7126147747039795,\n",
       "  2.659386396408081,\n",
       "  2.6779563426971436,\n",
       "  2.7517716884613037,\n",
       "  2.7364695072174072,\n",
       "  2.6812374591827393,\n",
       "  2.6719889640808105,\n",
       "  2.733600378036499,\n",
       "  2.6793465614318848,\n",
       "  2.693326234817505,\n",
       "  2.679896354675293,\n",
       "  2.7655866146087646,\n",
       "  2.7079741954803467,\n",
       "  2.7288131713867188,\n",
       "  2.6818385124206543,\n",
       "  2.6581296920776367,\n",
       "  2.7370216846466064,\n",
       "  2.683283805847168,\n",
       "  2.6472206115722656,\n",
       "  2.679823398590088,\n",
       "  2.716517925262451,\n",
       "  2.6379828453063965,\n",
       "  2.676727533340454,\n",
       "  2.6447808742523193,\n",
       "  2.6393072605133057,\n",
       "  2.67960262298584,\n",
       "  2.6409146785736084,\n",
       "  2.661038637161255,\n",
       "  2.629462480545044,\n",
       "  2.6416056156158447,\n",
       "  2.6390092372894287,\n",
       "  2.6651008129119873,\n",
       "  2.6443190574645996,\n",
       "  2.663240432739258,\n",
       "  2.6983327865600586,\n",
       "  2.619357109069824,\n",
       "  2.6320815086364746,\n",
       "  2.653057813644409,\n",
       "  2.658837080001831,\n",
       "  2.680227041244507,\n",
       "  2.687882423400879,\n",
       "  2.631863832473755,\n",
       "  2.6238772869110107,\n",
       "  2.6199936866760254,\n",
       "  2.6534149646759033,\n",
       "  2.6972897052764893,\n",
       "  2.606804132461548,\n",
       "  2.634700298309326,\n",
       "  2.6111717224121094,\n",
       "  2.665012836456299,\n",
       "  2.658320426940918,\n",
       "  2.609678030014038,\n",
       "  2.695688009262085,\n",
       "  2.6241812705993652,\n",
       "  2.5921690464019775,\n",
       "  2.5928127765655518,\n",
       "  2.61841082572937,\n",
       "  2.798943042755127,\n",
       "  2.6172945499420166,\n",
       "  2.7625136375427246,\n",
       "  2.6912076473236084,\n",
       "  2.5933940410614014,\n",
       "  2.600538969039917,\n",
       "  2.6024370193481445,\n",
       "  2.6159555912017822,\n",
       "  2.604966402053833,\n",
       "  2.6656315326690674,\n",
       "  2.606626033782959,\n",
       "  2.6595206260681152,\n",
       "  2.7409565448760986,\n",
       "  2.5936765670776367,\n",
       "  2.615783214569092,\n",
       "  2.66550350189209,\n",
       "  2.6399033069610596,\n",
       "  2.60459303855896,\n",
       "  2.611603021621704,\n",
       "  2.624662160873413,\n",
       "  2.5818862915039062,\n",
       "  2.6808555126190186,\n",
       "  2.587251901626587,\n",
       "  2.570025682449341,\n",
       "  2.5884125232696533,\n",
       "  2.6171581745147705,\n",
       "  2.637747287750244,\n",
       "  2.5771679878234863,\n",
       "  2.6242589950561523,\n",
       "  2.5831470489501953,\n",
       "  2.61911678314209,\n",
       "  2.57796311378479,\n",
       "  2.7004032135009766,\n",
       "  2.5812435150146484,\n",
       "  2.5738019943237305,\n",
       "  2.593829870223999,\n",
       "  2.6074600219726562,\n",
       "  2.704801082611084,\n",
       "  2.5829501152038574,\n",
       "  2.5711588859558105,\n",
       "  2.6628105640411377,\n",
       "  2.700289487838745,\n",
       "  2.5581276416778564,\n",
       "  2.574376344680786,\n",
       "  2.6039979457855225,\n",
       "  2.5950169563293457,\n",
       "  2.5673646926879883,\n",
       "  2.6295464038848877,\n",
       "  2.6076385974884033,\n",
       "  2.603135585784912,\n",
       "  2.574089765548706,\n",
       "  2.5461738109588623,\n",
       "  2.565523147583008,\n",
       "  2.5694150924682617,\n",
       "  2.703592538833618,\n",
       "  2.592700719833374,\n",
       "  2.62483811378479,\n",
       "  2.565866708755493,\n",
       "  2.5736968517303467,\n",
       "  2.6026718616485596,\n",
       "  2.5896389484405518,\n",
       "  2.561734437942505,\n",
       "  2.580570936203003,\n",
       "  2.70367693901062,\n",
       "  2.5492465496063232,\n",
       "  2.5570104122161865,\n",
       "  2.6303000450134277,\n",
       "  2.5676608085632324,\n",
       "  2.578256130218506,\n",
       "  2.5499966144561768,\n",
       "  2.5329818725585938,\n",
       "  2.5667920112609863,\n",
       "  2.5750794410705566,\n",
       "  2.6819772720336914,\n",
       "  2.5440878868103027,\n",
       "  2.5250046253204346,\n",
       "  2.545574188232422,\n",
       "  2.5302882194519043,\n",
       "  2.529607057571411,\n",
       "  2.5691640377044678,\n",
       "  2.5671041011810303,\n",
       "  2.625182628631592,\n",
       "  2.5485706329345703,\n",
       "  2.561309814453125,\n",
       "  2.5778799057006836,\n",
       "  2.530139446258545,\n",
       "  2.5766425132751465,\n",
       "  2.578217029571533,\n",
       "  2.57100772857666,\n",
       "  2.554506301879883,\n",
       "  2.5348422527313232,\n",
       "  2.5478627681732178,\n",
       "  2.550231456756592,\n",
       "  2.5623929500579834,\n",
       "  2.5124192237854004,\n",
       "  2.530822992324829,\n",
       "  2.548485279083252,\n",
       "  2.596038818359375,\n",
       "  2.5882039070129395,\n",
       "  2.5212223529815674,\n",
       "  2.5216548442840576,\n",
       "  2.50813364982605,\n",
       "  2.504995584487915,\n",
       "  2.547100067138672,\n",
       "  2.555156946182251,\n",
       "  2.563413143157959,\n",
       "  2.5116994380950928,\n",
       "  2.5088183879852295,\n",
       "  2.550901174545288,\n",
       "  2.5200695991516113,\n",
       "  2.526064872741699,\n",
       "  2.55295729637146,\n",
       "  2.5541293621063232,\n",
       "  2.538127899169922,\n",
       "  2.519399881362915,\n",
       "  2.5555896759033203,\n",
       "  2.5031964778900146,\n",
       "  2.527238607406616,\n",
       "  2.620731830596924,\n",
       "  2.525714159011841,\n",
       "  2.550142288208008,\n",
       "  2.641030788421631,\n",
       "  2.4878041744232178,\n",
       "  2.498053789138794,\n",
       "  2.524345636367798,\n",
       "  2.496640920639038,\n",
       "  2.530745506286621,\n",
       "  2.566404342651367,\n",
       "  2.5079803466796875,\n",
       "  2.5311081409454346,\n",
       "  2.671788215637207,\n",
       "  2.497030258178711,\n",
       "  2.4860455989837646,\n",
       "  2.5742273330688477,\n",
       "  2.5629968643188477,\n",
       "  2.6050424575805664,\n",
       "  2.4961540699005127,\n",
       "  2.481546640396118,\n",
       "  2.5195086002349854,\n",
       "  2.4840199947357178,\n",
       "  2.5530810356140137,\n",
       "  2.5129380226135254,\n",
       "  2.559297800064087,\n",
       "  2.570672035217285,\n",
       "  2.4873178005218506,\n",
       "  2.5282599925994873,\n",
       "  2.6117115020751953,\n",
       "  2.519561529159546,\n",
       "  2.479379415512085,\n",
       "  2.510251045227051,\n",
       "  2.481041431427002,\n",
       "  2.5048611164093018,\n",
       "  2.4940342903137207,\n",
       "  2.4946072101593018,\n",
       "  2.5129687786102295,\n",
       "  2.523737907409668,\n",
       "  2.4787349700927734,\n",
       "  2.5101170539855957,\n",
       "  2.541292428970337,\n",
       "  2.492169141769409,\n",
       "  2.5455503463745117,\n",
       "  2.5192816257476807,\n",
       "  2.498626947402954,\n",
       "  2.4890263080596924,\n",
       "  2.508782386779785,\n",
       "  2.4748528003692627,\n",
       "  2.5856645107269287,\n",
       "  2.62691330909729,\n",
       "  2.5246317386627197,\n",
       "  2.5877630710601807,\n",
       "  2.5209481716156006,\n",
       "  2.472551107406616,\n",
       "  2.4701294898986816,\n",
       "  2.557448148727417,\n",
       "  2.4946889877319336,\n",
       "  2.504232406616211,\n",
       "  2.499037027359009,\n",
       "  2.5085179805755615,\n",
       "  2.467667579650879,\n",
       "  2.519841432571411,\n",
       "  2.480988025665283,\n",
       "  2.458775520324707,\n",
       "  2.4754152297973633,\n",
       "  2.5531065464019775,\n",
       "  2.4917285442352295,\n",
       "  2.4618256092071533,\n",
       "  2.465985059738159,\n",
       "  2.524501323699951,\n",
       "  2.446554183959961,\n",
       "  2.5083906650543213,\n",
       "  2.5315873622894287,\n",
       "  2.460325241088867,\n",
       "  2.44275164604187,\n",
       "  2.4548861980438232,\n",
       "  2.4877851009368896,\n",
       "  2.4656450748443604,\n",
       "  2.4613711833953857,\n",
       "  2.451073408126831,\n",
       "  2.4455058574676514,\n",
       "  2.481754779815674,\n",
       "  2.459679365158081,\n",
       "  2.4609947204589844,\n",
       "  2.468801975250244,\n",
       "  2.4952924251556396,\n",
       "  2.620689630508423,\n",
       "  2.4679205417633057,\n",
       "  2.492025136947632,\n",
       "  2.433624744415283,\n",
       "  2.465458393096924,\n",
       "  2.4557950496673584,\n",
       "  2.4369382858276367,\n",
       "  2.5283315181732178,\n",
       "  2.4660909175872803,\n",
       "  2.5691211223602295,\n",
       "  2.4586000442504883,\n",
       "  2.446741819381714,\n",
       "  2.4874627590179443,\n",
       "  2.52522611618042,\n",
       "  2.4537882804870605,\n",
       "  2.5031113624572754,\n",
       "  2.5721065998077393,\n",
       "  2.4840588569641113,\n",
       "  2.533951997756958,\n",
       "  2.486591339111328,\n",
       "  2.497265100479126,\n",
       "  2.604097604751587,\n",
       "  2.4873905181884766,\n",
       "  2.4791698455810547]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_ES = ModelCheckpoint('Early_stopping_BEST.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model=create_model_ES()\n",
    "#model.add_loss(MEE_k)\n",
    "model.fit(X_dev, y_dev, epochs=min_k, \n",
    "                      batch_size=1036, callbacks=[mc_ES]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyVal_ES_mean=np.mean(historyVal_ES, axis=0)\n",
    "historyTr_ES_mean=np.mean(historyTr_ES, axis=0)\n",
    "\n",
    "historyVal_ES_sd=np.std(historyVal_ES, axis=0)\n",
    "historyTr_ES_sd=np.std(historyTr_ES, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(range(len(historyTr_ES_mean)),historyTr_ES_mean, label = 'Training error')\n",
    "plt.fill_between(range(len(historyTr_ES_mean)), historyTr_ES_mean - historyTr_ES_sd, \n",
    "                 historyTr_ES_mean + historyTr_ES_sd, \n",
    "                 color='b', alpha=0.15)\n",
    "\n",
    "plt.plot(range(len(historyVal_ES_mean)), historyVal_ES_mean, label = 'Validation error')\n",
    "plt.fill_between(range(len(historyVal_ES_mean)), historyVal_ES_mean - historyVal_ES_sd, \n",
    "                 historyVal_ES_mean + historyVal_ES_sd, \n",
    "                 color='orange', alpha=0.15)\n",
    "\n",
    "plt.ylabel('MEE', fontsize = 14)\n",
    "plt.xlabel('Iteration', fontsize = 14)\n",
    "plt.legend()\n",
    "plt.ylim(5,20)\n",
    "plt.xlim(-5,min_k)\n",
    "plt.yticks(np.arange(0, 20, +1))\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model with Early Stopping result:\n",
      "MEE on the validation 2.8103395462036134 with standard deviation 0.1416157867408675\n",
      "MEE on the training 2.565181541442871 with standard deviation 0.03064434635943835\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model with Early Stopping result:\")\n",
    "print(\"MEE on the validation\",historyVal_ES_mean[-1],\"with standard deviation\",historyVal_ES_sd[-1])\n",
    "print(\"MEE on the training\",historyTr_ES_mean[-1],\"with standard deviation\",historyTr_ES_sd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "print(min_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
